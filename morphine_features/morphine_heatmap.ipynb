{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdec574-8d0e-42a1-bff1-5f018b98c11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20f857-bc32-4e05-8d71-e752c2958eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb929b-8b75-4e1b-a9c5-93965e5cf71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "TREATMENT_MAP = {\n",
    "    4917: '5 mg/kg', 4918: 'Vehicle', 4919: '25 mg/kg',\n",
    "    4920: '25 mg/kg', 4921: '5 mg/kg', 4922: 'Vehicle',\n",
    "    4923: 'Vehicle', 4924: '25 mg/kg', 4925: '5 mg/kg'\n",
    "}\n",
    "\n",
    "# Replicate 1: Jan 14, 2025 at 6:00 AM EST\n",
    "INJECTION_DATETIME = datetime(2025, 1, 14, 6, 0, 0)\n",
    "\n",
    "# Define temporal windows based on the locomotor activity pattern\n",
    "TEMPORAL_WINDOWS = {\n",
    "    'baseline': (-180, -60),          # 3 hours to 1 hour before injection\n",
    "    'immediate': (0, 30),              # First 30 minutes (acute spike)\n",
    "    'peak_early': (30, 90),            # 30-90 min (early peak)\n",
    "    'peak_sustained': (90, 180),       # 90-180 min (sustained peak)\n",
    "    'decline_early': (180, 300),       # 180-300 min (early decline)\n",
    "    'decline_late': (300, 420),        # 300-420 min (late decline, return to baseline)\n",
    "    'post_6hr': (360, 540),            # 6-9 hours (extended recovery)\n",
    "    'post_12hr': (720, 900),           # 12-15 hours (long-term effects)\n",
    "    'next_day': (1380, 1560),          # 23-26 hours (next day effects)\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MORPHINE TEMPORAL WINDOW ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nInjection time: {INJECTION_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nTemporal windows defined:\")\n",
    "for name, (start, end) in TEMPORAL_WINDOWS.items():\n",
    "    print(f\"  {name:20s}: {start:5d} to {end:5d} minutes ({(end-start)/60:.1f} hours)\")\n",
    "\n",
    "# Initialize DuckDB\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_region='us-east-1';\")\n",
    "con.execute(\"SET s3_url_style='path';\")\n",
    "print(\"\\n✓ DuckDB initialized\")\n",
    "\n",
    "def generate_paths(cages, dates, filename):\n",
    "    \"\"\"Generate list of S3 paths for all cage/date combinations\"\"\"\n",
    "    paths = []\n",
    "    for cage in cages:\n",
    "        for date in dates:\n",
    "            path = f\"s3://jax-envision-public-data/study_1001/2025v3.3/tabular/cage_id={cage}/date={date}/{filename}\"\n",
    "            paths.append(f\"'{path}'\")\n",
    "    return ', '.join(paths)\n",
    "\n",
    "def load_temporal_data(cages, dates):\n",
    "    \"\"\"Load high-resolution behavioral data with timestamps\"\"\"\n",
    "    \n",
    "    print(\"\\nLoading behavioral metrics with timestamps...\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    # 1. Activity states (60-second resolution)\n",
    "    print(\"  1. Activity states...\")\n",
    "    activity_paths = generate_paths(cages, dates, 'animal_activity_db.parquet')\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cage_id,\n",
    "        time,\n",
    "        name,\n",
    "        value\n",
    "    FROM read_parquet([{activity_paths}])\n",
    "    WHERE resolution = 60\n",
    "        AND name IN ('animal_bouts.active', 'animal_bouts.inactive', \n",
    "                     'animal_bouts.climbing', 'animal_bouts.locomotion')\n",
    "    \"\"\"\n",
    "    df_activity = con.execute(query).df()\n",
    "    \n",
    "    # Pivot activity states\n",
    "    df_activity_pivot = df_activity.pivot_table(\n",
    "        index=['cage_id', 'time'],\n",
    "        columns='name',\n",
    "        values='value',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "    df_activity_pivot.columns.name = None\n",
    "    df_activity_pivot = df_activity_pivot.rename(columns={\n",
    "        'animal_bouts.active': 'active',\n",
    "        'animal_bouts.inactive': 'inactive',\n",
    "        'animal_bouts.climbing': 'climbing',\n",
    "        'animal_bouts.locomotion': 'locomotion'\n",
    "    })\n",
    "    all_metrics.append(df_activity_pivot)\n",
    "    \n",
    "    # 2. Distance traveled (60-second resolution)\n",
    "    print(\"  2. Distance traveled...\")\n",
    "    distance_paths = generate_paths(cages, dates, 'animal_aggs_short_id.parquet')\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cage_id,\n",
    "        time,\n",
    "        AVG(value) as distance\n",
    "    FROM read_parquet([{distance_paths}])\n",
    "    WHERE name = 'animal.distance_travelled' AND resolution = 60\n",
    "    GROUP BY cage_id, time\n",
    "    \"\"\"\n",
    "    df_distance = con.execute(query).df()\n",
    "    all_metrics.append(df_distance)\n",
    "    \n",
    "    # 3. Drinking behavior (60-second resolution)\n",
    "    print(\"  3. Drinking behavior...\")\n",
    "    drinking_paths = generate_paths(cages, dates, 'animal_drinking.parquet')\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cage_id,\n",
    "        time,\n",
    "        AVG(value) as drinking\n",
    "    FROM read_parquet([{drinking_paths}])\n",
    "    WHERE name = 'animal_bouts.drinking' AND resolution = 60\n",
    "    GROUP BY cage_id, time\n",
    "    \"\"\"\n",
    "    df_drinking = con.execute(query).df()\n",
    "    all_metrics.append(df_drinking)\n",
    "    \n",
    "    # 4. Motion scores (60-second resolution)\n",
    "    print(\"  4. Motion scores...\")\n",
    "    motion_paths = generate_paths(cages, dates, 'cage_motion_vector.parquet')\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cage_id,\n",
    "        time,\n",
    "        AVG(value) as motion_score\n",
    "    FROM read_parquet([{motion_paths}])\n",
    "    WHERE resolution = 60\n",
    "    GROUP BY cage_id, time\n",
    "    \"\"\"\n",
    "    df_motion = con.execute(query).df()\n",
    "    all_metrics.append(df_motion)\n",
    "    \n",
    "    # Merge all metrics\n",
    "    print(\"  Merging all metrics...\")\n",
    "    df_merged = all_metrics[0]\n",
    "    for df in all_metrics[1:]:\n",
    "        df_merged = df_merged.merge(df, on=['cage_id', 'time'], how='outer')\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# Generate date range: 2 days before to 2 days after injection\n",
    "dates = [(INJECTION_DATETIME + timedelta(days=d)).strftime('%Y-%m-%d') \n",
    "         for d in range(-2, 3)]\n",
    "\n",
    "print(f\"\\nLoading data for cages: {list(TREATMENT_MAP.keys())}\")\n",
    "print(f\"Date range: {dates[0]} to {dates[-1]}\")\n",
    "\n",
    "# Load high-resolution data\n",
    "df = load_temporal_data(list(TREATMENT_MAP.keys()), dates)\n",
    "\n",
    "# Convert time to datetime and calculate minutes from injection\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['minutes_from_injection'] = (df['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# Add treatment labels\n",
    "df['treatment'] = df['cage_id'].map(TREATMENT_MAP)\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(df)} records\")\n",
    "print(f\"  Time range: {df['minutes_from_injection'].min():.0f} to {df['minutes_from_injection'].max():.0f} minutes\")\n",
    "print(f\"  Features: {[c for c in df.columns if c not in ['cage_id', 'time', 'minutes_from_injection', 'treatment']]}\")\n",
    "\n",
    "# Calculate metrics for each temporal window\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING WINDOW STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "window_stats = []\n",
    "\n",
    "feature_cols = ['active', 'inactive', 'climbing', 'locomotion', 'distance', 'drinking', 'motion_score']\n",
    "\n",
    "for cage_id in df['cage_id'].unique():\n",
    "    cage_data = df[df['cage_id'] == cage_id].copy()\n",
    "    treatment = TREATMENT_MAP[cage_id]\n",
    "    \n",
    "    for window_name, (start_min, end_min) in TEMPORAL_WINDOWS.items():\n",
    "        window_data = cage_data[\n",
    "            (cage_data['minutes_from_injection'] >= start_min) & \n",
    "            (cage_data['minutes_from_injection'] < end_min)\n",
    "        ]\n",
    "        \n",
    "        if len(window_data) > 0:\n",
    "            for feature in feature_cols:\n",
    "                if feature in window_data.columns:\n",
    "                    mean_val = window_data[feature].mean()\n",
    "                    if not pd.isna(mean_val):\n",
    "                        window_stats.append({\n",
    "                            'cage_id': cage_id,\n",
    "                            'treatment': treatment,\n",
    "                            'window': window_name,\n",
    "                            'feature': feature,\n",
    "                            'mean_value': mean_val\n",
    "                        })\n",
    "\n",
    "df_windows = pd.DataFrame(window_stats)\n",
    "\n",
    "# Calculate percent change from baseline for each window\n",
    "print(\"\\nCalculating percent changes from baseline...\")\n",
    "\n",
    "pct_changes = []\n",
    "\n",
    "for cage_id in df_windows['cage_id'].unique():\n",
    "    cage_data = df_windows[df_windows['cage_id'] == cage_id]\n",
    "    treatment = cage_data['treatment'].iloc[0]\n",
    "    \n",
    "    baseline_data = cage_data[cage_data['window'] == 'baseline']\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        feature_baseline = baseline_data[baseline_data['feature'] == feature]\n",
    "        \n",
    "        if len(feature_baseline) > 0:\n",
    "            baseline_mean = feature_baseline['mean_value'].iloc[0]\n",
    "            \n",
    "            if baseline_mean > 0:\n",
    "                for window_name in [w for w in TEMPORAL_WINDOWS.keys() if w != 'baseline']:\n",
    "                    window_data = cage_data[\n",
    "                        (cage_data['window'] == window_name) & \n",
    "                        (cage_data['feature'] == feature)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(window_data) > 0:\n",
    "                        current_mean = window_data['mean_value'].iloc[0]\n",
    "                        pct_change = ((current_mean - baseline_mean) / baseline_mean) * 100\n",
    "                        \n",
    "                        pct_changes.append({\n",
    "                            'treatment': treatment,\n",
    "                            'window': window_name,\n",
    "                            'feature': feature,\n",
    "                            'pct_change': pct_change\n",
    "                        })\n",
    "\n",
    "df_pct = pd.DataFrame(pct_changes)\n",
    "\n",
    "print(f\"✓ Calculated {len(df_pct)} percent change values across {len(TEMPORAL_WINDOWS)-1} time windows\")\n",
    "\n",
    "# Create comprehensive heatmap\n",
    "print(\"\\nCreating temporal window heatmap...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 16))\n",
    "treatments = ['Vehicle', '5 mg/kg', '25 mg/kg']\n",
    "\n",
    "# Define window order for plotting (chronological)\n",
    "window_order = ['immediate', 'peak_early', 'peak_sustained', 'decline_early', \n",
    "                'decline_late', 'post_6hr', 'post_12hr', 'next_day']\n",
    "\n",
    "for idx, treatment in enumerate(treatments):\n",
    "    treatment_data = df_pct[df_pct['treatment'] == treatment]\n",
    "    \n",
    "    if len(treatment_data) > 0:\n",
    "        pivot = treatment_data.pivot_table(\n",
    "            index='feature',\n",
    "            columns='window',\n",
    "            values='pct_change',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Reorder columns chronologically\n",
    "        pivot = pivot[[col for col in window_order if col in pivot.columns]]\n",
    "        \n",
    "        # Sort features by maximum absolute change\n",
    "        feature_importance = pivot.abs().max(axis=1).sort_values(ascending=False)\n",
    "        pivot = pivot.loc[feature_importance.index]\n",
    "        \n",
    "        # Dynamic color scale\n",
    "        vmax = max(50, pivot.abs().max().max())\n",
    "        \n",
    "        sns.heatmap(pivot, \n",
    "                   ax=axes[idx],\n",
    "                   cmap='RdYlGn_r',\n",
    "                   center=0,\n",
    "                   vmin=-vmax, vmax=vmax,\n",
    "                   cbar_kws={'label': '% Change from Baseline'},\n",
    "                   linewidths=0.5,\n",
    "                   annot=True,\n",
    "                   fmt='.1f',\n",
    "                   annot_kws={'size': 8})\n",
    "        \n",
    "        axes[idx].set_title(f'{treatment} - Temporal Response Profile', \n",
    "                          fontsize=14, fontweight='bold', pad=10)\n",
    "        axes[idx].set_xlabel('Time Window After Injection', fontsize=11)\n",
    "        axes[idx].set_ylabel('Behavioral Feature', fontsize=11)\n",
    "        \n",
    "        # Rotate x-axis labels for readability\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('morphine_temporal_windows_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Analysis complete!\")\n",
    "print(\"  Saved: morphine_temporal_windows_heatmap.png\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print summary statistics for each treatment\n",
    "print(\"\\nPeak responses by treatment and time window:\")\n",
    "for treatment in treatments:\n",
    "    print(f\"\\n{treatment}:\")\n",
    "    treatment_data = df_pct[df_pct['treatment'] == treatment]\n",
    "    \n",
    "    for window_name in ['immediate', 'peak_sustained', 'post_6hr', 'next_day']:\n",
    "        window_data = treatment_data[treatment_data['window'] == window_name]\n",
    "        if len(window_data) > 0:\n",
    "            max_change = window_data.loc[window_data['pct_change'].abs().idxmax()]\n",
    "            print(f\"  {window_name:15s}: {max_change['feature']:15s} = {max_change['pct_change']:+6.1f}%\")\n",
    "\n",
    "# Close DuckDB\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde515c2-1653-4255-95db-75221aa80dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import duckdb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "#%% Configuration\n",
    "# S3 data location\n",
    "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular/\"\n",
    "\n",
    "# Dose mapping from DATA_DESCRIPTION.pdf\n",
    "DOSE_MAPPING = {\n",
    "    # Replicate 1\n",
    "    4917: '5 mg/kg', 4918: 'Vehicle', 4919: '25 mg/kg',\n",
    "    4920: '25 mg/kg', 4921: '5 mg/kg', 4922: 'Vehicle',\n",
    "    4923: 'Vehicle', 4924: '25 mg/kg', 4925: '5 mg/kg',\n",
    "    # Replicate 2\n",
    "    4926: '25 mg/kg', 4927: '5 mg/kg', 4928: 'Vehicle',\n",
    "    4929: 'Vehicle', 4930: '25 mg/kg', 4931: '5 mg/kg',\n",
    "    4932: '5 mg/kg', 4933: '25 mg/kg', 4934: 'Vehicle'\n",
    "}\n",
    "\n",
    "# Injection times from DATA_DESCRIPTION.pdf\n",
    "INJECTION_TIMES = {\n",
    "    'Rep1_Dose1': '2025-01-14 06:00:00',\n",
    "    'Rep1_Dose2': '2025-01-17 17:00:00',\n",
    "    'Rep2_Dose1': '2025-01-28 17:00:00',\n",
    "    'Rep2_Dose2': '2025-01-31 06:00:00'\n",
    "}\n",
    "\n",
    "# Time windows for analysis (minutes relative to injection)\n",
    "TIME_WINDOWS = {\n",
    "    'Early (0-30 min)': (0, 30),\n",
    "    'Onset (30-60 min)': (30, 60),\n",
    "    'Peak (60-180 min)': (60, 180),\n",
    "    'Sustained (180-360 min)': (180, 360)\n",
    "}\n",
    "\n",
    "#%% Connect to DuckDB and configure S3 access\n",
    "print(\"Connecting to DuckDB...\")\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Configure S3 for public access\n",
    "con.execute(\"SET s3_region='us-east-1';\")\n",
    "con.execute(\"SET s3_access_key_id='';\")\n",
    "con.execute(\"SET s3_secret_access_key='';\")\n",
    "\n",
    "print(\"✓ Connected to DuckDB with S3 access configured\")\n",
    "\n",
    "#%% Query animal_activity_db for behavioral metrics\n",
    "print(\"\\nQuerying behavioral metrics from animal_activity_db.parquet...\")\n",
    "\n",
    "# Example query for a subset of data - activity metrics at 60-second resolution\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    animal_id,\n",
    "    predicted_identity,\n",
    "    time,\n",
    "    name,\n",
    "    value,\n",
    "    resolution\n",
    "FROM read_parquet('{S3_BASE}cage_id=*/date=*/animal_activity_db.parquet')\n",
    "WHERE resolution = 60\n",
    "    AND name IN ('animal_bouts.active', 'animal_bouts.inactive', \n",
    "                 'animal_bouts.climbing', 'animal_bouts.locomotion')\n",
    "    AND date >= '2025-01-14' \n",
    "    AND date <= '2025-02-01'\n",
    "LIMIT 100000;\n",
    "\"\"\"\n",
    "\n",
    "df_activity = con.execute(query).fetchdf()\n",
    "print(f\"✓ Loaded {len(df_activity)} activity records\")\n",
    "print(f\"  Date range: {df_activity['time'].min()} to {df_activity['time'].max()}\")\n",
    "print(f\"  Cages: {df_activity['cage_id'].nunique()}\")\n",
    "print(f\"  Animals: {df_activity['animal_id'].nunique()}\")\n",
    "\n",
    "#%% Query distance traveled metrics\n",
    "print(\"\\nQuerying distance metrics from animal_aggs_short_id.parquet...\")\n",
    "\n",
    "query_distance = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    animal_id,\n",
    "    time,\n",
    "    name,\n",
    "    value,\n",
    "    resolution\n",
    "FROM read_parquet('{S3_BASE}cage_id=*/date=*/animal_aggs_short_id.parquet')\n",
    "WHERE resolution = 60\n",
    "    AND name = 'animal.distance_travelled'\n",
    "    AND date >= '2025-01-14'\n",
    "    AND date <= '2025-02-01'\n",
    "LIMIT 50000;\n",
    "\"\"\"\n",
    "\n",
    "df_distance = con.execute(query_distance).fetchdf()\n",
    "print(f\"✓ Loaded {len(df_distance)} distance records\")\n",
    "\n",
    "#%% Query respiration data\n",
    "print(\"\\nQuerying respiration metrics from animal_respiration.parquet...\")\n",
    "\n",
    "query_resp = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    animal_id,\n",
    "    predicted_identity,\n",
    "    time,\n",
    "    name,\n",
    "    value,\n",
    "    resolution\n",
    "FROM read_parquet('{S3_BASE}cage_id=*/date=*/animal_respiration.parquet')\n",
    "WHERE name = 'animal.respiration_rate_lucas_kanade_psd'\n",
    "    AND date >= '2025-01-14'\n",
    "    AND date <= '2025-02-01'\n",
    "LIMIT 20000;\n",
    "\"\"\"\n",
    "\n",
    "df_respiration = con.execute(query_resp).fetchdf()\n",
    "print(f\"✓ Loaded {len(df_respiration)} respiration records\")\n",
    "\n",
    "#%% Query social metrics\n",
    "print(\"\\nQuerying social metrics from animal_sociability_pairwise.parquet...\")\n",
    "\n",
    "query_social = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    name,\n",
    "    value,\n",
    "    animal_id_a,\n",
    "    animal_id_b\n",
    "FROM read_parquet('{S3_BASE}cage_id=*/date=*/animal_sociability_pairwise.parquet')\n",
    "WHERE date >= '2025-01-14'\n",
    "    AND date <= '2025-02-01'\n",
    "LIMIT 30000;\n",
    "\"\"\"\n",
    "\n",
    "df_social = con.execute(query_social).fetchdf()\n",
    "print(f\"✓ Loaded {len(df_social)} social interaction records\")\n",
    "\n",
    "#%% Add dose assignments\n",
    "print(\"\\nMapping cage IDs to dose groups...\")\n",
    "df_activity['dose'] = df_activity['cage_id'].map(DOSE_MAPPING)\n",
    "df_distance['dose'] = df_distance['cage_id'].map(DOSE_MAPPING)\n",
    "df_respiration['dose'] = df_respiration['cage_id'].map(DOSE_MAPPING)\n",
    "df_social['dose'] = df_social['cage_id'].map(DOSE_MAPPING)\n",
    "\n",
    "print(\"✓ Dose assignments complete\")\n",
    "\n",
    "#%% Function to calculate baseline and post-injection windows\n",
    "def calculate_effect_metrics(df, injection_time, metric_col='value', time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate effect size metrics comparing baseline to post-injection periods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Data for one animal/metric/dose/injection event\n",
    "    injection_time : str\n",
    "        Injection timestamp\n",
    "    metric_col : str\n",
    "        Column containing the metric values\n",
    "    time_col : str\n",
    "        Column containing timestamps\n",
    "    \"\"\"\n",
    "    inj_time = pd.to_datetime(injection_time)\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    \n",
    "    # Baseline: -30 to 0 minutes before injection\n",
    "    baseline = df[(df[time_col] >= inj_time - pd.Timedelta(minutes=30)) & \n",
    "                  (df[time_col] < inj_time)]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for window_name, (start_min, end_min) in TIME_WINDOWS.items():\n",
    "        # Post-injection window\n",
    "        post = df[(df[time_col] >= inj_time + pd.Timedelta(minutes=start_min)) & \n",
    "                  (df[time_col] < inj_time + pd.Timedelta(minutes=end_min))]\n",
    "        \n",
    "        if len(baseline) > 0 and len(post) > 0:\n",
    "            baseline_vals = baseline[metric_col].values\n",
    "            post_vals = post[metric_col].values\n",
    "            \n",
    "            # Calculate statistics\n",
    "            baseline_mean = np.mean(baseline_vals)\n",
    "            post_mean = np.mean(post_vals)\n",
    "            baseline_std = np.std(baseline_vals, ddof=1)\n",
    "            post_std = np.std(post_vals, ddof=1)\n",
    "            \n",
    "            # Cohen's d\n",
    "            pooled_std = np.sqrt(((len(baseline_vals)-1)*baseline_std**2 + \n",
    "                                  (len(post_vals)-1)*post_std**2) / \n",
    "                                 (len(baseline_vals) + len(post_vals) - 2))\n",
    "            cohens_d = (post_mean - baseline_mean) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            # Fold change (avoid division by zero)\n",
    "            fold_change = post_mean / baseline_mean if baseline_mean != 0 else 1.0\n",
    "            \n",
    "            # Effect size r\n",
    "            effect_size_r = cohens_d / np.sqrt(cohens_d**2 + 4) if not np.isnan(cohens_d) else 0\n",
    "            \n",
    "            # Wilcoxon signed-rank test (non-parametric)\n",
    "            try:\n",
    "                stat, p_value = stats.wilcoxon(baseline_vals, post_vals, \n",
    "                                               alternative='two-sided')\n",
    "            except:\n",
    "                p_value = 1.0\n",
    "            \n",
    "            results[window_name] = {\n",
    "                'cohens_d': cohens_d,\n",
    "                'fold_change': fold_change,\n",
    "                'effect_size_r': effect_size_r,\n",
    "                'p_value': p_value,\n",
    "                'baseline_mean': baseline_mean,\n",
    "                'post_mean': post_mean,\n",
    "                'n_baseline': len(baseline_vals),\n",
    "                'n_post': len(post_vals)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "#%% Analyze activity metrics\n",
    "print(\"\\nAnalyzing morphine effects on activity metrics...\")\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "# Process activity data\n",
    "for dose in df_activity['dose'].unique():\n",
    "    if pd.isna(dose):\n",
    "        continue\n",
    "    \n",
    "    for metric_name in df_activity['name'].unique():\n",
    "        df_subset = df_activity[(df_activity['dose'] == dose) & \n",
    "                                (df_activity['name'] == metric_name)]\n",
    "        \n",
    "        for animal_id in df_subset['animal_id'].unique():\n",
    "            df_animal = df_subset[df_subset['animal_id'] == animal_id]\n",
    "            cage_id = df_animal['cage_id'].iloc[0]\n",
    "            \n",
    "            # Determine which injection events apply to this cage\n",
    "            if cage_id <= 4925:  # Replicate 1\n",
    "                injection_events = ['Rep1_Dose1', 'Rep1_Dose2']\n",
    "            else:  # Replicate 2\n",
    "                injection_events = ['Rep2_Dose1', 'Rep2_Dose2']\n",
    "            \n",
    "            for event in injection_events:\n",
    "                metrics = calculate_effect_metrics(\n",
    "                    df_animal, \n",
    "                    INJECTION_TIMES[event]\n",
    "                )\n",
    "                \n",
    "                for window_name, stats in metrics.items():\n",
    "                    analysis_results.append({\n",
    "                        'category': 'Behavioral Bouts',\n",
    "                        'metric': metric_name.replace('animal_bouts.', ''),\n",
    "                        'dose': dose,\n",
    "                        'time_window': window_name,\n",
    "                        'animal_id': animal_id,\n",
    "                        'cage_id': cage_id,\n",
    "                        'injection_event': event,\n",
    "                        **stats\n",
    "                    })\n",
    "\n",
    "print(f\"✓ Analyzed {len(analysis_results)} dose-response comparisons\")\n",
    "\n",
    "#%% Create results DataFrame\n",
    "df_results = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Filter for significant effects\n",
    "df_sig = df_results[df_results['p_value'] < 0.05].copy()\n",
    "\n",
    "print(f\"\\nSignificant effects: {len(df_sig)} of {len(df_results)} \"\n",
    "      f\"({100*len(df_sig)/len(df_results):.1f}%)\")\n",
    "\n",
    "#%% Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal comparisons: {len(df_results)}\")\n",
    "print(f\"Significant effects (p < 0.05): {len(df_sig)}\")\n",
    "\n",
    "# By dose\n",
    "print(\"\\nSignificant effects by dose:\")\n",
    "for dose in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "    n_sig = len(df_sig[df_sig['dose'] == dose])\n",
    "    n_total = len(df_results[df_results['dose'] == dose])\n",
    "    if n_total > 0:\n",
    "        print(f\"  {dose:12s}: {n_sig:3d}/{n_total} ({100*n_sig/n_total:.1f}%)\")\n",
    "\n",
    "# By time window\n",
    "print(\"\\nSignificant effects by time window:\")\n",
    "for window in TIME_WINDOWS.keys():\n",
    "    n_sig = len(df_sig[df_sig['time_window'] == window])\n",
    "    n_total = len(df_results[df_results['time_window'] == window])\n",
    "    if n_total > 0:\n",
    "        print(f\"  {window:25s}: {n_sig:3d}/{n_total} ({100*n_sig/n_total:.1f}%)\")\n",
    "\n",
    "# Top effects\n",
    "if len(df_sig) > 0:\n",
    "    print(\"\\nTop 10 largest effects (Cohen's d):\")\n",
    "    top_effects = df_sig.nlargest(10, 'cohens_d')[\n",
    "        ['metric', 'dose', 'time_window', 'cohens_d', 'p_value']\n",
    "    ]\n",
    "    print(top_effects.to_string(index=False))\n",
    "\n",
    "#%% Helper function to create heatmap\n",
    "def create_heatmap(df, metric_col, metric_name, cmap='YlOrRd', vmax=None, \n",
    "                   fmt_func=lambda x: f\"{x:.2f}\", top_n=15):\n",
    "    \"\"\"\n",
    "    Create a heatmap for a specific metric across doses and time windows.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"No data available for {metric_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate average effect per metric\n",
    "    metric_avg = df.groupby('metric')[metric_col].mean().sort_values(ascending=False)\n",
    "    top_metrics = metric_avg.head(top_n).index.tolist()\n",
    "    \n",
    "    # Filter to top metrics\n",
    "    df_plot = df[df['metric'].isin(top_metrics)].copy()\n",
    "    \n",
    "    if len(df_plot) == 0:\n",
    "        print(f\"No data available for top metrics in {metric_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Create pivot table for each dose\n",
    "    doses = ['Vehicle', '5 mg/kg', '25 mg/kg']\n",
    "    time_windows = list(TIME_WINDOWS.keys())\n",
    "    \n",
    "    # Create figure with subplots for each dose\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 8), sharey=True)\n",
    "    fig.suptitle(f'{metric_name}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    for idx, dose in enumerate(doses):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create pivot table\n",
    "        df_dose = df_plot[df_plot['dose'] == dose]\n",
    "        \n",
    "        if len(df_dose) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for {dose}', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(dose, fontsize=14, fontweight='bold', pad=10)\n",
    "            continue\n",
    "        \n",
    "        pivot = df_dose.pivot_table(\n",
    "            values=metric_col,\n",
    "            index='metric',\n",
    "            columns='time_window',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to match time windows\n",
    "        available_windows = [w for w in time_windows if w in pivot.columns]\n",
    "        pivot = pivot[available_windows]\n",
    "        \n",
    "        # Reorder rows by average effect\n",
    "        row_order = pivot.mean(axis=1).sort_values(ascending=False).index\n",
    "        pivot = pivot.reindex(row_order)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=vmax if vmax else pivot.max().max(),\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cbar=idx == 2,  # Only show colorbar on last subplot\n",
    "            cbar_kws={'label': metric_name},\n",
    "            linewidths=0.5,\n",
    "            linecolor='white'\n",
    "        )\n",
    "        \n",
    "        ax.set_title(dose, fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.set_xlabel('Time Window', fontsize=11)\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Metric', fontsize=11)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        if idx > 0:\n",
    "            ax.set_yticklabels([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "#%% Generate Heatmaps\n",
    "\n",
    "if len(df_sig) > 0:\n",
    "    # Figure 1: Cohen's d Effect Size\n",
    "    print(\"\\nGenerating Figure 1: Cohen's d Effect Size...\")\n",
    "    fig1 = create_heatmap(\n",
    "        df_sig,\n",
    "        metric_col='cohens_d',\n",
    "        metric_name=\"Cohen's d Effect Size\",\n",
    "        cmap='YlOrRd',\n",
    "        vmax=12,\n",
    "        top_n=15\n",
    "    )\n",
    "    if fig1:\n",
    "        plt.show()\n",
    "    \n",
    "    # Figure 2: Fold Change\n",
    "    print(\"\\nGenerating Figure 2: Fold Change...\")\n",
    "    fig2 = create_heatmap(\n",
    "        df_sig,\n",
    "        metric_col='fold_change',\n",
    "        metric_name='Fold Change from Baseline',\n",
    "        cmap='YlOrRd',\n",
    "        vmax=10,\n",
    "        fmt_func=lambda x: f\"{x:.1f}x\",\n",
    "        top_n=15\n",
    "    )\n",
    "    if fig2:\n",
    "        plt.show()\n",
    "    \n",
    "    # Figure 3: Effect Size r\n",
    "    print(\"\\nGenerating Figure 3: Effect Size r...\")\n",
    "    fig3 = create_heatmap(\n",
    "        df_sig,\n",
    "        metric_col='effect_size_r',\n",
    "        metric_name='Effect Size r (Correlation)',\n",
    "        cmap='YlOrRd',\n",
    "        vmax=1.0,\n",
    "        top_n=15\n",
    "    )\n",
    "    if fig3:\n",
    "        plt.show()\n",
    "    \n",
    "    # Figure 4: Statistical Significance\n",
    "    print(\"\\nGenerating Figure 4: Statistical Significance...\")\n",
    "    df_sig['neg_log10_p'] = -np.log10(df_sig['p_value'])\n",
    "    fig4 = create_heatmap(\n",
    "        df_sig,\n",
    "        metric_col='neg_log10_p',\n",
    "        metric_name='-log10(p-value)',\n",
    "        cmap='viridis',\n",
    "        vmax=3,\n",
    "        top_n=15\n",
    "    )\n",
    "    if fig4:\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo significant effects found to plot. Try increasing sample size or date range.\")\n",
    "\n",
    "#%% Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving results to CSV...\")\n",
    "df_results.to_csv('morphine_effects_all.csv', index=False)\n",
    "df_sig.to_csv('morphine_effects_significant.csv', index=False)\n",
    "print(\"✓ Saved: morphine_effects_all.csv\")\n",
    "print(\"✓ Saved: morphine_effects_significant.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: This analysis used a SAMPLE of the data (LIMIT queries).\")\n",
    "print(\"For complete analysis, remove LIMIT clauses and expand date range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20feeb54-25b5-4423-8eb3-9259da476258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration from 180B study\n",
    "TREATMENT_MAP = {\n",
    "    4917: '5 mg/kg', 4918: 'Vehicle', 4919: '25 mg/kg',\n",
    "    4920: '25 mg/kg', 4921: '5 mg/kg', 4922: 'Vehicle',\n",
    "    4923: 'Vehicle', 4924: '25 mg/kg', 4925: '5 mg/kg'\n",
    "}\n",
    "\n",
    "# Replicate 1: Dose 1 at 6 AM (Jan 14)\n",
    "INJECTION_DATETIME = datetime(2025, 1, 14, 6, 0, 0)\n",
    "\n",
    "# CORRECTED: Use actual observed time windows from OUR data\n",
    "# Note: Our cage-level data shows delayed effect vs 180B individual animals\n",
    "TIME_WINDOWS = {\n",
    "    'baseline': (-30, 0),          # Pre-injection baseline\n",
    "    'early': (0, 120),             # Early phase (minimal effect)\n",
    "    'onset': (120, 240),           # Rise phase (2-4 hours)\n",
    "    'peak': (240, 420),            # PRIMARY ANALYSIS WINDOW (4-7 hours - observed plateau)\n",
    "    'late': (420, 540),            # Extended/decline (7-9 hours)\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MORPHINE BEHAVIORAL ANALYSIS - 180B VALIDATED PROTOCOL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nInjection time: {INJECTION_DATETIME.strftime('%Y-%m-%d %H:%M:%S')} (6 AM - light phase)\")\n",
    "print(\"\\nTime windows (adjusted for cage-level data):\")\n",
    "for name, (start, end) in TIME_WINDOWS.items():\n",
    "    print(f\"  {name:12s}: {start:4d} to {end:4d} minutes ({start/60:.1f}-{end/60:.1f} hrs)\")\n",
    "print(\"\\nPRIMARY WINDOW: 240-420 min (4-7 hrs - observed peak in cage-level data)\")\n",
    "print(\"NOTE: This is LATER than 180B individual animal data due to cage aggregation\")\n",
    "\n",
    "# Initialize DuckDB\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_region='us-east-1';\")\n",
    "con.execute(\"SET s3_url_style='path';\")\n",
    "\n",
    "def generate_paths(cages, dates, filename):\n",
    "    paths = []\n",
    "    for cage in cages:\n",
    "        for date in dates:\n",
    "            path = f\"s3://jax-envision-public-data/study_1001/2025v3.3/tabular/cage_id={cage}/date={date}/{filename}\"\n",
    "            paths.append(f\"'{path}'\")\n",
    "    return ', '.join(paths)\n",
    "\n",
    "# Load comprehensive behavioral metrics\n",
    "dates = [(INJECTION_DATETIME + timedelta(days=d)).strftime('%Y-%m-%d') \n",
    "         for d in range(0, 2)]  # Day of injection + next day\n",
    "cages = list(TREATMENT_MAP.keys())\n",
    "\n",
    "print(f\"\\nLoading behavioral metrics for {len(cages)} cages on {dates}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MULTIPLE BEHAVIORAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "all_features = {}\n",
    "\n",
    "# 1. Activity states (like 180B locomotion)\n",
    "print(\"\\n1. Loading activity states (locomotion, active, inactive, climbing)...\")\n",
    "activity_paths = generate_paths(cages, dates, 'animal_activity_db.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    name,\n",
    "    value\n",
    "FROM read_parquet([{activity_paths}])\n",
    "WHERE resolution = 60\n",
    "    AND name IN ('animal_bouts.locomotion', 'animal_bouts.active', \n",
    "                 'animal_bouts.inactive', 'animal_bouts.climbing',\n",
    "                 'animal_bouts.drinking', 'animal_bouts.feeding')\n",
    "\"\"\"\n",
    "df_activity = con.execute(query).df()\n",
    "df_activity['time'] = pd.to_datetime(df_activity['time'])\n",
    "df_activity['minutes_from_injection'] = (df_activity['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# Pivot to wide format\n",
    "activity_pivot = df_activity.pivot_table(\n",
    "    index=['cage_id', 'minutes_from_injection'],\n",
    "    columns='name',\n",
    "    values='value',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "activity_pivot.columns.name = None\n",
    "\n",
    "# Rename columns\n",
    "activity_pivot = activity_pivot.rename(columns={\n",
    "    'animal_bouts.locomotion': 'locomotion',\n",
    "    'animal_bouts.active': 'active',\n",
    "    'animal_bouts.inactive': 'inactive',\n",
    "    'animal_bouts.climbing': 'climbing',\n",
    "    'animal_bouts.drinking': 'drinking',\n",
    "    'animal_bouts.feeding': 'feeding'\n",
    "})\n",
    "\n",
    "# 2. Distance traveled\n",
    "print(\"2. Loading distance metrics...\")\n",
    "distance_paths = generate_paths(cages, dates, 'animal_aggs_short_id.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as distance_travelled\n",
    "FROM read_parquet([{distance_paths}])\n",
    "WHERE name = 'animal.distance_travelled' AND resolution = 60\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_distance = con.execute(query).df()\n",
    "df_distance['time'] = pd.to_datetime(df_distance['time'])\n",
    "df_distance['minutes_from_injection'] = (df_distance['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# 3. Respiration\n",
    "print(\"3. Loading respiration rates...\")\n",
    "resp_paths = generate_paths(cages, dates, 'animal_respiration.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as respiration_rate\n",
    "FROM read_parquet([{resp_paths}])\n",
    "WHERE name = 'animal.respiration_rate_lucas_kanade_psd'\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_respiration = con.execute(query).df()\n",
    "df_respiration['time'] = pd.to_datetime(df_respiration['time'])\n",
    "df_respiration['minutes_from_injection'] = (df_respiration['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# 4. Motion scores\n",
    "print(\"4. Loading motion scores...\")\n",
    "motion_paths = generate_paths(cages, dates, 'cage_motion_vector.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as motion_score\n",
    "FROM read_parquet([{motion_paths}])\n",
    "WHERE resolution = 60\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_motion = con.execute(query).df()\n",
    "df_motion['time'] = pd.to_datetime(df_motion['time'])\n",
    "df_motion['minutes_from_injection'] = (df_motion['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# Merge all features\n",
    "print(\"\\nMerging all behavioral features...\")\n",
    "df = activity_pivot.merge(df_distance, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df = df.merge(df_respiration, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df = df.merge(df_motion, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df['treatment'] = df['cage_id'].map(TREATMENT_MAP)\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} records\")\n",
    "\n",
    "# Get only numeric feature columns (exclude cage_id, minutes, treatment, and any datetime columns)\n",
    "feature_cols = [c for c in df.columns \n",
    "                if c not in ['cage_id', 'minutes_from_injection', 'treatment'] \n",
    "                and df[c].dtype in ['float64', 'float32', 'int64', 'int32']]\n",
    "print(f\"  Features: {feature_cols}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASELINE NORMALIZATION (per 180B protocol)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE NORMALIZATION (180B Protocol)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Each cage normalized to its own -30 to 0 min baseline\")\n",
    "\n",
    "# Calculate baseline for each cage\n",
    "baseline_window = TIME_WINDOWS['baseline']\n",
    "baseline_data = df[\n",
    "    (df['minutes_from_injection'] >= baseline_window[0]) & \n",
    "    (df['minutes_from_injection'] < baseline_window[1])\n",
    "]\n",
    "\n",
    "cage_baselines = {}\n",
    "for feature in feature_cols:\n",
    "    cage_baselines[feature] = baseline_data.groupby('cage_id')[feature].mean()\n",
    "\n",
    "print(\"\\nBaseline means per cage (locomotion):\")\n",
    "for cage_id in sorted(cage_baselines['locomotion'].index):\n",
    "    if not pd.isna(cage_baselines['locomotion'][cage_id]):\n",
    "        treatment = TREATMENT_MAP[cage_id]\n",
    "        print(f\"  Cage {cage_id} ({treatment:10s}): {cage_baselines['locomotion'][cage_id]:.4f}\")\n",
    "\n",
    "# Normalize all features\n",
    "for feature in feature_cols:\n",
    "    df[f'{feature}_baseline'] = df['cage_id'].map(cage_baselines[feature])\n",
    "    df[f'{feature}_fold_change'] = df[feature] / df[f'{feature}_baseline']\n",
    "    df[f'{feature}_pct_change'] = (df[f'{feature}_fold_change'] - 1) * 100\n",
    "\n",
    "# ============================================================================\n",
    "# WILCOXON SIGNED-RANK TESTS (like 180B)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WILCOXON SIGNED-RANK TESTS (180B Protocol)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "wilcoxon_results = []\n",
    "\n",
    "for window_name, (start, end) in TIME_WINDOWS.items():\n",
    "    if window_name == 'baseline':\n",
    "        continue\n",
    "    \n",
    "    window_data = df[\n",
    "        (df['minutes_from_injection'] >= start) & \n",
    "        (df['minutes_from_injection'] < end)\n",
    "    ]\n",
    "    \n",
    "    baseline_data = df[\n",
    "        (df['minutes_from_injection'] >= baseline_window[0]) & \n",
    "        (df['minutes_from_injection'] < baseline_window[1])\n",
    "    ]\n",
    "    \n",
    "    for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "        for feature in feature_cols:\n",
    "            # Get cage-level means\n",
    "            post_means = window_data[window_data['treatment'] == treatment].groupby('cage_id')[feature].mean()\n",
    "            pre_means = baseline_data[baseline_data['treatment'] == treatment].groupby('cage_id')[feature].mean()\n",
    "            \n",
    "            # Align cages\n",
    "            common_cages = list(set(post_means.index) & set(pre_means.index))\n",
    "            if len(common_cages) < 2:\n",
    "                continue\n",
    "            \n",
    "            post_vals = post_means.loc[common_cages].values\n",
    "            pre_vals = pre_means.loc[common_cages].values\n",
    "            \n",
    "            # Remove NaN pairs\n",
    "            mask = ~(np.isnan(post_vals) | np.isnan(pre_vals))\n",
    "            post_vals = post_vals[mask]\n",
    "            pre_vals = pre_vals[mask]\n",
    "            \n",
    "            if len(post_vals) < 2 or np.all(pre_vals == 0):\n",
    "                continue\n",
    "            \n",
    "            # Wilcoxon signed-rank test\n",
    "            try:\n",
    "                stat, p_val = stats.wilcoxon(post_vals, pre_vals, alternative='two-sided')\n",
    "                \n",
    "                # Calculate effect sizes\n",
    "                fold_change = np.mean(post_vals) / np.mean(pre_vals) if np.mean(pre_vals) > 0 else np.nan\n",
    "                \n",
    "                # Cohen's d\n",
    "                diffs = post_vals - pre_vals\n",
    "                cohens_d = np.mean(diffs) / np.std(diffs) if np.std(diffs) > 0 else np.nan\n",
    "                \n",
    "                # Rank-biserial correlation (effect size r)\n",
    "                n = len(post_vals)\n",
    "                W = stat\n",
    "                r = 1 - (4 * W) / (n * (n + 1))\n",
    "                \n",
    "                wilcoxon_results.append({\n",
    "                    'window': window_name,\n",
    "                    'treatment': treatment,\n",
    "                    'feature': feature,\n",
    "                    'n_cages': len(post_vals),\n",
    "                    'pre_mean': np.mean(pre_vals),\n",
    "                    'post_mean': np.mean(post_vals),\n",
    "                    'fold_change': fold_change,\n",
    "                    'p_value': p_val,\n",
    "                    'cohens_d': cohens_d,\n",
    "                    'effect_size_r': r\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "results_df = pd.DataFrame(wilcoxon_results)\n",
    "\n",
    "# ============================================================================\n",
    "# HEATMAP: PRIMARY WINDOW (60-180 min)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING HEATMAP - PRIMARY WINDOW (60-180 min)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter for peak window only\n",
    "peak_results = results_df[results_df['window'] == 'peak'].copy()\n",
    "\n",
    "# Add significance markers\n",
    "peak_results['sig'] = peak_results['p_value'].apply(\n",
    "    lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    ")\n",
    "\n",
    "# Calculate log10 p-value for visualization\n",
    "peak_results['neg_log10_p'] = -np.log10(peak_results['p_value'].clip(lower=1e-10))\n",
    "\n",
    "# Create comprehensive heatmap\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 18))\n",
    "\n",
    "for idx, treatment in enumerate(['Vehicle', '5 mg/kg', '25 mg/kg']):\n",
    "    treatment_data = peak_results[peak_results['treatment'] == treatment]\n",
    "    \n",
    "    if len(treatment_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create pivot for fold change\n",
    "    pivot = treatment_data.pivot_table(\n",
    "        index='feature',\n",
    "        columns='treatment',\n",
    "        values='fold_change',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Sort by absolute effect size (Cohen's d)\n",
    "    feature_importance = treatment_data.set_index('feature')['cohens_d'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    if len(feature_importance) > 0:\n",
    "        pivot = pivot.reindex(feature_importance.index)\n",
    "        \n",
    "        # Prepare data for single treatment column\n",
    "        plot_data = pivot[[treatment]].values.flatten()\n",
    "        \n",
    "        # Create heatmap\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Determine color scale\n",
    "        vmax = max(3, np.nanmax(np.abs(np.log2(plot_data[~np.isnan(plot_data)]))))\n",
    "        \n",
    "        # Convert to log2 fold change for better visualization\n",
    "        log2_fc = np.log2(plot_data.reshape(-1, 1))\n",
    "        \n",
    "        sns.heatmap(log2_fc,\n",
    "                   ax=ax,\n",
    "                   cmap='RdBu_r',\n",
    "                   center=0,\n",
    "                   vmin=-vmax, vmax=vmax,\n",
    "                   cbar_kws={'label': 'log2(Fold Change)'},\n",
    "                   yticklabels=feature_importance.index,\n",
    "                   xticklabels=[],\n",
    "                   linewidths=0.5,\n",
    "                   annot=True,\n",
    "                   fmt='.2f',\n",
    "                   annot_kws={'size': 9})\n",
    "        \n",
    "        # Add significance stars\n",
    "        for i, feature in enumerate(feature_importance.index):\n",
    "            feature_data = treatment_data[treatment_data['feature'] == feature]\n",
    "            if len(feature_data) > 0:\n",
    "                sig = feature_data['sig'].iloc[0]\n",
    "                if sig:\n",
    "                    ax.text(0.5, i + 0.5, sig, ha='center', va='center',\n",
    "                           fontsize=14, fontweight='bold', color='black')\n",
    "        \n",
    "        # Add effect size info\n",
    "        ax2 = ax.twinx()\n",
    "        effect_sizes = [treatment_data[treatment_data['feature'] == f]['cohens_d'].iloc[0] \n",
    "                       if len(treatment_data[treatment_data['feature'] == f]) > 0 else 0\n",
    "                       for f in feature_importance.index]\n",
    "        ax2.set_ylim(ax.get_ylim())\n",
    "        ax2.set_yticks(np.arange(len(feature_importance)) + 0.5)\n",
    "        ax2.set_yticklabels([f\"d={d:.1f}\" if not np.isnan(d) else \"\" \n",
    "                            for d in effect_sizes], fontsize=8)\n",
    "        ax2.set_ylabel(\"Cohen's d\", fontsize=10)\n",
    "        \n",
    "        title = f\"{treatment} - Peak Window (240-420 min = 4-7 hours post-injection)\"\n",
    "        title += f\"\\nInjection: Jan 14, 2025 6:00 AM (light phase)\"\n",
    "        title += f\"\\nNote: Peak delayed vs 180B due to cage-level aggregation\"\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
    "        ax.set_ylabel('Behavioral Feature', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('morphine_180B_validated_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: morphine_180B_validated_heatmap.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY TABLE (like 180B)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE - PEAK WINDOW (240-420 min = 4-7 hours)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = peak_results.sort_values(['treatment', 'cohens_d'], ascending=[True, False])\n",
    "summary_display = summary[['treatment', 'feature', 'fold_change', 'p_value', 'cohens_d', 'effect_size_r', 'sig']]\n",
    "\n",
    "print(\"\\nTop features by treatment (sorted by effect size):\")\n",
    "for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "    print(f\"\\n{treatment}:\")\n",
    "    treatment_summary = summary_display[summary_display['treatment'] == treatment].head(5)\n",
    "    print(treatment_summary.to_string(index=False))\n",
    "\n",
    "# Print expected 180B findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION AGAINST 180B FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "locomotion_results = peak_results[peak_results['feature'] == 'locomotion']\n",
    "print(\"\\nLocomotor activation (primary finding from 180B):\")\n",
    "for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "    loco = locomotion_results[locomotion_results['treatment'] == treatment]\n",
    "    if len(loco) > 0:\n",
    "        fc = loco['fold_change'].iloc[0]\n",
    "        p = loco['p_value'].iloc[0]\n",
    "        d = loco['cohens_d'].iloc[0]\n",
    "        r = loco['effect_size_r'].iloc[0]\n",
    "        print(f\"  {treatment:10s}: {fc:.1f}x fold change, p={p:.4f}, d={d:.2f}, r={r:.2f}\")\n",
    "\n",
    "print(\"\\n180B Expected Results (individual animals, 60-180 min):\")\n",
    "print(\"  Vehicle:  ~1x (no change, p > 0.05)\")\n",
    "print(\"  5 mg/kg:  2-8x (moderate, p < 0.05)\")\n",
    "print(\"  25 mg/kg: 7-10x (large, p < 0.004), d=4-11, r≈1.0\")\n",
    "\n",
    "print(\"\\nOUR Results (cage-level, 240-420 min):\")\n",
    "print(\"  Expected to show similar pattern but delayed time course\")\n",
    "print(\"  Due to cage aggregation and data resolution differences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTES:\")\n",
    "print(\"  - Analysis uses 6 AM injection (light phase) - no circadian confound\")\n",
    "print(\"  - Peak window (240-420 min = 4-7 hrs) captures observed plateau\")\n",
    "print(\"  - DELAYED vs 180B study due to:\")\n",
    "print(\"    * Cage-level aggregation (3 animals/cage)\")\n",
    "print(\"    * Different data resolution/processing\")\n",
    "print(\"  - Baseline normalization controls for pre-existing differences\")\n",
    "print(\"  - Wilcoxon test appropriate for n=3 cages per group\")\n",
    "print(\"  - Log2 fold change: +1 = 2x increase, +2 = 4x increase, +3 = 8x increase\")\n",
    "print(\"  - Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4e6f2-7f3a-4321-847c-d3981927c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "TREATMENT_MAP = {\n",
    "    4917: '5 mg/kg', 4918: 'Vehicle', 4919: '25 mg/kg',\n",
    "    4920: '25 mg/kg', 4921: '5 mg/kg', 4922: 'Vehicle',\n",
    "    4923: 'Vehicle', 4924: '25 mg/kg', 4925: '5 mg/kg'\n",
    "}\n",
    "\n",
    "INJECTION_DATETIME = datetime(2025, 1, 14, 6, 0, 0)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC: WHY RESULTS DON'T MATCH 180B STUDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize DuckDB\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_region='us-east-1';\")\n",
    "con.execute(\"SET s3_url_style='path';\")\n",
    "\n",
    "def generate_paths(cages, dates, filename):\n",
    "    paths = []\n",
    "    for cage in cages:\n",
    "        for date in dates:\n",
    "            path = f\"s3://jax-envision-public-data/study_1001/2025v3.3/tabular/cage_id={cage}/date={date}/{filename}\"\n",
    "            paths.append(f\"'{path}'\")\n",
    "    return ', '.join(paths)\n",
    "\n",
    "# Load data from injection day only\n",
    "dates = [INJECTION_DATETIME.strftime('%Y-%m-%d')]\n",
    "cages = list(TREATMENT_MAP.keys())\n",
    "\n",
    "print(f\"\\nAttempting to load data for:\")\n",
    "print(f\"  Cages: {cages}\")\n",
    "print(f\"  Date: {dates[0]}\")\n",
    "print(f\"  Injection time: {INJECTION_DATETIME}\")\n",
    "\n",
    "# Load locomotion data\n",
    "print(\"\\nLoading locomotion data...\")\n",
    "activity_paths = generate_paths(cages, dates, 'animal_activity_db.parquet')\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    name,\n",
    "    value,\n",
    "    resolution\n",
    "FROM read_parquet([{activity_paths}])\n",
    "WHERE name = 'animal_bouts.locomotion'\n",
    "    AND resolution = 60\n",
    "ORDER BY cage_id, time\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df = con.execute(query).df()\n",
    "    print(f\"✓ Loaded {len(df)} records\")\n",
    "    print(f\"  Unique cages: {sorted(df['cage_id'].unique())}\")\n",
    "    print(f\"  Missing cages: {set(cages) - set(df['cage_id'].unique())}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    con.close()\n",
    "    exit()\n",
    "\n",
    "# Convert time\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['minutes_from_injection'] = (df['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "df['hours_from_injection'] = df['minutes_from_injection'] / 60\n",
    "df['treatment'] = df['cage_id'].map(TREATMENT_MAP)\n",
    "\n",
    "print(f\"\\nTime range in data:\")\n",
    "print(f\"  Min: {df['hours_from_injection'].min():.1f} hours\")\n",
    "print(f\"  Max: {df['hours_from_injection'].max():.1f} hours\")\n",
    "print(f\"  Range around injection: {df[(df['hours_from_injection'] >= -1) & (df['hours_from_injection'] <= 6)].shape[0]} records\")\n",
    "\n",
    "# Check data availability around critical windows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "windows = {\n",
    "    'baseline': (-0.5, 0),\n",
    "    'early': (0, 0.5),\n",
    "    'onset': (0.5, 1),\n",
    "    'peak': (1, 3),\n",
    "    'sustained': (2, 4),\n",
    "}\n",
    "\n",
    "for window_name, (start, end) in windows.items():\n",
    "    window_data = df[(df['hours_from_injection'] >= start) & (df['hours_from_injection'] < end)]\n",
    "    print(f\"\\n{window_name.upper()} window ({start}-{end} hours):\")\n",
    "    \n",
    "    for cage_id in sorted(cages):\n",
    "        cage_window = window_data[window_data['cage_id'] == cage_id]\n",
    "        treatment = TREATMENT_MAP[cage_id]\n",
    "        \n",
    "        if len(cage_window) > 0:\n",
    "            mean_val = cage_window['value'].mean()\n",
    "            n_records = len(cage_window)\n",
    "            print(f\"  Cage {cage_id} ({treatment:10s}): {n_records:3d} records, mean={mean_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Cage {cage_id} ({treatment:10s}): *** NO DATA ***\")\n",
    "\n",
    "# Plot individual cage trajectories\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INDIVIDUAL CAGE TRAJECTORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, cage_id in enumerate(sorted(cages)):\n",
    "    cage_data = df[df['cage_id'] == cage_id].sort_values('hours_from_injection')\n",
    "    treatment = TREATMENT_MAP[cage_id]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if len(cage_data) > 0:\n",
    "        ax.plot(cage_data['hours_from_injection'], cage_data['value'], \n",
    "                linewidth=1, alpha=0.5, color='lightblue')\n",
    "        \n",
    "        # Add rolling mean\n",
    "        cage_data['rolling'] = cage_data['value'].rolling(window=10, center=True).mean()\n",
    "        ax.plot(cage_data['hours_from_injection'], cage_data['rolling'], \n",
    "                linewidth=2, color='darkblue', label='10-min avg')\n",
    "        \n",
    "        # Mark injection and key windows\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Injection')\n",
    "        ax.axvspan(-0.5, 0, alpha=0.1, color='green', label='Baseline')\n",
    "        ax.axvspan(1, 3, alpha=0.1, color='orange', label='Peak')\n",
    "        \n",
    "        # Calculate baseline and peak means\n",
    "        baseline = cage_data[(cage_data['hours_from_injection'] >= -0.5) & \n",
    "                            (cage_data['hours_from_injection'] < 0)]\n",
    "        peak = cage_data[(cage_data['hours_from_injection'] >= 1) & \n",
    "                        (cage_data['hours_from_injection'] < 3)]\n",
    "        \n",
    "        if len(baseline) > 0 and len(peak) > 0:\n",
    "            baseline_mean = baseline['value'].mean()\n",
    "            peak_mean = peak['value'].mean()\n",
    "            fold_change = peak_mean / baseline_mean if baseline_mean > 0 else np.nan\n",
    "            \n",
    "            title = f\"Cage {cage_id} - {treatment}\\n\"\n",
    "            title += f\"Baseline: {baseline_mean:.3f}, Peak: {peak_mean:.3f}\\n\"\n",
    "            title += f\"Fold Change: {fold_change:.2f}x\"\n",
    "        else:\n",
    "            title = f\"Cage {cage_id} - {treatment}\\nInsufficient data\"\n",
    "        \n",
    "        ax.set_title(title, fontsize=9, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'Cage {cage_id}\\nNO DATA', \n",
    "               ha='center', va='center', fontsize=12, color='red')\n",
    "        ax.set_title(f\"Cage {cage_id} - {treatment}\", fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Hours from Injection', fontsize=8)\n",
    "    ax.set_ylabel('Locomotion', fontsize=8)\n",
    "    ax.set_xlim(-1, 8)\n",
    "    ax.set_ylim(0, max(1, df['value'].max() * 1.1))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=6, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('diagnostic_individual_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Saved: diagnostic_individual_trajectories.png\")\n",
    "\n",
    "# Compare to 180B expected pattern\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON TO 180B EXPECTED PATTERN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n180B Study Results (for reference):\")\n",
    "print(\"  25 mg/kg at 6 AM injection:\")\n",
    "print(\"    - Baseline (~0-30 min before): ~0.10\")\n",
    "print(\"    - Peak (60-180 min after): ~0.77\")\n",
    "print(\"    - Fold change: ~7.7x\")\n",
    "print(\"    - Effect size: Cohen's d = 4-11\")\n",
    "\n",
    "print(\"\\nOUR RESULTS:\")\n",
    "\n",
    "for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "    treatment_data = df[df['treatment'] == treatment]\n",
    "    \n",
    "    baseline = treatment_data[\n",
    "        (treatment_data['hours_from_injection'] >= -0.5) & \n",
    "        (treatment_data['hours_from_injection'] < 0)\n",
    "    ]\n",
    "    \n",
    "    peak = treatment_data[\n",
    "        (treatment_data['hours_from_injection'] >= 1) & \n",
    "        (treatment_data['hours_from_injection'] < 3)\n",
    "    ]\n",
    "    \n",
    "    if len(baseline) > 0 and len(peak) > 0:\n",
    "        # Get cage-level means\n",
    "        baseline_means = baseline.groupby('cage_id')['value'].mean()\n",
    "        peak_means = peak.groupby('cage_id')['value'].mean()\n",
    "        \n",
    "        common_cages = list(set(baseline_means.index) & set(peak_means.index))\n",
    "        \n",
    "        if len(common_cages) > 0:\n",
    "            baseline_vals = baseline_means.loc[common_cages].values\n",
    "            peak_vals = peak_means.loc[common_cages].values\n",
    "            \n",
    "            fold_changes = peak_vals / baseline_vals\n",
    "            \n",
    "            print(f\"\\n{treatment}:\")\n",
    "            print(f\"  N cages: {len(common_cages)}\")\n",
    "            print(f\"  Baseline mean: {baseline_vals.mean():.4f} ± {baseline_vals.std():.4f}\")\n",
    "            print(f\"  Peak mean: {peak_vals.mean():.4f} ± {peak_vals.std():.4f}\")\n",
    "            print(f\"  Fold changes: {fold_changes}\")\n",
    "            print(f\"  Mean fold change: {fold_changes.mean():.2f}x\")\n",
    "\n",
    "# Check if we're looking at the right metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING OTHER METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query_check = f\"\"\"\n",
    "SELECT DISTINCT name\n",
    "FROM read_parquet([{activity_paths}])\n",
    "ORDER BY name\n",
    "\"\"\"\n",
    "available_metrics = con.execute(query_check).df()\n",
    "print(\"\\nAvailable metrics in animal_activity_db:\")\n",
    "print(available_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POTENTIAL ISSUES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Missing data for cage 4920 (25 mg/kg)?\")\n",
    "print(\"2. Using wrong time window for baseline/peak?\")\n",
    "print(\"3. Resolution issue (60s vs other)?\")\n",
    "print(\"4. Wrong file? Should we use animal_aggs_short_id instead?\")\n",
    "print(\"5. Data aggregation issue at cage level?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66157f02-e206-4a2d-9531-63da16174244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration from 180B study\n",
    "TREATMENT_MAP = {\n",
    "    4917: '5 mg/kg', 4918: 'Vehicle', 4919: '25 mg/kg',\n",
    "    4920: '25 mg/kg', 4921: '5 mg/kg', 4922: 'Vehicle',\n",
    "    4923: 'Vehicle', 4924: '25 mg/kg', 4925: '5 mg/kg'\n",
    "}\n",
    "\n",
    "# Replicate 1: Dose 1 at 6 AM (Jan 14)\n",
    "INJECTION_DATETIME = datetime(2025, 1, 14, 6, 0, 0)\n",
    "\n",
    "# CORRECTED: Use actual observed time windows from OUR data\n",
    "TIME_WINDOWS = {\n",
    "    'baseline': (-30, 0),          # Pre-injection baseline\n",
    "    'early': (0, 120),             # Early phase (minimal effect)\n",
    "    'onset': (120, 240),           # Rise phase (2-4 hours)\n",
    "    'peak': (240, 420),            # PRIMARY ANALYSIS WINDOW (4-7 hours - observed plateau)\n",
    "    'late': (420, 540),            # Extended/decline (7-9 hours)\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MORPHINE BEHAVIORAL ANALYSIS - ALL TIME WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nInjection time: {INJECTION_DATETIME.strftime('%Y-%m-%d %H:%M:%S')} (6 AM - light phase)\")\n",
    "print(\"\\nTime windows (adjusted for cage-level data):\")\n",
    "for name, (start, end) in TIME_WINDOWS.items():\n",
    "    print(f\"  {name:12s}: {start:4d} to {end:4d} minutes ({start/60:.1f}-{end/60:.1f} hrs)\")\n",
    "\n",
    "# Initialize DuckDB\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_region='us-east-1';\")\n",
    "con.execute(\"SET s3_url_style='path';\")\n",
    "\n",
    "def generate_paths(cages, dates, filename):\n",
    "    paths = []\n",
    "    for cage in cages:\n",
    "        for date in dates:\n",
    "            path = f\"s3://jax-envision-public-data/study_1001/2025v3.3/tabular/cage_id={cage}/date={date}/{filename}\"\n",
    "            paths.append(f\"'{path}'\")\n",
    "    return ', '.join(paths)\n",
    "\n",
    "# Load comprehensive behavioral metrics\n",
    "dates = [(INJECTION_DATETIME + timedelta(days=d)).strftime('%Y-%m-%d') \n",
    "         for d in range(0, 2)]  # Day of injection + next day\n",
    "cages = list(TREATMENT_MAP.keys())\n",
    "\n",
    "print(f\"\\nLoading behavioral metrics for {len(cages)} cages on {dates}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MULTIPLE BEHAVIORAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "all_features = {}\n",
    "\n",
    "# 1. Activity states (like 180B locomotion)\n",
    "print(\"\\n1. Loading activity states (locomotion, active, inactive, climbing)...\")\n",
    "activity_paths = generate_paths(cages, dates, 'animal_activity_db.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    name,\n",
    "    value\n",
    "FROM read_parquet([{activity_paths}])\n",
    "WHERE resolution = 60\n",
    "    AND name IN ('animal_bouts.locomotion', 'animal_bouts.active', \n",
    "                 'animal_bouts.inactive', 'animal_bouts.climbing',\n",
    "                 'animal_bouts.drinking', 'animal_bouts.feeding')\n",
    "\"\"\"\n",
    "df_activity = con.execute(query).df()\n",
    "df_activity['time'] = pd.to_datetime(df_activity['time'])\n",
    "df_activity['minutes_from_injection'] = (df_activity['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# Pivot to wide format\n",
    "activity_pivot = df_activity.pivot_table(\n",
    "    index=['cage_id', 'minutes_from_injection'],\n",
    "    columns='name',\n",
    "    values='value',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "activity_pivot.columns.name = None\n",
    "\n",
    "# Rename columns\n",
    "activity_pivot = activity_pivot.rename(columns={\n",
    "    'animal_bouts.locomotion': 'locomotion',\n",
    "    'animal_bouts.active': 'active',\n",
    "    'animal_bouts.inactive': 'inactive',\n",
    "    'animal_bouts.climbing': 'climbing',\n",
    "    'animal_bouts.drinking': 'drinking',\n",
    "    'animal_bouts.feeding': 'feeding'\n",
    "})\n",
    "\n",
    "# 2. Distance traveled\n",
    "print(\"2. Loading distance metrics...\")\n",
    "distance_paths = generate_paths(cages, dates, 'animal_aggs_short_id.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as distance_travelled\n",
    "FROM read_parquet([{distance_paths}])\n",
    "WHERE name = 'animal.distance_travelled' AND resolution = 60\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_distance = con.execute(query).df()\n",
    "df_distance['time'] = pd.to_datetime(df_distance['time'])\n",
    "df_distance['minutes_from_injection'] = (df_distance['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# 3. Respiration\n",
    "print(\"3. Loading respiration rates...\")\n",
    "resp_paths = generate_paths(cages, dates, 'animal_respiration.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as respiration_rate\n",
    "FROM read_parquet([{resp_paths}])\n",
    "WHERE name = 'animal.respiration_rate_lucas_kanade_psd'\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_respiration = con.execute(query).df()\n",
    "df_respiration['time'] = pd.to_datetime(df_respiration['time'])\n",
    "df_respiration['minutes_from_injection'] = (df_respiration['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# 4. Motion scores\n",
    "print(\"4. Loading motion scores...\")\n",
    "motion_paths = generate_paths(cages, dates, 'cage_motion_vector.parquet')\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    cage_id,\n",
    "    time,\n",
    "    AVG(value) as motion_score\n",
    "FROM read_parquet([{motion_paths}])\n",
    "WHERE resolution = 60\n",
    "GROUP BY cage_id, time\n",
    "\"\"\"\n",
    "df_motion = con.execute(query).df()\n",
    "df_motion['time'] = pd.to_datetime(df_motion['time'])\n",
    "df_motion['minutes_from_injection'] = (df_motion['time'] - INJECTION_DATETIME).dt.total_seconds() / 60\n",
    "\n",
    "# Merge all features\n",
    "print(\"\\nMerging all behavioral features...\")\n",
    "df = activity_pivot.merge(df_distance, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df = df.merge(df_respiration, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df = df.merge(df_motion, on=['cage_id', 'minutes_from_injection'], how='outer')\n",
    "df['treatment'] = df['cage_id'].map(TREATMENT_MAP)\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} records\")\n",
    "\n",
    "# Get only numeric feature columns\n",
    "feature_cols = [c for c in df.columns \n",
    "                if c not in ['cage_id', 'minutes_from_injection', 'treatment'] \n",
    "                and df[c].dtype in ['float64', 'float32', 'int64', 'int32']]\n",
    "print(f\"  Features: {feature_cols}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASELINE NORMALIZATION (per 180B protocol)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE NORMALIZATION (180B Protocol)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate baseline for each cage\n",
    "baseline_window = TIME_WINDOWS['baseline']\n",
    "baseline_data = df[\n",
    "    (df['minutes_from_injection'] >= baseline_window[0]) & \n",
    "    (df['minutes_from_injection'] < baseline_window[1])\n",
    "]\n",
    "\n",
    "cage_baselines = {}\n",
    "for feature in feature_cols:\n",
    "    cage_baselines[feature] = baseline_data.groupby('cage_id')[feature].mean()\n",
    "\n",
    "# Normalize all features\n",
    "for feature in feature_cols:\n",
    "    df[f'{feature}_baseline'] = df['cage_id'].map(cage_baselines[feature])\n",
    "    df[f'{feature}_pct_change'] = ((df[feature] / df[f'{feature}_baseline']) - 1) * 100\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATE PERCENT CHANGE FOR ALL TIME WINDOWS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING PERCENT CHANGE FOR ALL TIME WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pct_change_results = []\n",
    "\n",
    "for window_name, (start, end) in TIME_WINDOWS.items():\n",
    "    if window_name == 'baseline':\n",
    "        continue\n",
    "    \n",
    "    window_data = df[\n",
    "        (df['minutes_from_injection'] >= start) & \n",
    "        (df['minutes_from_injection'] < end)\n",
    "    ]\n",
    "    \n",
    "    baseline_data = df[\n",
    "        (df['minutes_from_injection'] >= baseline_window[0]) & \n",
    "        (df['minutes_from_injection'] < baseline_window[1])\n",
    "    ]\n",
    "    \n",
    "    for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "        for feature in feature_cols:\n",
    "            # Get cage-level means\n",
    "            post_means = window_data[window_data['treatment'] == treatment].groupby('cage_id')[feature].mean()\n",
    "            pre_means = baseline_data[baseline_data['treatment'] == treatment].groupby('cage_id')[feature].mean()\n",
    "            \n",
    "            # Align cages\n",
    "            common_cages = list(set(post_means.index) & set(pre_means.index))\n",
    "            if len(common_cages) < 2:\n",
    "                continue\n",
    "            \n",
    "            post_vals = post_means.loc[common_cages].values\n",
    "            pre_vals = pre_means.loc[common_cages].values\n",
    "            \n",
    "            # Remove NaN pairs\n",
    "            mask = ~(np.isnan(post_vals) | np.isnan(pre_vals))\n",
    "            post_vals = post_vals[mask]\n",
    "            pre_vals = pre_vals[mask]\n",
    "            \n",
    "            if len(post_vals) < 2 or np.all(pre_vals == 0):\n",
    "                continue\n",
    "            \n",
    "            # Calculate mean log2 fold change\n",
    "            fold_changes = post_vals / pre_vals\n",
    "            log2_fold_changes = np.log2(fold_changes)\n",
    "            mean_log2fc = np.mean(log2_fold_changes)\n",
    "            \n",
    "            # Wilcoxon test for significance\n",
    "            try:\n",
    "                stat, p_val = stats.wilcoxon(post_vals, pre_vals, alternative='two-sided')\n",
    "            except:\n",
    "                p_val = np.nan\n",
    "            \n",
    "            pct_change_results.append({\n",
    "                'window': window_name,\n",
    "                'treatment': treatment,\n",
    "                'feature': feature,\n",
    "                'n_cages': len(post_vals),\n",
    "                'mean_log2fc': mean_log2fc,\n",
    "                'p_value': p_val\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(pct_change_results)\n",
    "\n",
    "# Add significance markers\n",
    "results_df['sig'] = results_df['p_value'].apply(\n",
    "    lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE COMPREHENSIVE HEATMAP - ALL TIME WINDOWS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING HEATMAP - ALL TIME WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define time window order\n",
    "window_order = ['early', 'onset', 'peak', 'late']\n",
    "window_labels = {\n",
    "    'early': 'Early\\n(0-2h)',\n",
    "    'onset': 'Onset\\n(2-4h)',\n",
    "    'peak': 'Peak\\n(4-7h)',\n",
    "    'late': 'Late\\n(7-9h)'\n",
    "}\n",
    "\n",
    "# Create figure with subplots for each treatment\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 20))\n",
    "\n",
    "for idx, treatment in enumerate(['Vehicle', '5 mg/kg', '25 mg/kg']):\n",
    "    treatment_data = results_df[results_df['treatment'] == treatment]\n",
    "    \n",
    "    if len(treatment_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create pivot table: features x time windows\n",
    "    pivot = treatment_data.pivot_table(\n",
    "        index='feature',\n",
    "        columns='window',\n",
    "        values='mean_log2fc',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Reorder columns by time\n",
    "    pivot = pivot[[w for w in window_order if w in pivot.columns]]\n",
    "    \n",
    "    # Get significance for annotations\n",
    "    sig_pivot = treatment_data.pivot_table(\n",
    "        index='feature',\n",
    "        columns='window',\n",
    "        values='sig',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sig_pivot = sig_pivot[[w for w in window_order if w in sig_pivot.columns]]\n",
    "    \n",
    "    # Sort features by peak window effect size (or another criterion)\n",
    "    peak_effects = pivot['peak'].abs() if 'peak' in pivot.columns else pivot.iloc[:, -1].abs()\n",
    "    pivot = pivot.loc[peak_effects.sort_values(ascending=False).index]\n",
    "    sig_pivot = sig_pivot.loc[pivot.index]\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Determine color scale\n",
    "    vmax = max(3, np.nanmax(np.abs(pivot.values)))\n",
    "    \n",
    "    sns.heatmap(pivot,\n",
    "               ax=ax,\n",
    "               cmap='RdBu_r',\n",
    "               center=0,\n",
    "               vmin=-vmax, vmax=vmax,\n",
    "               cbar_kws={'label': 'log2(Fold Change)'},\n",
    "               yticklabels=pivot.index,\n",
    "               xticklabels=[window_labels[w] for w in pivot.columns],\n",
    "               linewidths=0.5,\n",
    "               annot=True,\n",
    "               fmt='.2f',\n",
    "               annot_kws={'size': 8})\n",
    "    \n",
    "    # Add significance stars\n",
    "    for i, feature in enumerate(pivot.index):\n",
    "        for j, window in enumerate(pivot.columns):\n",
    "            sig = sig_pivot.loc[feature, window]\n",
    "            if pd.notna(sig) and sig:\n",
    "                ax.text(j + 0.5, i + 0.7, sig, ha='center', va='center',\n",
    "                       fontsize=12, fontweight='bold', color='black')\n",
    "    \n",
    "    title = f\"{treatment} - log2(Fold Change) Across All Time Windows\"\n",
    "    title += f\"\\nBaseline: -30 to 0 min | Injection: Jan 14, 2025 6:00 AM\"\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold', pad=15)\n",
    "    ax.set_ylabel('Behavioral Feature', fontsize=11)\n",
    "    ax.set_xlabel('Time Window', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('morphine_all_windows_log2fc_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: morphine_all_windows_log2fc_heatmap.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - LOG2 FOLD CHANGE BY WINDOW AND TREATMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for window in window_order:\n",
    "    print(f\"\\n{window.upper()} WINDOW ({window_labels[window].replace(chr(10), ' ')}):\")\n",
    "    window_data = results_df[results_df['window'] == window]\n",
    "    \n",
    "    for treatment in ['Vehicle', '5 mg/kg', '25 mg/kg']:\n",
    "        print(f\"\\n  {treatment}:\")\n",
    "        treat_data = window_data[window_data['treatment'] == treatment].sort_values('mean_log2fc', ascending=False)\n",
    "        \n",
    "        if len(treat_data) > 0:\n",
    "            print(f\"    Top 3 increases:\")\n",
    "            for _, row in treat_data.head(3).iterrows():\n",
    "                fold = 2 ** row['mean_log2fc']\n",
    "                print(f\"      {row['feature']:20s}: log2FC={row['mean_log2fc']:+6.2f} ({fold:.1f}x) {row['sig']}\")\n",
    "            \n",
    "            print(f\"    Top 3 decreases:\")\n",
    "            for _, row in treat_data.tail(3).iloc[::-1].iterrows():\n",
    "                fold = 2 ** row['mean_log2fc']\n",
    "                print(f\"      {row['feature']:20s}: log2FC={row['mean_log2fc']:+6.2f} ({fold:.2f}x) {row['sig']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTES:\")\n",
    "print(\"  - log2FC=+1 means 2x increase, log2FC=+2 means 4x increase, log2FC=+3 means 8x increase\")\n",
    "print(\"  - log2FC=-1 means 0.5x (50% of baseline), log2FC=-2 means 0.25x (25% of baseline)\")\n",
    "print(\"  - Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "print(\"  - Each value is mean log2(fold change) across n=3 cages per treatment\")\n",
    "print(\"  - Wilcoxon signed-rank test used for statistical testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc085d-091f-48b2-92d0-75242331fd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a69933-ec4b-4ba5-ac50-bb133b3ca2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
