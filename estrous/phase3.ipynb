{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1822fde-5ae0-49f6-8de2-1b5dd277ad91",
   "metadata": {},
   "source": [
    "## Literature Foundation: Khatiz et al. (2025)\n",
    "\n",
    "**\"Real-time behavioral monitoring of C57BL/6J mice during reproductive cycle\"**  \n",
    "*Frontiers in Neuroscience, 19:1509822*\n",
    "\n",
    "### Key Behavioral Markers\n",
    "\n",
    "| Estrus (High Estrogen) | Metestrus/Diestrus (Low Estrogen) |\n",
    "|------------------------|-----------------------------------|\n",
    "| 30% more physically demanding activity | Lower overall activity |\n",
    "| Sustained activity bouts (low fragmentation) | Fragmented activity (more bouts) |\n",
    "| Higher exploratory behavior | More sleep-related behavior |\n",
    "| Lower feeding during dark cycle | Higher feeding/habituation |\n",
    "| More locomotion bout counts | Fewer locomotion bouts |\n",
    "| Less sleep fragmentation | More sleep fragmentation (more rousings) |\n",
    "\n",
    "### Statistical Methods from Khatiz\n",
    "\n",
    "1. **Hierarchical Clustering** - Group behaviors by statistical relationships\n",
    "2. **Factor Analysis** - Identify underlying behavioral dimensions\n",
    "3. **PCA** - Reduce dimensionality, find primary axes of differentiation\n",
    "4. **K-Means Clustering** - Cluster nights into groups\n",
    "5. **ANOVA + Fisher's LSD** - Compare groups (we lack ground truth for this)\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis Pipeline\n",
    "\n",
    "1. **Data Loading:** Load bout data from S3, filter to dark cycle\n",
    "2. **Feature Engineering:** Compute duration AND bout-based metrics (per Khatiz)\n",
    "3. **Normalization:** Z-score within each animal to remove individual differences\n",
    "4. **Classification Approaches:**\n",
    "   - Weighted composite score (based on Khatiz markers)\n",
    "   - K-Means clustering (unsupervised)\n",
    "   - Hierarchical clustering (unsupervised)\n",
    "5. **Validation:** Factor analysis, dimensionality reduction, method comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- ~25% of nights classified as \"estrus-like\" (matches 1-2 days of 4-5 day cycle)\n",
    "- Significant feature differences between classified states\n",
    "- Agreement between supervised (weighted score) and unsupervised (clustering) methods\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Khatiz A, et al. (2025). Real-time behavioral monitoring of C57BL/6J mice during reproductive cycle. *Front. Neurosci.* 19:1509822.\n",
    "2. Levy DR, et al. (2023). Mouse spontaneous behavior reflects individual variation rather than estrous state. *Curr Biol.* 33:1358-1364.\n",
    "3. Wollnik F, Turek FW. (1988). Estrous correlated modulations of circadian and ultradian wheel-running activity rhythms. *Physiol Behav.* 43:389-396."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a55b36-ce58-47a4-bbf7-7764a829f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb pyarrow astropy umap-learn -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84dbbd90-3445-4a79-8c6a-993e251de28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 12:38:22.121582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-03 12:38:22.121654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-03 12:38:22.164333: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-03 12:38:22.256154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tools\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, ttest_ind\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# ML tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import umap\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdcf6d15-e923-46bc-b0d7-051d9c428128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Vehicle cages: [4918, 4922, 4923, 4928, 4929, 4934]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular\"\n",
    "\n",
    "# Vehicle control cages (14-16 days of unconfounded baseline)\n",
    "VEHICLE_CAGES = {\n",
    "    'Rep1': {\n",
    "        'cages': [4918, 4922, 4923],\n",
    "        'start_date': '2025-01-07',\n",
    "        'end_date': '2025-01-22',\n",
    "    },\n",
    "    'Rep2': {\n",
    "        'cages': [4928, 4929, 4934],\n",
    "        'start_date': '2025-01-22',\n",
    "        'end_date': '2025-02-04',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Light cycle parameters (EST → UTC)\n",
    "# Lights ON: 6:00 AM EST = 11:00 UTC\n",
    "# Lights OFF: 6:00 PM EST = 23:00 UTC\n",
    "# Dark phase: hour >= 23 OR hour < 11 (UTC)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Vehicle cages: {[c for rep in VEHICLE_CAGES.values() for c in rep['cages']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44075223-4180-421d-b17d-988fd094f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bout_data(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal bout data for a single cage across date range.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_bouts.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            # Skip missing dates\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_all_vehicle_data():\n",
    "    \"\"\"Load bout data for all vehicle control cages.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for rep_name, rep_config in VEHICLE_CAGES.items():\n",
    "        print(f\"Loading {rep_name}...\")\n",
    "        for cage_id in rep_config['cages']:\n",
    "            print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "            df = load_bout_data(cage_id, rep_config['start_date'], rep_config['end_date'])\n",
    "            if len(df) > 0:\n",
    "                df['replicate'] = rep_name\n",
    "                all_data.append(df)\n",
    "                print(f\"{len(df):,} bouts\")\n",
    "            else:\n",
    "                print(\"No data\")\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b533c8e8-8dc4-4f74-ba37-83514efd1e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. animal_activity_features.parquet\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3d18eec84147fc809223578adae67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns (157):\n",
      "Movement-related columns: ['start_to_end_displacement', 'total_displacement', 'average_velocity', 'max_velocity', 'variance_velocity', 'sum_displacement_cents', 'average_displacement_cents', 'min_displacement_cents', 'max_displacement_cents', 'sum_displacement_nose', 'average_displacement_nose', 'min_displacement_nose', 'max_displacement_nose', 'stationary_ratio', 'avg_distance_per_timestep', 'max_distance_per_timestep', 'min_distance_per_timestep', 'total_distance_moved', 'avg_total_distance_moved', 'first_to_last_avg_distance', 'first_to_last_max_distance', 'first_to_last_min_distance', 'avg_kpt_4_to_6_distance', 'max_kpt_4_to_6_distance', 'min_kpt_4_to_6_distance', 'avg_kpt_1_to_6_distance', 'max_kpt_1_to_6_distance', 'min_kpt_1_to_6_distance', 'avg_kpt_1_to_5_distance', 'max_kpt_1_to_5_distance', 'min_kpt_1_to_5_distance', 'avg_distance_to_0', 'avg_distance_to_1', 'avg_distance_to_2']\n",
      "\n",
      "Sample data:\n",
      "                 time predicted_identity  start_to_end_displacement  \\\n",
      "0 2025-01-10 03:00:00      blackallwhite                  46.355407   \n",
      "1 2025-01-10 03:00:01      blackallwhite                        NaN   \n",
      "2 2025-01-10 03:00:02      blackallwhite                        NaN   \n",
      "3 2025-01-10 03:00:03      blackallwhite                        NaN   \n",
      "4 2025-01-10 03:00:04      blackallwhite                  73.979391   \n",
      "\n",
      "   total_displacement  average_velocity  max_velocity  variance_velocity  \n",
      "0           72.412082          2.139097      5.382966           2.512915  \n",
      "1                 NaN               NaN           NaN                NaN  \n",
      "2                 NaN               NaN           NaN                NaN  \n",
      "3                 NaN               NaN           NaN                NaN  \n",
      "4           75.444317          7.137754     16.826122          20.176143  \n",
      "\n",
      "======================================================================\n",
      "2. animal_activity_db.parquet\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa2ecc132b44e9da5a94ed6d882caf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['predicted_identity', 'time', 'resolution', 'name', 'value', 'units', 'version_str', 'organization_id', 'cage_id', 'study_id', 'device_id', 'run_id', 'animal_id', 'ULID', '__index_level_0__', 'filename', 'source_file', 'date']\n",
      "\n",
      "Unique metric names:\n",
      "['animal_bouts.active' 'animal_bouts.climbing' 'animal_bouts.inactive'\n",
      " 'animal_bouts.locomotion']\n",
      "\n",
      "======================================================================\n",
      "3. animal_bout_metrics.parquet\n",
      "======================================================================\n",
      "Columns: ['predicted_identity', 'start_time', 'end_time', 'state_name', 'organization_id', 'cage_id', 'study_id', 'device_id', 'animal_id', 'bout_length_seconds', 'metric_name', 'metric_agg', 'metric_value', 'source_file', 'date']\n",
      "\n",
      "Unique metric names:\n",
      "['animal.distance_travelled']\n",
      "\n",
      "Unique state names:\n",
      "['animal_bouts.drinking' 'animal_bouts.locomotion' 'animal_bouts.active'\n",
      " 'animal_bouts.social.in_proximity_other' 'animal_bouts.inactive'\n",
      " 'animal_bouts.climbing' 'animal_bouts.feeding'\n",
      " 'animal_bouts.social.isolated_other' 'animal_bouts.inferred_sleep'\n",
      " 'animal_bouts.social.proximal_all' 'animal_bouts.social.isolated_all']\n"
     ]
    }
   ],
   "source": [
    "# Check what exploration/movement data is available\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "conn.execute(\"SET s3_region='us-east-1';\")\n",
    "\n",
    "sample_cage = 4918\n",
    "sample_date = '2025-01-10'\n",
    "\n",
    "# 1. Check animal_activity_features.parquet\n",
    "print(\"=\"*70)\n",
    "print(\"1. animal_activity_features.parquet\")\n",
    "print(\"=\"*70)\n",
    "path = f\"{S3_BASE}/cage_id={sample_cage}/date={sample_date}/animal_activity_features.parquet\"\n",
    "try:\n",
    "    df_features = conn.execute(f\"SELECT * FROM read_parquet('{path}') LIMIT 5\").fetchdf()\n",
    "    print(f\"Columns ({len(df_features.columns)}):\")\n",
    "    # Show columns related to movement/exploration\n",
    "    movement_cols = [c for c in df_features.columns if any(x in c.lower() for x in \n",
    "                    ['displacement', 'velocity', 'distance', 'motion', 'stationary', 'travel'])]\n",
    "    print(f\"Movement-related columns: {movement_cols}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    if movement_cols:\n",
    "        print(df_features[['time', 'predicted_identity'] + movement_cols[:5]].head())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 2. Check animal_activity_db.parquet\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. animal_activity_db.parquet\")\n",
    "print(\"=\"*70)\n",
    "path = f\"{S3_BASE}/cage_id={sample_cage}/date={sample_date}/animal_activity_db.parquet\"\n",
    "try:\n",
    "    df_activity = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "    print(f\"Columns: {df_activity.columns.tolist()}\")\n",
    "    print(f\"\\nUnique metric names:\")\n",
    "    print(df_activity['name'].unique())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Check animal_bout_metrics.parquet\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. animal_bout_metrics.parquet\")\n",
    "print(\"=\"*70)\n",
    "path = f\"{S3_BASE}/cage_id={sample_cage}/date={sample_date}/animal_bout_metrics.parquet\"\n",
    "try:\n",
    "    df_bout_metrics = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "    print(f\"Columns: {df_bout_metrics.columns.tolist()}\")\n",
    "    print(f\"\\nUnique metric names:\")\n",
    "    print(df_bout_metrics['metric_name'].unique())\n",
    "    print(f\"\\nUnique state names:\")\n",
    "    print(df_bout_metrics['state_name'].unique())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d5bfa5-7b1c-4f71-8084-57bc940373a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading activity features for cage 4918...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58348a6097345838de3188f9671625d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6028af06424be4a1175d3aafa9fccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1348dde6cbfb485199cc2ed7fb822413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a62b92a4f9b4a2ea66f1709b4d7c650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 799200 rows\n",
      "\n",
      "Movement columns found: ['start_to_end_displacement', 'total_displacement', 'average_velocity', 'max_velocity', 'variance_velocity', 'sum_displacement_cents', 'average_displacement_cents', 'min_displacement_cents', 'max_displacement_cents', 'sum_displacement_nose', 'average_displacement_nose', 'min_displacement_nose', 'max_displacement_nose', 'avg_speed', 'max_speed', 'variance_speed', 'stationary_ratio', 'avg_distance_per_timestep', 'max_distance_per_timestep', 'min_distance_per_timestep', 'total_distance_moved', 'avg_total_distance_moved', 'first_to_last_avg_distance', 'first_to_last_max_distance', 'first_to_last_min_distance', 'avg_kpt_4_to_6_distance', 'max_kpt_4_to_6_distance', 'min_kpt_4_to_6_distance', 'avg_kpt_1_to_6_distance', 'max_kpt_1_to_6_distance', 'min_kpt_1_to_6_distance', 'avg_kpt_1_to_5_distance', 'max_kpt_1_to_5_distance', 'min_kpt_1_to_5_distance', 'avg_distance_to_0', 'avg_distance_to_1', 'avg_distance_to_2']\n"
     ]
    }
   ],
   "source": [
    "def load_activity_features(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal_activity_features.parquet for exploration metrics.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_activity_features.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Load for one cage first to see what we get\n",
    "print(\"Loading activity features for cage 4918...\")\n",
    "df_features_test = load_activity_features(4918, '2025-01-07', '2025-01-10')\n",
    "print(f\"Loaded {len(df_features_test)} rows\")\n",
    "\n",
    "# Check movement-related columns\n",
    "movement_cols = [c for c in df_features_test.columns if any(x in c.lower() for x in \n",
    "                ['displacement', 'velocity', 'distance', 'motion', 'stationary', 'travel', 'speed'])]\n",
    "print(f\"\\nMovement columns found: {movement_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f8c8ee-9e5f-433a-85a3-ac669c14ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bout metrics (distance traveled)...\n",
      "============================================================\n",
      "Rep1:\n",
      "  Cage 4918... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedc4e050fcb4c1fa906a9b4a416d1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,153,458 rows\n",
      "  Cage 4922... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26477ff23f64db396442f24a2c129ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,114,699 rows\n",
      "  Cage 4923... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab80bb1bc1994396adb0c9dd65a74a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c95fed1ae04bc2be7c87e9629f0fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30fd7bfcd6c4764ab1da73dffc7ac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,112,969 rows\n",
      "Rep2:\n",
      "  Cage 4928... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fee39c7b6ee426fa473c4f178f0aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,967,936 rows\n",
      "  Cage 4929... 1,828,562 rows\n",
      "  Cage 4934... 1,945,821 rows\n",
      "============================================================\n",
      "Total bout metrics loaded: 12,123,445\n"
     ]
    }
   ],
   "source": [
    "def load_bout_metrics(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal_bout_metrics.parquet with distance traveled per bout.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_bout_metrics.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Load for all vehicle cages\n",
    "print(\"Loading bout metrics (distance traveled)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_bout_metrics = []\n",
    "for rep_name, rep_config in VEHICLE_CAGES.items():\n",
    "    print(f\"{rep_name}:\")\n",
    "    for cage_id in rep_config['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_bout_metrics(cage_id, rep_config['start_date'], rep_config['end_date'])\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep_name\n",
    "            all_bout_metrics.append(df)\n",
    "            print(f\"{len(df):,} rows\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bout_metrics = pd.concat(all_bout_metrics, ignore_index=True)\n",
    "print(\"=\"*60)\n",
    "print(f\"Total bout metrics loaded: {len(df_bout_metrics):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c7a2b5-3680-496b-b4a9-8176b0b539bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bout Metrics Structure:\n",
      "============================================================\n",
      "Columns: ['predicted_identity', 'start_time', 'end_time', 'state_name', 'organization_id', 'cage_id', 'study_id', 'device_id', 'animal_id', 'bout_length_seconds', 'metric_name', 'metric_agg', 'metric_value', 'source_file', 'date', 'replicate']\n",
      "\n",
      "Metric names: ['animal.distance_travelled']\n",
      "State names: ['animal_bouts.active' 'animal_bouts.climbing' 'animal_bouts.locomotion'\n",
      " 'animal_bouts.feeding' 'animal_bouts.inactive'\n",
      " 'animal_bouts.social.in_proximity_other'\n",
      " 'animal_bouts.social.isolated_other' 'animal_bouts.inferred_sleep'\n",
      " 'animal_bouts.drinking' 'animal_bouts.social.isolated_all'\n",
      " 'animal_bouts.social.proximal_all']\n",
      "\n",
      "Sample data:\n",
      "           start_time  animal_id               state_name  \\\n",
      "0 2025-01-07 23:00:00       9259      animal_bouts.active   \n",
      "1 2025-01-07 23:06:37       9259    animal_bouts.climbing   \n",
      "2 2025-01-07 23:05:49       9259  animal_bouts.locomotion   \n",
      "3 2025-01-07 23:08:33       9258  animal_bouts.locomotion   \n",
      "4 2025-01-07 23:08:37       9258  animal_bouts.locomotion   \n",
      "5 2025-01-07 23:08:43       9258  animal_bouts.locomotion   \n",
      "6 2025-01-07 23:12:03       9258     animal_bouts.feeding   \n",
      "7 2025-01-07 23:14:02       9258    animal_bouts.climbing   \n",
      "8 2025-01-07 23:13:23       9259     animal_bouts.feeding   \n",
      "9 2025-01-07 23:09:13       9258  animal_bouts.locomotion   \n",
      "\n",
      "   bout_length_seconds                metric_name  metric_value  \n",
      "0                    2  animal.distance_travelled      0.013211  \n",
      "1                    2  animal.distance_travelled      0.819643  \n",
      "2                    3  animal.distance_travelled     11.939537  \n",
      "3                    1  animal.distance_travelled      3.163343  \n",
      "4                    1  animal.distance_travelled      1.396007  \n",
      "5                    1  animal.distance_travelled      1.529670  \n",
      "6                    1  animal.distance_travelled      1.960239  \n",
      "7                    4  animal.distance_travelled      8.712209  \n",
      "8                    2  animal.distance_travelled      0.907708  \n",
      "9                    4  animal.distance_travelled     17.086460  \n",
      "\n",
      "============================================================\n",
      "Mean distance traveled by behavioral state:\n",
      "------------------------------------------------------------\n",
      "                                         mean    std    count\n",
      "state_name                                                   \n",
      "animal_bouts.active                      3.23   6.69  1606538\n",
      "animal_bouts.climbing                   16.48  23.24   218640\n",
      "animal_bouts.drinking                    0.60   0.88    21493\n",
      "animal_bouts.feeding                     2.58   3.81   194205\n",
      "animal_bouts.inactive                    0.92   2.73   993362\n",
      "animal_bouts.inferred_sleep              4.54   6.56    36301\n",
      "animal_bouts.locomotion                  6.76   7.59   564486\n",
      "animal_bouts.social.in_proximity_other   0.39   1.18  4977584\n",
      "animal_bouts.social.isolated_all         0.81   2.07    14481\n",
      "animal_bouts.social.isolated_other       1.19   2.51  3232473\n",
      "animal_bouts.social.proximal_all         0.19   0.46   263882\n"
     ]
    }
   ],
   "source": [
    "print(\"Bout Metrics Structure:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Columns: {df_bout_metrics.columns.tolist()}\")\n",
    "print(f\"\\nMetric names: {df_bout_metrics['metric_name'].unique()}\")\n",
    "print(f\"State names: {df_bout_metrics['state_name'].unique()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df_bout_metrics[['start_time', 'animal_id', 'state_name', 'bout_length_seconds', \n",
    "                       'metric_name', 'metric_value']].head(10))\n",
    "\n",
    "# Check distance traveled by state\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mean distance traveled by behavioral state:\")\n",
    "print(\"-\"*60)\n",
    "distance_by_state = df_bout_metrics.groupby('state_name')['metric_value'].agg(['mean', 'std', 'count'])\n",
    "print(distance_by_state.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ccf0ae-66cb-4f06-a4c7-439cf07ee606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bout data (animal_bouts.parquet)...\n",
      "============================================================\n",
      "Rep1:\n",
      "  Cage 4918... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edc53a1848e4b3abd2b118710ed26dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,558,344 bouts\n",
      "  Cage 4922... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defcc89157564aa1ac35b04b19cdaa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13f477fb42a4be3b04a1defeaf401b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,401,455 bouts\n",
      "  Cage 4923... 2,458,754 bouts\n",
      "Rep2:\n",
      "  Cage 4928... 2,259,981 bouts\n",
      "  Cage 4929... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc337a85e4ac4a32bc7df0a4a46a3b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b392d3931a894c7ab45f98c9b1cd6573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,071,012 bouts\n",
      "  Cage 4934... 2,201,662 bouts\n",
      "============================================================\n",
      "Total bouts loaded: 13,951,208\n"
     ]
    }
   ],
   "source": [
    "# Load bout data for all vehicle control cages\n",
    "def load_bout_data(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal_bouts.parquet for a single cage across date range.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_bouts.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Load for all vehicle cages\n",
    "print(\"Loading bout data (animal_bouts.parquet)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_bouts = []\n",
    "for rep_name, rep_config in VEHICLE_CAGES.items():\n",
    "    print(f\"{rep_name}:\")\n",
    "    for cage_id in rep_config['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_bout_data(cage_id, rep_config['start_date'], rep_config['end_date'])\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep_name\n",
    "            all_bouts.append(df)\n",
    "            print(f\"{len(df):,} bouts\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bouts = pd.concat(all_bouts, ignore_index=True)\n",
    "print(\"=\"*60)\n",
    "print(f\"Total bouts loaded: {len(df_bouts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7749c7f-2e14-414e-af0a-6f00fde666c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing nightly summaries with exploration metrics...\n",
      "Nightly summaries: 258 animal-nights\n",
      "Animals: 18\n"
     ]
    }
   ],
   "source": [
    "def compute_nightly_summaries_with_exploration(df_bouts, df_bout_metrics):\n",
    "    \"\"\"\n",
    "    Compute comprehensive nightly summaries including:\n",
    "    - Duration metrics (from animal_bouts)\n",
    "    - Bout count metrics (from animal_bouts)  \n",
    "    - Exploration metrics (from animal_bout_metrics - distance traveled)\n",
    "    \"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    STATE_MAP = {\n",
    "        'active': 'animal_bouts.active',\n",
    "        'inactive': 'animal_bouts.inactive',\n",
    "        'locomotion': 'animal_bouts.locomotion',\n",
    "        'feeding': 'animal_bouts.feeding',\n",
    "        'drinking': 'animal_bouts.drinking',\n",
    "        'climbing': 'animal_bouts.climbing',\n",
    "        'inferred_sleep': 'animal_bouts.inferred_sleep',\n",
    "    }\n",
    "    \n",
    "    # ============================================\n",
    "    # Process bouts data\n",
    "    # ============================================\n",
    "    df_bouts = df_bouts.copy()\n",
    "    df_bouts['start_time'] = pd.to_datetime(df_bouts['start_time'])\n",
    "    df_bouts['hour_utc'] = df_bouts['start_time'].dt.hour\n",
    "    df_bouts['is_dark'] = (df_bouts['hour_utc'] >= 23) | (df_bouts['hour_utc'] < 11)\n",
    "    \n",
    "    # Night date assignment\n",
    "    df_bouts['night_date'] = df_bouts['start_time'].dt.date\n",
    "    mask_early = df_bouts['hour_utc'] < 11\n",
    "    df_bouts.loc[mask_early, 'night_date'] = (\n",
    "        pd.to_datetime(df_bouts.loc[mask_early, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    \n",
    "    df_dark = df_bouts[df_bouts['is_dark']].copy()\n",
    "    \n",
    "    # ============================================\n",
    "    # Process bout metrics (distance traveled)\n",
    "    # ============================================\n",
    "    df_metrics = df_bout_metrics.copy()\n",
    "    df_metrics['start_time'] = pd.to_datetime(df_metrics['start_time'])\n",
    "    df_metrics['hour_utc'] = df_metrics['start_time'].dt.hour\n",
    "    df_metrics['is_dark'] = (df_metrics['hour_utc'] >= 23) | (df_metrics['hour_utc'] < 11)\n",
    "    \n",
    "    df_metrics['night_date'] = df_metrics['start_time'].dt.date\n",
    "    mask_early = df_metrics['hour_utc'] < 11\n",
    "    df_metrics.loc[mask_early, 'night_date'] = (\n",
    "        pd.to_datetime(df_metrics.loc[mask_early, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    \n",
    "    df_metrics_dark = df_metrics[df_metrics['is_dark']].copy()\n",
    "    \n",
    "    # ============================================\n",
    "    # Aggregate bout-based metrics\n",
    "    # ============================================\n",
    "    results = []\n",
    "    \n",
    "    for (cage_id, animal_id, night_date), group in df_dark.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'night_date': night_date,\n",
    "        }\n",
    "        \n",
    "        # Duration and bout count per state\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_data = group[group['state_name'] == full_name]\n",
    "            row[f'{short_name}_duration'] = state_data['bout_length_seconds'].sum()\n",
    "            row[f'{short_name}_bout_count'] = len(state_data)\n",
    "            row[f'{short_name}_mean_bout'] = state_data['bout_length_seconds'].mean() if len(state_data) > 0 else 0\n",
    "        \n",
    "        row['total_dark_seconds'] = group['bout_length_seconds'].sum()\n",
    "        results.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(results)\n",
    "    \n",
    "    # ============================================\n",
    "    # Aggregate exploration metrics (distance traveled)\n",
    "    # ============================================\n",
    "    exploration_results = []\n",
    "    \n",
    "    for (cage_id, animal_id, night_date), group in df_metrics_dark.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'night_date': night_date,\n",
    "        }\n",
    "        \n",
    "        # Total distance traveled (all states)\n",
    "        row['total_distance'] = group['metric_value'].sum()\n",
    "        \n",
    "        # Distance by state\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_data = group[group['state_name'] == full_name]\n",
    "            row[f'{short_name}_distance'] = state_data['metric_value'].sum()\n",
    "            row[f'{short_name}_distance_per_bout'] = state_data['metric_value'].mean() if len(state_data) > 0 else 0\n",
    "        \n",
    "        exploration_results.append(row)\n",
    "    \n",
    "    df_exploration = pd.DataFrame(exploration_results)\n",
    "    \n",
    "    # ============================================\n",
    "    # Merge bout and exploration summaries\n",
    "    # ============================================\n",
    "    df_summary = df_summary.merge(\n",
    "        df_exploration, \n",
    "        on=['cage_id', 'animal_id', 'night_date'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute derived features\n",
    "    # ============================================\n",
    "    \n",
    "    # Activity metrics\n",
    "    df_summary['activity_amplitude'] = df_summary['active_duration'] + df_summary['locomotion_duration']\n",
    "    \n",
    "    # Fragmentation metrics (bouts per unit time)\n",
    "    df_summary['sleep_fragmentation'] = df_summary['inferred_sleep_bout_count'] / (df_summary['inferred_sleep_duration'] + 1)\n",
    "    df_summary['active_fragmentation'] = df_summary['active_bout_count'] / (df_summary['active_duration'] + 1)\n",
    "    \n",
    "    # Ratios\n",
    "    df_summary['feeding_ratio'] = df_summary['feeding_duration'] / (df_summary['active_duration'] + 1)\n",
    "    df_summary['sleep_ratio'] = df_summary['inferred_sleep_duration'] / (df_summary['total_dark_seconds'] + 1)\n",
    "    \n",
    "    # Exploration metrics (NEW!)\n",
    "    df_summary['exploration_intensity'] = df_summary['total_distance'] / (df_summary['activity_amplitude'] + 1)  # Distance per active second\n",
    "    df_summary['locomotion_efficiency'] = df_summary['locomotion_distance'] / (df_summary['locomotion_duration'] + 1)  # Distance per locomotion second\n",
    "    \n",
    "    return df_summary\n",
    "\n",
    "# Compute enhanced summaries\n",
    "print(\"Computing nightly summaries with exploration metrics...\")\n",
    "df_nightly = compute_nightly_summaries_with_exploration(df_bouts, df_bout_metrics)\n",
    "\n",
    "# Filter out animal_id = 0\n",
    "df_nightly = df_nightly[df_nightly['animal_id'] != 0].copy()\n",
    "\n",
    "print(f\"Nightly summaries: {len(df_nightly)} animal-nights\")\n",
    "print(f\"Animals: {df_nightly['animal_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e85015-aa3c-44ce-86bb-1e49ad20811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Exploration Features:\n",
      "======================================================================\n",
      "total_distance                : mean=58561.80, std=21644.41\n",
      "locomotion_distance           : mean=12027.49, std=4634.39\n",
      "active_distance               : mean=13271.41, std=2700.43\n",
      "locomotion_distance_per_bout  : mean=7.19, std=0.82\n",
      "exploration_intensity         : mean=2.11, std=0.80\n",
      "locomotion_efficiency         : mean=3.65, std=0.23\n",
      "\n",
      "======================================================================\n",
      "Correlation of exploration metrics with activity_amplitude:\n",
      "----------------------------------------------------------------------\n",
      "total_distance                : r = 0.282\n",
      "locomotion_distance           : r = 0.437\n",
      "active_distance               : r = 0.864\n",
      "locomotion_distance_per_bout  : r = -0.105\n",
      "exploration_intensity         : r = -0.177\n",
      "locomotion_efficiency         : r = -0.213\n"
     ]
    }
   ],
   "source": [
    "print(\"New Exploration Features:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "exploration_features = [\n",
    "    'total_distance',\n",
    "    'locomotion_distance', \n",
    "    'active_distance',\n",
    "    'locomotion_distance_per_bout',\n",
    "    'exploration_intensity',\n",
    "    'locomotion_efficiency',\n",
    "]\n",
    "\n",
    "for feat in exploration_features:\n",
    "    if feat in df_nightly.columns:\n",
    "        print(f\"{feat:30}: mean={df_nightly[feat].mean():.2f}, std={df_nightly[feat].std():.2f}\")\n",
    "    else:\n",
    "        print(f\"{feat:30}: NOT FOUND\")\n",
    "\n",
    "# Correlation with activity\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Correlation of exploration metrics with activity_amplitude:\")\n",
    "print(\"-\"*70)\n",
    "for feat in exploration_features:\n",
    "    if feat in df_nightly.columns:\n",
    "        corr = df_nightly[feat].corr(df_nightly['activity_amplitude'])\n",
    "        print(f\"{feat:30}: r = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1272c420-058e-497c-914c-94112bff3e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Classification Weights (with Exploration):\n",
      "======================================================================\n",
      "  activity_amplitude_z           weight=+2.0   (Higher activity → Estrus)\n",
      "  locomotion_duration_z          weight=+1.5   (More locomotion → Estrus)\n",
      "  inactive_duration_z            weight=-1.5  (Less inactive time → Estrus)\n",
      "  inferred_sleep_duration_z      weight=-1.0  (Less sleep → Estrus)\n",
      "  locomotion_bout_count_z        weight=+1.0   (More locomotion bouts → Estrus)\n",
      "  active_mean_bout_z             weight=+1.0   (Longer bouts (sustained) → Estrus)\n",
      "  active_fragmentation_z         weight=-1.0  (Less fragmented activity → Estrus)\n",
      "  sleep_fragmentation_z          weight=-0.5  (Less sleep fragmentation → Estrus)\n",
      "  feeding_ratio_z                weight=-1.0  (Less feeding → Estrus)\n",
      "  total_distance_z               weight=+1.5   (More distance traveled → Estrus)\n",
      "  locomotion_distance_z          weight=+1.0   (More locomotion distance → Estrus)\n",
      "  exploration_intensity_z        weight=+1.0   (Higher exploration intensity → Estrus)\n",
      "  climbing_duration_z            weight=+0.5   (More climbing → Estrus)\n",
      "  climbing_bout_count_z          weight=+0.5   (More climbing bouts → Estrus)\n",
      "\n",
      "Total features: 14\n"
     ]
    }
   ],
   "source": [
    "# Updated classification weights including exploration features\n",
    "# Based on Khatiz: Estrus = MORE exploration, MORE distance traveled\n",
    "\n",
    "CLASSIFICATION_WEIGHTS_V2 = {\n",
    "    # Duration-based (existing)\n",
    "    'activity_amplitude_z':       (+2.0, 'Higher activity → Estrus'),\n",
    "    'locomotion_duration_z':      (+1.5, 'More locomotion → Estrus'),\n",
    "    'inactive_duration_z':        (-1.5, 'Less inactive time → Estrus'),\n",
    "    'inferred_sleep_duration_z':  (-1.0, 'Less sleep → Estrus'),\n",
    "    \n",
    "    # Bout-based (existing)\n",
    "    'locomotion_bout_count_z':    (+1.0, 'More locomotion bouts → Estrus'),\n",
    "    'active_mean_bout_z':         (+1.0, 'Longer bouts (sustained) → Estrus'),\n",
    "    'active_fragmentation_z':     (-1.0, 'Less fragmented activity → Estrus'),\n",
    "    'sleep_fragmentation_z':      (-0.5, 'Less sleep fragmentation → Estrus'),\n",
    "    \n",
    "    # Feeding\n",
    "    'feeding_ratio_z':            (-1.0, 'Less feeding → Estrus'),\n",
    "    \n",
    "    # Exploration (NEW!)\n",
    "    'total_distance_z':           (+1.5, 'More distance traveled → Estrus'),\n",
    "    'locomotion_distance_z':      (+1.0, 'More locomotion distance → Estrus'),\n",
    "    'exploration_intensity_z':    (+1.0, 'Higher exploration intensity → Estrus'),\n",
    "    \n",
    "    # Vertical exploration\n",
    "    'climbing_duration_z':        (+0.5, 'More climbing → Estrus'),\n",
    "    'climbing_bout_count_z':      (+0.5, 'More climbing bouts → Estrus'),\n",
    "}\n",
    "\n",
    "print(\"Updated Classification Weights (with Exploration):\")\n",
    "print(\"=\"*70)\n",
    "for feature, (weight, direction) in CLASSIFICATION_WEIGHTS_V2.items():\n",
    "    sign = \"+\" if weight > 0 else \"\"\n",
    "    print(f\"  {feature:30} weight={sign}{weight:<5} ({direction})\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(CLASSIFICATION_WEIGHTS_V2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d89235d0-11f6-41be-8f5b-56f93042d436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 14/14 features for scoring\n",
      "\n",
      "======================================================================\n",
      "CLASSIFICATION RESULTS (WITH EXPLORATION FEATURES)\n",
      "======================================================================\n",
      "  Estrus-like nights:    72 (27.9%)\n",
      "  Diestrus-like nights: 186 (72.1%)\n",
      "  Expected: ~25% Estrus-like\n",
      "\n",
      "======================================================================\n",
      "FEATURE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Feature                   Estrus       Diestrus     p-value    Dir?  Sig?\n",
      "---------------------------------------------------------------------------\n",
      "activity_amplitude        29303.86     27727.74     0.0339     ✓     *\n",
      "locomotion_duration       4057.39      2971.91      0.0000     ✓     *\n",
      "total_distance            74143.72     52530.09     0.0000     ✓     *\n",
      "locomotion_distance       15111.03     10833.85     0.0000     ✓     *\n",
      "climbing_distance         18946.65     10520.73     0.0000     ✓     *\n",
      "exploration_intensity     2.59         1.92         0.0000     ✓     *\n",
      "active_mean_bout          7.29         8.23         0.0000     ✗     *\n",
      "inactive_duration         9006.81      10379.78     0.0002     ✓     *\n",
      "inferred_sleep_duration   2794.29      3982.05      0.0000     ✓     *\n",
      "active_fragmentation      0.14         0.12         0.0000     ✗     *\n",
      "feeding_ratio             0.11         0.12         0.1311     ✓     \n",
      "---------------------------------------------------------------------------\n",
      "Significant (p<0.05): 10/11\n",
      "Correct direction:    9/11\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have z-scored features\n",
    "FEATURES_TO_ZSCORE = [\n",
    "    'activity_amplitude', 'locomotion_duration', 'inactive_duration',\n",
    "    'inferred_sleep_duration', 'feeding_duration', 'climbing_duration',\n",
    "    'locomotion_bout_count', 'active_bout_count', 'climbing_bout_count',\n",
    "    'active_mean_bout', 'active_fragmentation', 'sleep_fragmentation',\n",
    "    'feeding_ratio', 'sleep_ratio',\n",
    "    'total_distance', 'locomotion_distance', 'active_distance',\n",
    "    'climbing_distance', 'exploration_intensity', 'locomotion_efficiency',\n",
    "]\n",
    "\n",
    "def compute_within_animal_zscores(df, features):\n",
    "    df_out = df.copy()\n",
    "    for feature in features:\n",
    "        if feature in df_out.columns:\n",
    "            zscore_col = f'{feature}_z'\n",
    "            df_out[zscore_col] = df_out.groupby('animal_id')[feature].transform(\n",
    "                lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
    "            )\n",
    "    return df_out\n",
    "\n",
    "df_nightly = compute_within_animal_zscores(df_nightly, FEATURES_TO_ZSCORE)\n",
    "\n",
    "# Classification weights\n",
    "CLASSIFICATION_WEIGHTS_V2 = {\n",
    "    'activity_amplitude_z':       (+2.0, 'Higher activity → Estrus'),\n",
    "    'locomotion_duration_z':      (+1.5, 'More locomotion → Estrus'),\n",
    "    'inactive_duration_z':        (-1.5, 'Less inactive time → Estrus'),\n",
    "    'inferred_sleep_duration_z':  (-1.0, 'Less sleep → Estrus'),\n",
    "    'climbing_duration_z':        (+0.5, 'More climbing → Estrus'),\n",
    "    'locomotion_bout_count_z':    (+1.0, 'More locomotion bouts → Estrus'),\n",
    "    'climbing_bout_count_z':      (+0.5, 'More climbing bouts → Estrus'),\n",
    "    'active_mean_bout_z':         (+1.0, 'Longer bouts (sustained) → Estrus'),\n",
    "    'active_fragmentation_z':     (-1.0, 'Less fragmented activity → Estrus'),\n",
    "    'sleep_fragmentation_z':      (-0.5, 'Less sleep fragmentation → Estrus'),\n",
    "    'feeding_ratio_z':            (-1.0, 'Less feeding → Estrus'),\n",
    "    'total_distance_z':           (+1.5, 'More distance traveled → Estrus'),\n",
    "    'locomotion_distance_z':      (+1.0, 'More locomotion distance → Estrus'),\n",
    "    'exploration_intensity_z':    (+1.0, 'Higher exploration intensity → Estrus'),\n",
    "}\n",
    "\n",
    "# Compute estrous score\n",
    "df_nightly['estrous_score'] = 0\n",
    "used = 0\n",
    "for feature, (weight, _) in CLASSIFICATION_WEIGHTS_V2.items():\n",
    "    if feature in df_nightly.columns:\n",
    "        df_nightly['estrous_score'] += weight * df_nightly[feature]\n",
    "        used += 1\n",
    "print(f\"Used {used}/{len(CLASSIFICATION_WEIGHTS_V2)} features for scoring\")\n",
    "\n",
    "# Classify\n",
    "def classify_estrous_state(df, threshold_percentile=75):\n",
    "    df_out = df.copy()\n",
    "    def classify_animal(group):\n",
    "        threshold = np.percentile(group['estrous_score'], threshold_percentile)\n",
    "        group['estrous_state'] = np.where(\n",
    "            group['estrous_score'] >= threshold, 'Estrus-like', 'Diestrus-like'\n",
    "        )\n",
    "        return group\n",
    "    return df_out.groupby('animal_id', group_keys=False).apply(classify_animal)\n",
    "\n",
    "df_nightly = classify_estrous_state(df_nightly, threshold_percentile=75)\n",
    "\n",
    "# Results\n",
    "state_counts = df_nightly['estrous_state'].value_counts()\n",
    "n_total = len(df_nightly)\n",
    "n_estrus = state_counts.get('Estrus-like', 0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION RESULTS (WITH EXPLORATION FEATURES)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Estrus-like nights:   {n_estrus:3d} ({100*n_estrus/n_total:.1f}%)\")\n",
    "print(f\"  Diestrus-like nights: {n_total-n_estrus:3d} ({100*(n_total-n_estrus)/n_total:.1f}%)\")\n",
    "print(f\"  Expected: ~25% Estrus-like\")\n",
    "\n",
    "# Validate\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "estrus_df = df_nightly[df_nightly['estrous_state'] == 'Estrus-like']\n",
    "diestrus_df = df_nightly[df_nightly['estrous_state'] == 'Diestrus-like']\n",
    "\n",
    "validation_features = [\n",
    "    ('activity_amplitude', 'Higher in Estrus'),\n",
    "    ('locomotion_duration', 'Higher in Estrus'),\n",
    "    ('total_distance', 'Higher in Estrus'),\n",
    "    ('locomotion_distance', 'Higher in Estrus'),\n",
    "    ('climbing_distance', 'Higher in Estrus'),\n",
    "    ('exploration_intensity', 'Higher in Estrus'),\n",
    "    ('active_mean_bout', 'Higher in Estrus'),\n",
    "    ('inactive_duration', 'Lower in Estrus'),\n",
    "    ('inferred_sleep_duration', 'Lower in Estrus'),\n",
    "    ('active_fragmentation', 'Lower in Estrus'),\n",
    "    ('feeding_ratio', 'Lower in Estrus'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Feature':<25} {'Estrus':<12} {'Diestrus':<12} {'p-value':<10} {'Dir?':<5} {'Sig?'}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "sig_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "for feat, expected in validation_features:\n",
    "    if feat not in df_nightly.columns:\n",
    "        continue\n",
    "    \n",
    "    e_vals = estrus_df[feat].dropna()\n",
    "    d_vals = diestrus_df[feat].dropna()\n",
    "    \n",
    "    if len(e_vals) == 0 or len(d_vals) == 0:\n",
    "        continue\n",
    "    \n",
    "    e_mean = e_vals.mean()\n",
    "    d_mean = d_vals.mean()\n",
    "    _, p_val = ttest_ind(e_vals, d_vals)\n",
    "    \n",
    "    if 'Higher' in expected:\n",
    "        correct = e_mean > d_mean\n",
    "    else:\n",
    "        correct = e_mean < d_mean\n",
    "    \n",
    "    dir_mark = \"✓\" if correct else \"✗\"\n",
    "    sig_mark = \"*\" if p_val < 0.05 else \"\"\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        sig_count += 1\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print(f\"{feat:<25} {e_mean:<12.2f} {d_mean:<12.2f} {p_val:<10.4f} {dir_mark:<5} {sig_mark}\")\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Significant (p<0.05): {sig_count}/{len(validation_features)}\")\n",
    "print(f\"Correct direction:    {correct_count}/{len(validation_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "418b5a49-d6fc-42b7-a75e-941461c6321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared: 258 nights × 14 features\n",
      "Features (all equally weighted after standardization):\n",
      "  - activity_amplitude\n",
      "  - locomotion_duration\n",
      "  - inactive_duration\n",
      "  - inferred_sleep_duration\n",
      "  - feeding_duration\n",
      "  - climbing_duration\n",
      "  - locomotion_bout_count\n",
      "  - climbing_bout_count\n",
      "  - active_mean_bout\n",
      "  - active_fragmentation\n",
      "  - sleep_fragmentation\n",
      "  - total_distance\n",
      "  - locomotion_distance\n",
      "  - exploration_intensity\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CORRECTED APPROACH: Data-driven weights, not manual weights\n",
    "# Based on Khatiz et al. methodology\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Features to use (all standardized equally - no manual weights!)\n",
    "FEATURES_FOR_ANALYSIS = [\n",
    "    # Duration metrics\n",
    "    'activity_amplitude',\n",
    "    'locomotion_duration', \n",
    "    'inactive_duration',\n",
    "    'inferred_sleep_duration',\n",
    "    'feeding_duration',\n",
    "    'climbing_duration',\n",
    "    \n",
    "    # Bout metrics\n",
    "    'locomotion_bout_count',\n",
    "    'climbing_bout_count',\n",
    "    'active_mean_bout',\n",
    "    'active_fragmentation',\n",
    "    'sleep_fragmentation',\n",
    "    \n",
    "    # Exploration metrics\n",
    "    'total_distance',\n",
    "    'locomotion_distance',\n",
    "    'exploration_intensity',\n",
    "]\n",
    "\n",
    "# Prepare data - drop rows with missing values\n",
    "df_analysis = df_nightly.dropna(subset=FEATURES_FOR_ANALYSIS).copy()\n",
    "X = df_analysis[FEATURES_FOR_ANALYSIS].values\n",
    "\n",
    "# STANDARDIZE (like Khatiz - puts all metrics on equal footing)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Data prepared: {X_scaled.shape[0]} nights × {X_scaled.shape[1]} features\")\n",
    "print(f\"Features (all equally weighted after standardization):\")\n",
    "for f in FEATURES_FOR_ANALYSIS:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cde198-a3b7-4ee8-8bdd-656f633a1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FACTOR ANALYSIS LOADINGS (Data-Derived Weights)\n",
      "======================================================================\n",
      "                         Factor_1  Factor_2  Factor_3\n",
      "activity_amplitude          0.494    -0.437    -0.231\n",
      "locomotion_duration         1.000     0.003    -0.009\n",
      "inactive_duration           0.060    -0.995    -0.004\n",
      "inferred_sleep_duration    -0.021    -0.825    -0.042\n",
      "feeding_duration            0.215    -0.095     0.061\n",
      "climbing_duration           0.476     0.021     0.831\n",
      "locomotion_bout_count       0.981    -0.050    -0.020\n",
      "climbing_bout_count         0.548     0.013     0.731\n",
      "active_mean_bout           -0.739     0.229    -0.232\n",
      "active_fragmentation        0.759    -0.138     0.252\n",
      "sleep_fragmentation        -0.194     0.323     0.027\n",
      "total_distance              0.758    -0.053     0.635\n",
      "locomotion_distance         0.985     0.020     0.058\n",
      "exploration_intensity       0.514     0.128     0.783\n",
      "\n",
      "======================================================================\n",
      "FACTOR INTERPRETATION:\n",
      "======================================================================\n",
      "\n",
      "Factor_1:\n",
      "  HIGH: locomotion_duration (+1.00), locomotion_distance (+0.98), locomotion_bout_count (+0.98)\n",
      "  LOW:  active_mean_bout (-0.74), sleep_fragmentation (-0.19), inferred_sleep_duration (-0.02)\n",
      "\n",
      "Factor_2:\n",
      "  HIGH: sleep_fragmentation (+0.32), active_mean_bout (+0.23), exploration_intensity (+0.13)\n",
      "  LOW:  inactive_duration (-0.99), inferred_sleep_duration (-0.83), activity_amplitude (-0.44)\n",
      "\n",
      "Factor_3:\n",
      "  HIGH: climbing_duration (+0.83), exploration_intensity (+0.78), climbing_bout_count (+0.73)\n",
      "  LOW:  active_mean_bout (-0.23), activity_amplitude (-0.23), inferred_sleep_duration (-0.04)\n"
     ]
    }
   ],
   "source": [
    "# Factor Analysis - let the DATA determine the weights (loadings)\n",
    "# This is exactly what Khatiz did\n",
    "\n",
    "n_factors = 3  # Khatiz found ~3 main factors\n",
    "fa = FactorAnalysis(n_components=n_factors, random_state=42)\n",
    "fa.fit(X_scaled)\n",
    "\n",
    "# Get loadings (these are the DATA-DERIVED weights!)\n",
    "loadings = pd.DataFrame(\n",
    "    fa.components_.T,\n",
    "    index=FEATURES_FOR_ANALYSIS,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FACTOR ANALYSIS LOADINGS (Data-Derived Weights)\")\n",
    "print(\"=\"*70)\n",
    "print(loadings.round(3).to_string())\n",
    "\n",
    "# Interpret factors\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FACTOR INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "for i in range(n_factors):\n",
    "    col = f'Factor_{i+1}'\n",
    "    print(f\"\\n{col}:\")\n",
    "    \n",
    "    # Top positive loadings\n",
    "    top_pos = loadings[col].nlargest(3)\n",
    "    print(f\"  HIGH: {', '.join([f'{idx} ({val:+.2f})' for idx, val in top_pos.items()])}\")\n",
    "    \n",
    "    # Top negative loadings\n",
    "    top_neg = loadings[col].nsmallest(3)\n",
    "    print(f\"  LOW:  {', '.join([f'{idx} ({val:+.2f})' for idx, val in top_neg.items()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e09962-6adb-489b-845f-cc6d8aa22ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PCA RESULTS (Standardized - Equal Feature Weighting)\n",
      "======================================================================\n",
      "\n",
      "Variance explained:\n",
      "  PC1: 49.2%\n",
      "  PC2: 18.1%\n",
      "  PC3: 10.6%\n",
      "  PC4: 8.2%\n",
      "  PC5: 5.7%\n",
      "  Total (5 PCs): 91.8%\n",
      "\n",
      "PCA Loadings (first 3 components):\n",
      "                           PC1    PC2    PC3\n",
      "activity_amplitude      -0.145 -0.399 -0.342\n",
      "locomotion_duration     -0.338 -0.067 -0.317\n",
      "inactive_duration       -0.058 -0.504  0.283\n",
      "inferred_sleep_duration -0.009 -0.503  0.394\n",
      "feeding_duration        -0.101 -0.206 -0.026\n",
      "climbing_duration       -0.296  0.185  0.346\n",
      "locomotion_bout_count   -0.336 -0.111 -0.318\n",
      "climbing_bout_count     -0.316  0.148  0.265\n",
      "active_mean_bout         0.325  0.023  0.036\n",
      "active_fragmentation    -0.328  0.042 -0.022\n",
      "sleep_fragmentation      0.074  0.372 -0.198\n",
      "total_distance          -0.358  0.078  0.155\n",
      "locomotion_distance     -0.342 -0.039 -0.270\n",
      "exploration_intensity   -0.293  0.264  0.345\n"
     ]
    }
   ],
   "source": [
    "# PCA - standardized, no manual weights\n",
    "# Khatiz used this to find axes of differentiation\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Add PCA coordinates to dataframe\n",
    "for i in range(5):\n",
    "    df_analysis[f'PC{i+1}'] = X_pca[:, i]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PCA RESULTS (Standardized - Equal Feature Weighting)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nVariance explained:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var*100:.1f}%\")\n",
    "print(f\"  Total (5 PCs): {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n",
    "\n",
    "# PCA loadings\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=FEATURES_FOR_ANALYSIS,\n",
    "    columns=[f'PC{i+1}' for i in range(5)]\n",
    ")\n",
    "print(\"\\nPCA Loadings (first 3 components):\")\n",
    "print(pca_loadings[['PC1', 'PC2', 'PC3']].round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef37f737-74ef-4027-a3db-f136705ff56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "K-MEANS CLUSTERING (Data-Driven Grouping)\n",
      "======================================================================\n",
      "\n",
      "k     Silhouette   Interpretation\n",
      "----------------------------------------\n",
      "2     0.272        High vs Low estrogen?\n",
      "3     0.331        \n",
      "4     0.227        4 estrous phases?\n",
      "5     0.201        \n",
      "6     0.189        \n",
      "\n",
      "Optimal k by silhouette: 3\n",
      "\n",
      "Cluster means (key features):\n",
      "                activity_amplitude  total_distance  inactive_duration\n",
      "kmeans_cluster                                                       \n",
      "0                          28054.7         45488.5            10158.9\n",
      "1                          28329.4         77308.4             9764.0\n",
      "\n",
      "K-Means Classification (k=2):\n",
      "  Estrus-like:   106 (41.1%)\n",
      "  Diestrus-like: 152 (58.9%)\n"
     ]
    }
   ],
   "source": [
    "# K-Means - no weights, let algorithm find natural clusters\n",
    "# Khatiz used k=5 (4 estrous phases + male), we use k=2 or k=4\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"K-MEANS CLUSTERING (Data-Driven Grouping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try different k values\n",
    "print(f\"\\n{'k':<5} {'Silhouette':<12} {'Interpretation'}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "silhouettes = []\n",
    "for k in range(2, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    silhouettes.append(sil)\n",
    "    \n",
    "    if k == 2:\n",
    "        interp = \"High vs Low estrogen?\"\n",
    "    elif k == 4:\n",
    "        interp = \"4 estrous phases?\"\n",
    "    else:\n",
    "        interp = \"\"\n",
    "    print(f\"{k:<5} {sil:<12.3f} {interp}\")\n",
    "\n",
    "optimal_k = range(2, 7)[np.argmax(silhouettes)]\n",
    "print(f\"\\nOptimal k by silhouette: {optimal_k}\")\n",
    "\n",
    "# Apply k=2 for binary classification\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_analysis['kmeans_cluster'] = kmeans_2.fit_predict(X_scaled)\n",
    "\n",
    "# Determine which cluster is \"estrus-like\" based on activity\n",
    "cluster_means = df_analysis.groupby('kmeans_cluster')[FEATURES_FOR_ANALYSIS].mean()\n",
    "print(\"\\nCluster means (key features):\")\n",
    "print(cluster_means[['activity_amplitude', 'total_distance', 'inactive_duration']].round(1))\n",
    "\n",
    "# Higher activity cluster = estrus-like\n",
    "estrus_cluster = cluster_means['activity_amplitude'].idxmax()\n",
    "df_analysis['kmeans_state'] = df_analysis['kmeans_cluster'].apply(\n",
    "    lambda x: 'Estrus-like' if x == estrus_cluster else 'Diestrus-like'\n",
    ")\n",
    "\n",
    "km_counts = df_analysis['kmeans_state'].value_counts()\n",
    "print(f\"\\nK-Means Classification (k=2):\")\n",
    "print(f\"  Estrus-like:   {km_counts.get('Estrus-like', 0)} ({100*km_counts.get('Estrus-like', 0)/len(df_analysis):.1f}%)\")\n",
    "print(f\"  Diestrus-like: {km_counts.get('Diestrus-like', 0)} ({100*km_counts.get('Diestrus-like', 0)/len(df_analysis):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "461bbb25-59fa-468a-ac81-a9866a8ebad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HIERARCHICAL CLUSTERING\n",
      "======================================================================\n",
      "Hierarchical Classification (k=2):\n",
      "  Estrus-like:   207 (80.2%)\n",
      "  Diestrus-like: 51 (19.8%)\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical clustering with Ward linkage (like Khatiz)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Cut at k=2\n",
    "hc_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n",
    "df_analysis['hc_cluster'] = hc_labels\n",
    "\n",
    "# Determine which cluster is estrus-like\n",
    "hc_means = df_analysis.groupby('hc_cluster')[FEATURES_FOR_ANALYSIS].mean()\n",
    "hc_estrus = hc_means['activity_amplitude'].idxmax()\n",
    "\n",
    "df_analysis['hc_state'] = df_analysis['hc_cluster'].apply(\n",
    "    lambda x: 'Estrus-like' if x == hc_estrus else 'Diestrus-like'\n",
    ")\n",
    "\n",
    "hc_counts = df_analysis['hc_state'].value_counts()\n",
    "print(f\"Hierarchical Classification (k=2):\")\n",
    "print(f\"  Estrus-like:   {hc_counts.get('Estrus-like', 0)} ({100*hc_counts.get('Estrus-like', 0)/len(df_analysis):.1f}%)\")\n",
    "print(f\"  Diestrus-like: {hc_counts.get('Diestrus-like', 0)} ({100*hc_counts.get('Diestrus-like', 0)/len(df_analysis):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c15ff6bf-2baa-40ec-8f75-02f02baea16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FACTOR SCORE CLASSIFICATION (Data-Derived Weights)\n",
      "======================================================================\n",
      "\n",
      "Factor loadings for key features:\n",
      "                         Factor_1  Factor_2  Factor_3\n",
      "activity_amplitude          0.494    -0.437    -0.231\n",
      "locomotion_duration         1.000     0.003    -0.009\n",
      "inactive_duration           0.060    -0.995    -0.004\n",
      "total_distance              0.758    -0.053     0.635\n",
      "inferred_sleep_duration    -0.021    -0.825    -0.042\n",
      "\n",
      "Factor_1: Activity mean=0.751, Rest mean=0.019\n",
      "\n",
      "Factor_2: Activity mean=-0.162, Rest mean=-0.910\n",
      "\n",
      "Factor_3: Activity mean=0.131, Rest mean=-0.023\n"
     ]
    }
   ],
   "source": [
    "# Use Factor Analysis loadings as DATA-DERIVED weights\n",
    "# This is the closest to Khatiz's approach\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FACTOR SCORE CLASSIFICATION (Data-Derived Weights)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get factor scores (projection onto factor axes)\n",
    "factor_scores = fa.transform(X_scaled)\n",
    "df_analysis['Factor_1'] = factor_scores[:, 0]\n",
    "df_analysis['Factor_2'] = factor_scores[:, 1]\n",
    "df_analysis['Factor_3'] = factor_scores[:, 2]\n",
    "\n",
    "# Identify which factor best captures \"activity vs rest\" dimension\n",
    "# Look at factor loadings to interpret\n",
    "print(\"\\nFactor loadings for key features:\")\n",
    "key_features = ['activity_amplitude', 'locomotion_duration', 'inactive_duration', \n",
    "                'total_distance', 'inferred_sleep_duration']\n",
    "print(loadings.loc[key_features].round(3))\n",
    "\n",
    "# The factor with high activity/distance and low inactive/sleep loadings = \"estrus factor\"\n",
    "# Let's compute a composite: activity-related features positive, rest-related negative\n",
    "activity_features = ['activity_amplitude', 'locomotion_duration', 'total_distance']\n",
    "rest_features = ['inactive_duration', 'inferred_sleep_duration']\n",
    "\n",
    "# Check which factor has this pattern\n",
    "for i in range(n_factors):\n",
    "    col = f'Factor_{i+1}'\n",
    "    activity_loadings = loadings.loc[activity_features, col].mean()\n",
    "    rest_loadings = loadings.loc[rest_features, col].mean()\n",
    "    print(f\"\\n{col}: Activity mean={activity_loadings:.3f}, Rest mean={rest_loadings:.3f}\")\n",
    "    if activity_loadings > 0.3 and rest_loadings < -0.1:\n",
    "        estrus_factor = col\n",
    "        print(f\"  → This appears to be the 'Activity/Estrus' factor\")\n",
    "    elif rest_loadings > 0.3:\n",
    "        print(f\"  → This appears to be the 'Rest/Diestrus' factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af7c5c58-1a65-4770-9c9e-33890ba33b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON OF DATA-DRIVEN CLASSIFICATION METHODS\n",
      "======================================================================\n",
      "\n",
      "Classification Distribution:\n",
      "--------------------------------------------------\n",
      "  K-Means        : 41.1% Estrus-like\n",
      "  Hierarchical   : 80.2% Estrus-like\n",
      "\n",
      "--------------------------------------------------\n",
      "Method Agreement (Adjusted Rand Index):\n",
      "  K-Means vs Hierarchical: ARI = 0.320\n",
      "\n",
      "Using K-Means as primary classification\n"
     ]
    }
   ],
   "source": [
    "# Compare classifications from different methods\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON OF DATA-DRIVEN CLASSIFICATION METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "methods = {\n",
    "    'K-Means': 'kmeans_state',\n",
    "    'Hierarchical': 'hc_state',\n",
    "}\n",
    "\n",
    "print(\"\\nClassification Distribution:\")\n",
    "print(\"-\"*50)\n",
    "for name, col in methods.items():\n",
    "    if col in df_analysis.columns:\n",
    "        counts = df_analysis[col].value_counts()\n",
    "        e_pct = 100 * counts.get('Estrus-like', 0) / len(df_analysis)\n",
    "        print(f\"  {name:15}: {e_pct:.1f}% Estrus-like\")\n",
    "\n",
    "# Agreement between methods\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Method Agreement (Adjusted Rand Index):\")\n",
    "\n",
    "if 'kmeans_state' in df_analysis.columns and 'hc_state' in df_analysis.columns:\n",
    "    km_binary = (df_analysis['kmeans_state'] == 'Estrus-like').astype(int)\n",
    "    hc_binary = (df_analysis['hc_state'] == 'Estrus-like').astype(int)\n",
    "    ari = adjusted_rand_score(km_binary, hc_binary)\n",
    "    print(f\"  K-Means vs Hierarchical: ARI = {ari:.3f}\")\n",
    "\n",
    "# Use K-Means as primary classification (most common in literature)\n",
    "df_analysis['estrous_state'] = df_analysis['kmeans_state']\n",
    "print(f\"\\nUsing K-Means as primary classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95d01fc1-d505-4804-8528-526fdc2bc9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASSIFICATION VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Feature                   Estrus       Diestrus     p-value    Expected?\n",
      "----------------------------------------------------------------------\n",
      "activity_amplitude        28329.41     28054.74     0.6864     ✓ \n",
      "locomotion_duration       4178.69      2644.51      0.0000     ✓ ***\n",
      "total_distance            77308.36     45488.53     0.0000     ✓ ***\n",
      "exploration_intensity     2.78         1.64         0.0000     ✓ ***\n",
      "climbing_duration         4767.00      2452.18      0.0000     ✓ ***\n",
      "inactive_duration         9763.97      10158.88     0.2387     ✓ \n",
      "inferred_sleep_duration   3345.65      3863.23      0.0128     ✓ *\n",
      "feeding_duration          2858.08      2792.55      0.6942     ✗ \n",
      "----------------------------------------------------------------------\n",
      "Significant (p<0.05): 5/8\n",
      "Correct direction:    7/8\n"
     ]
    }
   ],
   "source": [
    "# Validate that classified groups differ on key features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLASSIFICATION VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "estrus_df = df_analysis[df_analysis['estrous_state'] == 'Estrus-like']\n",
    "diestrus_df = df_analysis[df_analysis['estrous_state'] == 'Diestrus-like']\n",
    "\n",
    "validation_features = [\n",
    "    ('activity_amplitude', 'Higher in Estrus'),\n",
    "    ('locomotion_duration', 'Higher in Estrus'),\n",
    "    ('total_distance', 'Higher in Estrus'),\n",
    "    ('exploration_intensity', 'Higher in Estrus'),\n",
    "    ('climbing_duration', 'Higher in Estrus'),\n",
    "    ('inactive_duration', 'Lower in Estrus'),\n",
    "    ('inferred_sleep_duration', 'Lower in Estrus'),\n",
    "    ('feeding_duration', 'Lower in Estrus'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Feature':<25} {'Estrus':<12} {'Diestrus':<12} {'p-value':<10} {'Expected?'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "sig_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "for feat, expected in validation_features:\n",
    "    if feat not in df_analysis.columns:\n",
    "        continue\n",
    "    \n",
    "    e_vals = estrus_df[feat].dropna()\n",
    "    d_vals = diestrus_df[feat].dropna()\n",
    "    \n",
    "    e_mean = e_vals.mean()\n",
    "    d_mean = d_vals.mean()\n",
    "    _, p_val = ttest_ind(e_vals, d_vals)\n",
    "    \n",
    "    if 'Higher' in expected:\n",
    "        correct = e_mean > d_mean\n",
    "    else:\n",
    "        correct = e_mean < d_mean\n",
    "    \n",
    "    dir_mark = \"✓\" if correct else \"✗\"\n",
    "    sig_mark = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        sig_count += 1\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print(f\"{feat:<25} {e_mean:<12.2f} {d_mean:<12.2f} {p_val:<10.4f} {dir_mark} {sig_mark}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"Significant (p<0.05): {sig_count}/{len(validation_features)}\")\n",
    "print(f\"Correct direction:    {correct_count}/{len(validation_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179d75f-b300-496f-a708-54b9c4b8ef26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
