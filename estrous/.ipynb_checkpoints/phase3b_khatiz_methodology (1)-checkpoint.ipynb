{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3B: Estrous Detection Following Khatiz et al. (2025) Methodology\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Goal\n",
    "\n",
    "**Primary Objective:** Classify individual nights of female mouse behavior into estrous cycle phases (Proestrus, Estrus, Metestrus, Diestrus) using behavioral features alone, without vaginal cytology ground truth.\n",
    "\n",
    "**Why This Matters:** The parent study (Morph2REP) investigates morphine effects in female mice. Estrous cycle phase may modulate drug response, so identifying the phase from behavioral data allows us to:\n",
    "1. Stratify analyses by hormonal state\n",
    "2. Reduce unexplained variance in morphine response\n",
    "3. Identify if certain estrous phases show differential drug sensitivity\n",
    "\n",
    "**Approach:** Replicate the statistical methodology from Khatiz et al. (2025), who used:\n",
    "- Hierarchical clustering (Spearman correlation)\n",
    "- Factor Analysis (Varimax rotation)\n",
    "- Principal Component Analysis (standardized)\n",
    "- K-Means clustering (k=5 for their study: 4 female phases + male control)\n",
    "\n",
    "**Key Difference from Phase 3A:** \n",
    "- Phase 3A used k=2 (binary: estrus-like vs diestrus-like)\n",
    "- Phase 3B uses k=4 (matching the 4 biological estrous phases)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset: JAX Envision Morph2REP Study\n",
    "\n",
    "### 2.1 Study Overview\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Study ID | 1001 (Morph2REP) |\n",
    "| Version | 2025v3.3 |\n",
    "| Platform | Envision 2025 v3.0 automated monitoring |\n",
    "| Location | JAX Envision East |\n",
    "| Animals | 54 female C57BL/6J mice (strain 664) |\n",
    "| Housing | 3 mice per cage, 18 cages total |\n",
    "| Study Period | January 7 - February 4, 2025 |\n",
    "| Light Cycle | 6:00 AM - 6:00 PM EST (12h:12h) |\n",
    "\n",
    "### 2.2 Treatment Groups\n",
    "\n",
    "| Group | Cages (Rep1) | Cages (Rep2) | N Mice |\n",
    "|-------|--------------|--------------|--------|\n",
    "| **Vehicle Control** | 4918, 4922, 4923 | 4928, 4929, 4934 | 18 |\n",
    "| 5 mg/kg Morphine | 4917, 4921, 4925 | 4927, 4931, 4932 | 18 |\n",
    "| 25 mg/kg Morphine | 4919, 4920, 4924 | 4926, 4930, 4933 | 18 |\n",
    "\n",
    "**For estrous detection, we use only VEHICLE CONTROLS** (no drug confounds).\n",
    "\n",
    "### 2.3 Experimental Timeline\n",
    "\n",
    "```\n",
    "REPLICATE 1 (Jan 7-22, 2025)\n",
    "─────────────────────────────────────────────────────────────────\n",
    "Jan 7-9     │ ACCLIMATION (3 days) - EXCLUDE from analysis\n",
    "Jan 10-13   │ BASELINE (4 days) - Clean behavioral data\n",
    "Jan 14      │ Morphine Dose #1 (6:00 AM) - Treatment begins\n",
    "Jan 15      │ Cage Change (12:00 PM) - Environmental perturbation\n",
    "Jan 17      │ Morphine Dose #2 (5:00 PM)\n",
    "Jan 18-22   │ Post-treatment monitoring\n",
    "\n",
    "REPLICATE 2 (Jan 22 - Feb 4, 2025)  \n",
    "─────────────────────────────────────────────────────────────────\n",
    "Jan 22-24   │ ACCLIMATION (3 days) - EXCLUDE from analysis\n",
    "Jan 25-27   │ BASELINE (3 days) - Clean behavioral data\n",
    "Jan 28      │ Morphine Dose #1 (5:00 PM) - Treatment begins\n",
    "Jan 29      │ Cage Change (12:00 PM) - Environmental perturbation\n",
    "Jan 31      │ Morphine Dose #2 (6:00 AM)\n",
    "Feb 1-4     │ Post-treatment monitoring\n",
    "```\n",
    "\n",
    "**For vehicle controls:** Since no drug is administered, we can use all post-acclimation days.\n",
    "\n",
    "### 2.4 Available Data Tables (S3)\n",
    "\n",
    "```\n",
    "s3://jax-envision-public-data/study_1001/2025v3.3/tabular/\n",
    "└── cage_id={cage}/date={date}/{table}.parquet\n",
    "```\n",
    "\n",
    "| Table | Description | Key Columns | Rows/Day |\n",
    "|-------|-------------|-------------|----------|\n",
    "| `animal_bouts.parquet` | Behavioral state bouts | start_time, end_time, state_name, bout_length_seconds | ~12,000 |\n",
    "| `animal_bout_metrics.parquet` | Per-bout metrics (distance) | state_name, metric_name, metric_value | ~11,000 |\n",
    "| `animal_activity_features.parquet` | 155 extracted features | total_displacement, average_velocity, stationary_ratio | ~21,600 |\n",
    "| `animal_activity_state_bboxes.parquet` | Frame-by-frame positions | bb_left, bb_top, animal_id, predicted_state | ~600,000 |\n",
    "\n",
    "### 2.5 Behavioral States\n",
    "\n",
    "| State Name | Description |\n",
    "|------------|-------------|\n",
    "| `animal_bouts.active` | General activity (not sleeping or stationary) |\n",
    "| `animal_bouts.inactive` | Stationary, not sleeping |\n",
    "| `animal_bouts.locomotion` | Walking/running |\n",
    "| `animal_bouts.feeding` | Eating behavior |\n",
    "| `animal_bouts.drinking` | Drinking behavior |\n",
    "| `animal_bouts.climbing` | Vertical exploration (hanging, rearing) |\n",
    "| `animal_bouts.inferred_sleep` | Sleep (inferred from posture + inactivity) |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Estrous Cycle Biology\n",
    "\n",
    "### 3.1 Four Phases (Khatiz determined via vaginal cytology)\n",
    "\n",
    "| Phase | Duration | Vaginal Cytology | Behavioral Signature |\n",
    "|-------|----------|------------------|---------------------|\n",
    "| **Proestrus** | ~12h | Round, nucleated epithelial cells in clusters | Moderate-high activity, exploration |\n",
    "| **Estrus** | ~12h | Irregular, anucleated, cornified squamous cells | **Highest** physically demanding activity |\n",
    "| **Metestrus** | ~24h | Neutrophils + keratinized epithelial cells | Lower activity, transition to rest |\n",
    "| **Diestrus** | ~48-72h | Mostly neutrophils, low cellularity | Lowest activity, highest feeding/sleep |\n",
    "\n",
    "### 3.2 Expected Distribution (based on phase durations)\n",
    "\n",
    "- Proestrus: ~10-15%\n",
    "- Estrus: ~10-15%  \n",
    "- Metestrus: ~20-25%\n",
    "- Diestrus: ~45-55%\n",
    "\n",
    "### 3.3 Khatiz Findings (Key Behavioral Markers)\n",
    "\n",
    "**Estrus (highest activity):**\n",
    "- 30% more physically demanding activity than males\n",
    "- Highest locomotion, climbing, jumping\n",
    "- Lower feeding behavior\n",
    "\n",
    "**Diestrus (lowest activity):**\n",
    "- Highest sleep-related behaviors\n",
    "- Highest feeding/drinking\n",
    "- More fragmented activity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup and Configuration\n",
    "---\n",
    "\n",
    "## What we're doing:\n",
    "Installing necessary Python packages and importing libraries for:\n",
    "- **duckdb**: SQL engine that can query Parquet files directly from S3\n",
    "- **pandas/numpy**: Data manipulation\n",
    "- **scipy**: Statistical tests (ANOVA, Spearman correlation, hierarchical clustering)\n",
    "- **sklearn**: Machine learning (PCA, Factor Analysis, K-Means)\n",
    "- **matplotlib/seaborn**: Visualization\n",
    "\n",
    "## Why:\n",
    "These tools replicate Khatiz's analysis pipeline. DuckDB is particularly useful because it lets us query large Parquet files on S3 without downloading them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install duckdb pyarrow umap-learn scikit-learn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tools\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, f_oneway\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# ML tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we're doing next:\n",
    "Setting up configuration variables that define:\n",
    "1. **S3 path**: Where the data lives\n",
    "2. **Vehicle control cages**: Which cages to analyze (no drug given)\n",
    "3. **Analysis dates**: Excluding acclimation period (first 3 days)\n",
    "4. **Light cycle timing**: When lights are on/off (important for separating active vs rest periods)\n",
    "\n",
    "## Why we exclude acclimation:\n",
    "Khatiz excluded the first day of recording because mice need time to adapt to being monitored. Our study design specifies 3 days of acclimation. During this period, behavior is abnormal (stress, novelty) and doesn't reflect typical estrous-related patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular\"\n",
    "\n",
    "# Vehicle control cages with timeline (excluding acclimation)\n",
    "# We ONLY use post-acclimation data because:\n",
    "# 1. Mice need time to adapt to monitoring environment\n",
    "# 2. Acclimation behavior is abnormal (stress, novelty)\n",
    "# 3. Khatiz also excluded first day for this reason\n",
    "\n",
    "VEHICLE_CAGES = {\n",
    "    'Rep1': {\n",
    "        'cages': [4918, 4922, 4923],\n",
    "        'acclimation_end': '2025-01-09',    # 3 days acclimation (Jan 7-9)\n",
    "        'analysis_start': '2025-01-10',     # First clean day\n",
    "        'analysis_end': '2025-01-22',       # End of replicate\n",
    "    },\n",
    "    'Rep2': {\n",
    "        'cages': [4928, 4929, 4934],\n",
    "        'acclimation_end': '2025-01-24',    # 3 days acclimation (Jan 22-24)\n",
    "        'analysis_start': '2025-01-25',     # First clean day\n",
    "        'analysis_end': '2025-02-04',       # End of replicate\n",
    "    }\n",
    "}\n",
    "\n",
    "# Light cycle timing\n",
    "# JAX facility: Lights ON 6:00 AM - 6:00 PM EST\n",
    "# Data is stored in UTC, so we convert:\n",
    "# - Light ON:  6:00 AM EST = 11:00 UTC\n",
    "# - Light OFF: 6:00 PM EST = 23:00 UTC\n",
    "# Mice are NOCTURNAL, so they're most active during DARK cycle (6PM-6AM)\n",
    "\n",
    "LIGHT_START_UTC = 11  # 6 AM EST\n",
    "LIGHT_END_UTC = 23    # 6 PM EST\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"\\nVehicle control cages (post-acclimation data only):\")\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"  {rep}: Cages {cfg['cages']}\")\n",
    "    print(f\"         Acclimation excluded: up to {cfg['acclimation_end']}\")\n",
    "    print(f\"         Analysis period: {cfg['analysis_start']} to {cfg['analysis_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Loading\n",
    "---\n",
    "\n",
    "## What we're doing:\n",
    "Creating a function to load Parquet files from S3 for a specific cage and date range.\n",
    "\n",
    "## How it works:\n",
    "1. Connect to DuckDB with S3 access enabled\n",
    "2. Loop through each date in the range\n",
    "3. Construct the S3 path: `s3://bucket/study/cage_id={cage}/date={date}/table.parquet`\n",
    "4. Read the Parquet file and add cage_id and date columns\n",
    "5. Concatenate all days into one DataFrame\n",
    "\n",
    "## Why Parquet + DuckDB:\n",
    "- Parquet is columnar format (fast for analytics)\n",
    "- DuckDB can query S3 directly without downloading entire files\n",
    "- Hive-style partitioning (`cage_id=X/date=Y/`) makes it easy to select specific subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_s3(cage_id, start_date, end_date, table_name):\n",
    "    \"\"\"\n",
    "    Load parquet table from S3 for a single cage across a date range.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cage_id : int - The cage number (e.g., 4918)\n",
    "    start_date : str - Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str - End date in 'YYYY-MM-DD' format  \n",
    "    table_name : str - Name of parquet file (e.g., 'animal_bouts.parquet')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with all data for that cage/date range\n",
    "    \"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")  # Enable S3 access\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/{table_name}\"\n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            # Skip missing dates silently\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we're doing next:\n",
    "Loading `animal_bouts.parquet` for all 6 vehicle control cages.\n",
    "\n",
    "## What this table contains:\n",
    "Each row is a **bout** - a continuous period of one behavioral state. For example:\n",
    "- Mouse started sleeping at 14:00:00, ended at 14:15:30 → bout_length = 930 seconds\n",
    "- Mouse was locomoting from 14:15:30 to 14:16:00 → bout_length = 30 seconds\n",
    "\n",
    "## Key columns:\n",
    "- `animal_id`: Which mouse (1, 2, or 3 in each cage)\n",
    "- `state_name`: Behavioral state (e.g., 'animal_bouts.locomotion')\n",
    "- `start_time`, `end_time`: When the bout occurred\n",
    "- `bout_length_seconds`: Duration of the bout\n",
    "\n",
    "## Why we need this:\n",
    "Khatiz computed **total duration** of each behavior per day. We'll do the same by summing bout lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load animal_bouts.parquet for all vehicle control cages\n",
    "print(\"Loading animal_bouts.parquet (behavioral state bouts)...\")\n",
    "print(\"=\"*70)\n",
    "print(\"This contains one row per behavioral bout (continuous period of one state)\")\n",
    "print(\"We'll sum bout durations to get total time in each state per night.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_bouts = []\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"\\n{rep}:\")\n",
    "    for cage_id in cfg['cages']:\n",
    "        print(f\"  Cage {cage_id} ({cfg['analysis_start']} to {cfg['analysis_end']})...\", end=\" \")\n",
    "        df = load_parquet_s3(cage_id, cfg['analysis_start'], cfg['analysis_end'], 'animal_bouts.parquet')\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep\n",
    "            all_bouts.append(df)\n",
    "            print(f\"{len(df):,} bouts loaded\")\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "\n",
    "df_bouts = pd.concat(all_bouts, ignore_index=True)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total bouts loaded: {len(df_bouts):,}\")\n",
    "print(f\"Unique animals: {df_bouts['animal_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we're doing next:\n",
    "Loading `animal_bout_metrics.parquet` which contains **distance traveled** during each bout.\n",
    "\n",
    "## Why this matters:\n",
    "Khatiz found that **exploration** (distance traveled, especially during locomotion and climbing) was a key differentiator between estrous phases:\n",
    "- Estrus: Highest exploration, longest distances\n",
    "- Diestrus: Lowest exploration\n",
    "\n",
    "## Key columns:\n",
    "- `metric_name`: Type of metric (usually 'distance_traveled')\n",
    "- `metric_value`: The value (in pixels or mm)\n",
    "- `state_name`: Which behavioral state the distance was measured during"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load animal_bout_metrics.parquet (distance traveled per bout)\n",
    "print(\"Loading animal_bout_metrics.parquet (distance traveled)...\")\n",
    "print(\"=\"*70)\n",
    "print(\"This contains distance traveled during each bout.\")\n",
    "print(\"Exploration metrics are key for distinguishing estrous phases.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_metrics = []\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"\\n{rep}:\")\n",
    "    for cage_id in cfg['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_parquet_s3(cage_id, cfg['analysis_start'], cfg['analysis_end'], 'animal_bout_metrics.parquet')\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep\n",
    "            all_metrics.append(df)\n",
    "            print(f\"{len(df):,} rows\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bout_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total bout metrics loaded: {len(df_bout_metrics):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we're doing next:\n",
    "Inspecting the data to understand what behavioral states are available.\n",
    "\n",
    "## Why:\n",
    "Before computing features, we need to know:\n",
    "1. What states exist in the data\n",
    "2. How many bouts of each type\n",
    "3. The naming convention (e.g., 'animal_bouts.locomotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the behavioral states available in our data\n",
    "print(\"Behavioral states in animal_bouts.parquet:\")\n",
    "print(\"=\"*50)\n",
    "state_counts = df_bouts['state_name'].value_counts()\n",
    "for state, count in state_counts.items():\n",
    "    print(f\"  {state}: {count:,} bouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Feature Engineering\n",
    "---\n",
    "\n",
    "## What we're doing:\n",
    "Computing behavioral features for each animal-night, following Khatiz's approach.\n",
    "\n",
    "## Khatiz analyzed three time periods separately:\n",
    "1. **24-hour**: Full day summary\n",
    "2. **Light cycle** (6 AM - 6 PM): Rest period for nocturnal mice\n",
    "3. **Dark cycle** (6 PM - 6 AM): Active period for mice\n",
    "\n",
    "## Why separate periods matter:\n",
    "Khatiz found that estrous differences were most pronounced during the **dark cycle** when mice are naturally active. Light cycle showed different patterns (more sleep-related differences).\n",
    "\n",
    "## Features we'll compute (matching Khatiz):\n",
    "\n",
    "**Duration features:**\n",
    "- Total time in each state (locomotion, sleep, feeding, etc.)\n",
    "\n",
    "**Bout features:**\n",
    "- Number of bouts (how many times they started that behavior)\n",
    "- Mean bout length (how long each bout lasted on average)\n",
    "\n",
    "**Derived features:**\n",
    "- `physically_demanding` = locomotion + climbing (Khatiz's cluster)\n",
    "- `sleep_related` = inferred sleep duration\n",
    "- `feeding_resourcing` = feeding + drinking\n",
    "- `fragmentation` = bout_count / duration (more bouts per unit time = more fragmented)\n",
    "- `exploration_intensity` = distance / activity time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_period_features(df_bouts, df_metrics, period='dark'):\n",
    "    \"\"\"\n",
    "    Compute behavioral features for a specific time period.\n",
    "    \n",
    "    This function:\n",
    "    1. Filters bouts to the specified period (dark/light/24h)\n",
    "    2. Groups by animal and date\n",
    "    3. Computes duration, bout count, and mean bout length for each state\n",
    "    4. Merges distance metrics\n",
    "    5. Computes derived features (physically_demanding, etc.)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_bouts : DataFrame with bout data\n",
    "    df_metrics : DataFrame with distance metrics\n",
    "    period : 'dark' (6PM-6AM), 'light' (6AM-6PM), or '24h'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with one row per animal-night, columns for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map short names to full state names in the data\n",
    "    STATE_MAP = {\n",
    "        'active': 'animal_bouts.active',\n",
    "        'inactive': 'animal_bouts.inactive',\n",
    "        'locomotion': 'animal_bouts.locomotion',\n",
    "        'feeding': 'animal_bouts.feeding',\n",
    "        'drinking': 'animal_bouts.drinking',\n",
    "        'climbing': 'animal_bouts.climbing',\n",
    "        'inferred_sleep': 'animal_bouts.inferred_sleep',\n",
    "    }\n",
    "    \n",
    "    # =====================================================================\n",
    "    # STEP 1: Parse timestamps and determine light/dark period\n",
    "    # =====================================================================\n",
    "    df_b = df_bouts.copy()\n",
    "    df_b['start_time'] = pd.to_datetime(df_b['start_time'])\n",
    "    df_b['hour_utc'] = df_b['start_time'].dt.hour\n",
    "    \n",
    "    # Light cycle: 11:00-23:00 UTC (6AM-6PM EST)\n",
    "    # Dark cycle: 23:00-11:00 UTC (6PM-6AM EST)\n",
    "    df_b['is_light'] = (df_b['hour_utc'] >= LIGHT_START_UTC) & (df_b['hour_utc'] < LIGHT_END_UTC)\n",
    "    df_b['is_dark'] = ~df_b['is_light']\n",
    "    \n",
    "    # Night date: For dark cycle, bouts after midnight belong to previous night\n",
    "    # e.g., 2AM on Jan 11 is part of the \"Jan 10 night\"\n",
    "    df_b['night_date'] = df_b['start_time'].dt.date\n",
    "    df_b.loc[df_b['hour_utc'] < LIGHT_START_UTC, 'night_date'] = (\n",
    "        pd.to_datetime(df_b.loc[df_b['hour_utc'] < LIGHT_START_UTC, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    df_b['day_date'] = df_b['start_time'].dt.date\n",
    "    \n",
    "    # =====================================================================\n",
    "    # STEP 2: Filter to the requested time period\n",
    "    # =====================================================================\n",
    "    if period == 'dark':\n",
    "        df_b = df_b[df_b['is_dark']]\n",
    "        date_col = 'night_date'\n",
    "    elif period == 'light':\n",
    "        df_b = df_b[df_b['is_light']]\n",
    "        date_col = 'day_date'\n",
    "    else:  # 24h\n",
    "        date_col = 'day_date'\n",
    "    \n",
    "    # =====================================================================\n",
    "    # STEP 3: Aggregate bouts by animal and date\n",
    "    # =====================================================================\n",
    "    results = []\n",
    "    for (cage_id, animal_id, pdate), grp in df_b.groupby(['cage_id', 'animal_id', date_col]):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'period_date': pdate,\n",
    "            'period': period\n",
    "        }\n",
    "        \n",
    "        # For each behavioral state, compute duration, bout count, mean bout length\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_df = grp[grp['state_name'] == full_name]\n",
    "            row[f'{short_name}_duration'] = state_df['bout_length_seconds'].sum()\n",
    "            row[f'{short_name}_bout_count'] = len(state_df)\n",
    "            row[f'{short_name}_mean_bout'] = state_df['bout_length_seconds'].mean() if len(state_df) > 0 else 0\n",
    "        \n",
    "        row['total_duration'] = grp['bout_length_seconds'].sum()\n",
    "        results.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(results)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # STEP 4: Process distance metrics similarly\n",
    "    # =====================================================================\n",
    "    df_m = df_metrics.copy()\n",
    "    df_m['start_time'] = pd.to_datetime(df_m['start_time'])\n",
    "    df_m['hour_utc'] = df_m['start_time'].dt.hour\n",
    "    df_m['is_light'] = (df_m['hour_utc'] >= LIGHT_START_UTC) & (df_m['hour_utc'] < LIGHT_END_UTC)\n",
    "    df_m['is_dark'] = ~df_m['is_light']\n",
    "    df_m['night_date'] = df_m['start_time'].dt.date\n",
    "    df_m.loc[df_m['hour_utc'] < LIGHT_START_UTC, 'night_date'] = (\n",
    "        pd.to_datetime(df_m.loc[df_m['hour_utc'] < LIGHT_START_UTC, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    df_m['day_date'] = df_m['start_time'].dt.date\n",
    "    \n",
    "    if period == 'dark':\n",
    "        df_m = df_m[df_m['is_dark']]\n",
    "    elif period == 'light':\n",
    "        df_m = df_m[df_m['is_light']]\n",
    "    \n",
    "    dist_results = []\n",
    "    for (cage_id, animal_id, pdate), grp in df_m.groupby(['cage_id', 'animal_id', date_col]):\n",
    "        row = {'cage_id': cage_id, 'animal_id': animal_id, 'period_date': pdate}\n",
    "        row['total_distance'] = grp['metric_value'].sum()\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_df = grp[grp['state_name'] == full_name]\n",
    "            row[f'{short_name}_distance'] = state_df['metric_value'].sum()\n",
    "        dist_results.append(row)\n",
    "    \n",
    "    df_dist = pd.DataFrame(dist_results)\n",
    "    \n",
    "    # Merge distance data\n",
    "    if len(df_dist) > 0:\n",
    "        df_summary = df_summary.merge(df_dist, on=['cage_id', 'animal_id', 'period_date'], how='left')\n",
    "    \n",
    "    # =====================================================================\n",
    "    # STEP 5: Compute derived features (matching Khatiz's clusters)\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Physically demanding = locomotion + climbing (Khatiz Cluster 4)\n",
    "    df_summary['physically_demanding'] = df_summary['locomotion_duration'] + df_summary['climbing_duration']\n",
    "    \n",
    "    # Sleep-related = inferred sleep (Khatiz Cluster 3)\n",
    "    df_summary['sleep_related'] = df_summary['inferred_sleep_duration']\n",
    "    \n",
    "    # Feeding/resourcing = feeding + drinking (Khatiz Cluster 1)\n",
    "    df_summary['feeding_resourcing'] = df_summary['feeding_duration'] + df_summary['drinking_duration']\n",
    "    \n",
    "    # Activity amplitude = active + locomotion\n",
    "    df_summary['activity_amplitude'] = df_summary['active_duration'] + df_summary['locomotion_duration']\n",
    "    \n",
    "    # Fragmentation = bout_count / duration\n",
    "    # Higher fragmentation = more transitions, shorter bouts\n",
    "    df_summary['sleep_fragmentation'] = df_summary['inferred_sleep_bout_count'] / (df_summary['inferred_sleep_duration'] + 1)\n",
    "    df_summary['active_fragmentation'] = df_summary['active_bout_count'] / (df_summary['active_duration'] + 1)\n",
    "    \n",
    "    # Exploration intensity = distance / activity time\n",
    "    if 'total_distance' in df_summary.columns:\n",
    "        df_summary['exploration_intensity'] = df_summary['total_distance'] / (df_summary['activity_amplitude'] + 1)\n",
    "    \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we're doing next:\n",
    "Running the feature computation for all three time periods.\n",
    "\n",
    "## Expected output:\n",
    "- **Dark cycle**: One row per animal per night (e.g., 18 animals × ~13 nights = ~234 rows)\n",
    "- **Light cycle**: One row per animal per day\n",
    "- **24-hour**: Full day summaries\n",
    "\n",
    "## Note on animal_id = 0:\n",
    "Sometimes the tracking system fails to identify which mouse is which. These get labeled as `animal_id = 0`. We exclude these from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for each time period\n",
    "print(\"Computing behavioral features for each time period...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dark cycle - PRIMARY ANALYSIS (mice most active at night)\n",
    "print(\"\\n[1/3] Dark cycle (6 PM - 6 AM EST) - PRIMARY ANALYSIS\")\n",
    "print(\"      Mice are nocturnal, so this is when estrous differences are most visible.\")\n",
    "df_dark = compute_period_features(df_bouts, df_bout_metrics, 'dark')\n",
    "df_dark = df_dark[df_dark['animal_id'] != 0]  # Remove tracking errors\n",
    "print(f\"      Result: {len(df_dark)} animal-nights from {df_dark['animal_id'].nunique()} animals\")\n",
    "\n",
    "# Light cycle - SECONDARY\n",
    "print(\"\\n[2/3] Light cycle (6 AM - 6 PM EST)\")\n",
    "print(\"      Mice rest during day, but sleep patterns differ by estrous phase.\")\n",
    "df_light = compute_period_features(df_bouts, df_bout_metrics, 'light')\n",
    "df_light = df_light[df_light['animal_id'] != 0]\n",
    "print(f\"      Result: {len(df_light)} animal-days\")\n",
    "\n",
    "# 24-hour - FULL DAY\n",
    "print(\"\\n[3/3] 24-hour summaries\")\n",
    "print(\"      Combined light + dark for overall daily patterns.\")\n",
    "df_24h = compute_period_features(df_bouts, df_bout_metrics, '24h')\n",
    "df_24h = df_24h[df_24h['animal_id'] != 0]\n",
    "print(f\"      Result: {len(df_24h)} animal-days\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Feature computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Analysis (Khatiz Methodology)\n",
    "---\n",
    "\n",
    "## What we're doing:\n",
    "Now we apply Khatiz's analysis pipeline to classify nights into estrous phases.\n",
    "\n",
    "## The pipeline:\n",
    "1. **Select features** - Choose which behavioral metrics to include\n",
    "2. **Standardize** - Put all features on same scale (mean=0, std=1)\n",
    "3. **Hierarchical clustering of FEATURES** - See which behaviors co-vary\n",
    "4. **Factor Analysis** - Find underlying behavioral dimensions\n",
    "5. **PCA** - Reduce dimensionality for visualization\n",
    "6. **K-Means clustering of NIGHTS** - Group nights by behavioral similarity\n",
    "7. **Label clusters** - Assign estrous phase names based on behavioral profiles\n",
    "8. **Validate** - Test if clusters differ significantly (ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Feature Selection\n",
    "\n",
    "### What we're doing:\n",
    "Selecting which features to include in the analysis.\n",
    "\n",
    "### Why these features:\n",
    "We're matching Khatiz's behavioral categories:\n",
    "- **Duration features**: Time spent in each state (like Khatiz's 35 activities)\n",
    "- **Bout features**: Number of bouts, mean bout length\n",
    "- **Derived features**: Combinations that match Khatiz's clusters (physically_demanding, etc.)\n",
    "- **Distance features**: Exploration metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for analysis\n",
    "FEATURES = [\n",
    "    # Duration features (total time in each state)\n",
    "    'active_duration', 'inactive_duration', 'locomotion_duration',\n",
    "    'feeding_duration', 'drinking_duration', 'climbing_duration', 'inferred_sleep_duration',\n",
    "    \n",
    "    # Bout features (how many times, how long each)\n",
    "    'active_bout_count', 'locomotion_bout_count', 'climbing_bout_count', 'inferred_sleep_bout_count',\n",
    "    \n",
    "    # Derived features (matching Khatiz clusters)\n",
    "    'physically_demanding',   # locomotion + climbing (Khatiz Cluster 4)\n",
    "    'sleep_related',          # sleep duration (Khatiz Cluster 3)\n",
    "    'feeding_resourcing',     # feeding + drinking (Khatiz Cluster 1)\n",
    "    'activity_amplitude',     # overall activity level\n",
    "    \n",
    "    # Fragmentation features\n",
    "    'sleep_fragmentation',    # more bouts = more fragmented sleep\n",
    "    'active_fragmentation',\n",
    "    \n",
    "    # Distance/exploration features\n",
    "    'total_distance',\n",
    "    'locomotion_distance',\n",
    "    'climbing_distance',\n",
    "    'exploration_intensity',   # distance per unit activity time\n",
    "]\n",
    "\n",
    "# Keep only features that exist in our data\n",
    "FEATURES = [f for f in FEATURES if f in df_dark.columns]\n",
    "\n",
    "print(f\"Features selected for analysis: {len(FEATURES)}\")\n",
    "print(\"=\"*50)\n",
    "for i, f in enumerate(FEATURES, 1):\n",
    "    print(f\"  {i:2d}. {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2: Data Preparation\n",
    "\n",
    "### What we're doing:\n",
    "1. Remove any rows with missing values\n",
    "2. Extract the feature matrix\n",
    "3. **Standardize** (z-score normalization)\n",
    "\n",
    "### Why standardize:\n",
    "Different features have different scales:\n",
    "- `locomotion_duration` might be 0-5000 seconds\n",
    "- `sleep_fragmentation` might be 0-0.1\n",
    "\n",
    "Without standardization, features with larger values would dominate the analysis. Standardization puts all features on the same scale (mean=0, standard deviation=1).\n",
    "\n",
    "### Khatiz did this too:\n",
    "\"Principal component analysis aims to extract the main orthogonal contributors... we **standardized the dataset**\" (Methods section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for analysis\n",
    "df_analysis = df_dark.dropna(subset=FEATURES).copy()\n",
    "X = df_analysis[FEATURES].values\n",
    "\n",
    "# Standardize: z = (x - mean) / std\n",
    "# After this, each feature has mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Data prepared for analysis:\")\n",
    "print(f\"  • Observations (animal-nights): {X_scaled.shape[0]}\")\n",
    "print(f\"  • Features: {X_scaled.shape[1]}\")\n",
    "print(f\"\\nAfter standardization:\")\n",
    "print(f\"  • Mean of each feature: ~0 (actual: {X_scaled.mean(axis=0).mean():.6f})\")\n",
    "print(f\"  • Std of each feature:  ~1 (actual: {X_scaled.std(axis=0).mean():.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Hierarchical Clustering of FEATURES\n",
    "\n",
    "### What we're doing:\n",
    "Clustering the **features** (not nights) to see which behaviors co-vary.\n",
    "\n",
    "### This is exactly what Khatiz did (Figure 2):\n",
    "They used hierarchical clustering to identify 5 behavioral clusters:\n",
    "1. Feeding & Resource Interaction (Drink, Eat, Sniff...)\n",
    "2. Exploratory (Stretch Body, Hang Vertically...)\n",
    "3. Sleep-Related (Sleep, Awaken, Twitch)\n",
    "4. Physically Demanding (Dig, Walk Left/Right, Jump...)\n",
    "5. Habituation-Like (Pause, Groom, Turn...)\n",
    "\n",
    "### Method:\n",
    "1. Compute **Spearman correlation** between all pairs of features\n",
    "2. Convert to distance: distance = 1 - correlation\n",
    "3. Use **complete linkage** hierarchical clustering\n",
    "4. Visualize as dendrogram\n",
    "\n",
    "### Why Spearman (not Pearson):\n",
    "Behavioral data is often not normally distributed. Spearman correlation is based on ranks, making it robust to outliers and non-normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering of FEATURES\n",
    "# This groups features that co-vary together across nights\n",
    "\n",
    "# Step 1: Compute Spearman correlation matrix\n",
    "corr_matrix = df_analysis[FEATURES].corr(method='spearman')\n",
    "\n",
    "# Step 2: Convert to distance (1 - correlation)\n",
    "# Highly correlated features have low distance\n",
    "distance_matrix = 1 - corr_matrix\n",
    "\n",
    "# Step 3: Hierarchical clustering with complete linkage\n",
    "linkage_features = linkage(squareform(distance_matrix), method='complete')\n",
    "\n",
    "# Step 4: Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "dendrogram(\n",
    "    linkage_features,\n",
    "    labels=FEATURES,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=9,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Hierarchical Clustering of Behavioral Features\\n(Spearman Correlation, Complete Linkage)\\n'\n",
    "             'Features that cluster together co-vary across nights', fontsize=12)\n",
    "ax.set_ylabel('Distance (1 - Spearman r)')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Potential cut height')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"• Features that join at LOW height are highly correlated\")\n",
    "print(\"• Features that join at HIGH height are less related\")\n",
    "print(\"• We expect activity features to cluster together, sleep features together, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.4: Factor Analysis (Varimax Rotation)\n",
    "\n",
    "### What we're doing:\n",
    "Using Factor Analysis to identify underlying **latent dimensions** of behavior.\n",
    "\n",
    "### What this means:\n",
    "Instead of 21 individual features, we ask: \"What are the underlying behavioral dimensions that explain the variation in all these features?\"\n",
    "\n",
    "For example, Khatiz found 7 factors:\n",
    "- Factor 1: Exploratory (Hang, Walk, Climb)\n",
    "- Factor 2: Foraging (Dig, Forage, Sniff)\n",
    "- Factor 3: Postural Locomotor (Rear Up, Come Down)\n",
    "- Factor 4: Sleep-Related (Sleep, Groom, Pause)\n",
    "- Factor 5: Physically Demanding (Jump, Land)\n",
    "- Factor 6: Pre/Post Sleep (Awaken, Twitch)\n",
    "- Factor 7: Nourishment (Eat, Drink)\n",
    "\n",
    "### Varimax rotation:\n",
    "Makes the factors easier to interpret by maximizing the variance of loadings within each factor. This makes each factor have a few high loadings and many low loadings.\n",
    "\n",
    "### Output - Loadings:\n",
    "Each feature gets a \"loading\" on each factor. High loading (>0.3) means that feature is strongly associated with that factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Analysis with Varimax rotation\n",
    "# This finds underlying behavioral dimensions\n",
    "\n",
    "# Number of factors (Khatiz used 7, we'll use min of 7 or features-1)\n",
    "n_factors = min(7, len(FEATURES) - 1)\n",
    "\n",
    "# Fit factor analysis\n",
    "fa = FactorAnalysis(n_components=n_factors, rotation='varimax', random_state=42)\n",
    "fa.fit(X_scaled)\n",
    "\n",
    "# Extract loadings (how much each feature contributes to each factor)\n",
    "loadings = pd.DataFrame(\n",
    "    fa.components_.T,  # Transpose to get features as rows\n",
    "    index=FEATURES,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"Factor Analysis Loadings:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loadings show how much each feature contributes to each factor.\")\n",
    "print(\"|loading| > 0.3 is typically considered meaningful.\")\n",
    "print(\"=\"*80)\n",
    "print(loadings.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize factor loadings as heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    loadings,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    annot_kws={'size': 8}\n",
    ")\n",
    "ax.set_title('Factor Analysis Loadings (Dark Cycle)\\n'\n",
    "             'Red = positive loading, Blue = negative loading', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret each factor\n",
    "print(\"\\nFactor Interpretation:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(n_factors):\n",
    "    col = f'Factor_{i+1}'\n",
    "    top_pos = loadings[col].nlargest(3)\n",
    "    top_neg = loadings[col].nsmallest(3)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    high = [f\"{idx} ({val:+.2f})\" for idx, val in top_pos.items() if val > 0.25]\n",
    "    low = [f\"{idx} ({val:+.2f})\" for idx, val in top_neg.items() if val < -0.25]\n",
    "    \n",
    "    if high:\n",
    "        print(f\"  HIGH: {', '.join(high)}\")\n",
    "    if low:\n",
    "        print(f\"  LOW:  {', '.join(low)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: PCA (Principal Component Analysis)\n",
    "\n",
    "### What we're doing:\n",
    "Reducing the dimensionality of the data to visualize patterns.\n",
    "\n",
    "### How PCA works:\n",
    "1. Find the direction of maximum variance in the data (PC1)\n",
    "2. Find the next direction perpendicular to PC1 with maximum variance (PC2)\n",
    "3. Continue for more components\n",
    "\n",
    "### Why we use it:\n",
    "- Can't visualize 21-dimensional data directly\n",
    "- PC1 and PC2 capture the most important variation\n",
    "- Helps us see if there are natural clusters in the data\n",
    "\n",
    "### What to look for:\n",
    "- If estrous phases are distinct, we should see some separation in PCA space\n",
    "- If phases overlap a lot, estrous signal may be weak (consistent with Levy et al. finding that estrous = only 3% of variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - dimensionality reduction for visualization\n",
    "pca = PCA(n_components=min(10, len(FEATURES)))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Add PC coordinates to dataframe for later plotting\n",
    "for i in range(min(5, X_pca.shape[1])):\n",
    "    df_analysis[f'PC{i+1}'] = X_pca[:, i]\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVariance explained by each principal component:\")\n",
    "cumulative = 0\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:5]):\n",
    "    cumulative += var\n",
    "    bar = '█' * int(var * 50)\n",
    "    print(f\"  PC{i+1}: {var*100:5.1f}% {bar}  (cumulative: {cumulative*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nFirst 2 PCs explain {pca.explained_variance_ratio_[:2].sum()*100:.1f}% of variance\")\n",
    "print(f\"First 5 PCs explain {pca.explained_variance_ratio_[:5].sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.6: K-Means Clustering (k=4 for 4 Estrous Phases)\n",
    "\n",
    "### What we're doing:\n",
    "Clustering the **nights** (not features) into groups based on behavioral similarity.\n",
    "\n",
    "### Why k=4:\n",
    "There are 4 biological estrous phases:\n",
    "1. **Proestrus** (~12h) - Estrogen rising\n",
    "2. **Estrus** (~12h) - Ovulation, highest activity\n",
    "3. **Metestrus** (~24h) - Transition\n",
    "4. **Diestrus** (~48-72h) - Lowest activity\n",
    "\n",
    "### Alternative k values:\n",
    "- k=2: Binary (high estrogen vs low estrogen)\n",
    "- k=3: Often optimal by silhouette score\n",
    "- k=5: What Khatiz used (4 phases + male control)\n",
    "\n",
    "### Silhouette score:\n",
    "Measures how well-separated the clusters are. Range is -1 to 1:\n",
    "- 1.0 = Perfect separation\n",
    "- 0.0 = Overlapping clusters\n",
    "- <0 = Points assigned to wrong cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different k values to find optimal clustering\n",
    "print(\"K-Means Clustering: Testing Different k Values\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSilhouette score measures cluster separation (higher = better separated)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "k_range = range(2, 8)\n",
    "silhouettes = []\n",
    "inertias = []\n",
    "\n",
    "print(f\"{'k':<5} {'Silhouette':<12} {'Inertia':<15} {'Biological Interpretation'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    silhouettes.append(sil)\n",
    "    inertias.append(km.inertia_)\n",
    "    \n",
    "    interp = {\n",
    "        2: \"High estrogen (P+E) vs Low (M+D)\",\n",
    "        3: \"Might be statistically optimal\",\n",
    "        4: \"4 estrous phases (P, E, M, D)\",\n",
    "        5: \"Khatiz used this (4 phases + male)\",\n",
    "        6: \"Too many for 4-phase cycle\",\n",
    "        7: \"Too many\"\n",
    "    }.get(k, \"\")\n",
    "    \n",
    "    marker = \" ← OPTIMAL\" if sil == max(silhouettes) else \"\"\n",
    "    print(f\"{k:<5} {sil:<12.3f} {km.inertia_:<15.1f} {interp}{marker}\")\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouettes)]\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nStatistically optimal k: {optimal_k} (highest silhouette)\")\n",
    "print(f\"Biologically meaningful k: 4 (matches 4 estrous phases)\")\n",
    "print(f\"\\nWe'll use k=4 for biological interpretability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(list(k_range), silhouettes, 'bo-', markersize=10, linewidth=2)\n",
    "ax.axvline(x=4, color='red', linestyle='--', linewidth=2, label='k=4 (biological)')\n",
    "ax.axvline(x=optimal_k, color='green', linestyle=':', linewidth=2, label=f'k={optimal_k} (optimal)')\n",
    "ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax.set_title('Silhouette Score vs k\\n(Higher = better separation)', fontsize=12)\n",
    "ax.set_xticks(list(k_range))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(list(k_range), inertias, 'ro-', markersize=10, linewidth=2)\n",
    "ax.axvline(x=4, color='red', linestyle='--', linewidth=2, label='k=4')\n",
    "ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax.set_ylabel('Inertia (Within-cluster SS)', fontsize=12)\n",
    "ax.set_title('Elbow Plot\\n(Look for \"elbow\" where curve bends)', fontsize=12)\n",
    "ax.set_xticks(list(k_range))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.7: Apply K-Means with k=4\n",
    "\n",
    "### What we're doing:\n",
    "Running K-Means with k=4 to assign each night to one of 4 clusters.\n",
    "\n",
    "### The algorithm:\n",
    "1. Randomly initialize 4 cluster centers\n",
    "2. Assign each night to nearest center (Euclidean distance in standardized feature space)\n",
    "3. Update centers to be mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "### Output:\n",
    "Each night gets a cluster label (0, 1, 2, or 3). We'll then characterize each cluster to assign biological phase names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with k=4\n",
    "K = 4\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "df_analysis['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"K-Means Clustering Results (k={K}):\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nCluster sizes:\")\n",
    "counts = df_analysis['cluster'].value_counts().sort_index()\n",
    "for c, n in counts.items():\n",
    "    pct = 100 * n / len(df_analysis)\n",
    "    bar = '█' * int(pct / 2)\n",
    "    print(f\"  Cluster {c}: {n:3d} nights ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\nExpected distribution (based on cycle biology):\")\n",
    "print(\"  Proestrus:  10-15%\")\n",
    "print(\"  Estrus:     10-15%\")\n",
    "print(\"  Metestrus:  20-25%\")\n",
    "print(\"  Diestrus:   45-55%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.8: Characterize and Label Clusters\n",
    "\n",
    "### What we're doing:\n",
    "Computing the mean of key behavioral features for each cluster, then assigning estrous phase labels based on Khatiz's findings.\n",
    "\n",
    "### Labeling logic (from Khatiz):\n",
    "- **Estrus**: Highest physically demanding activity, locomotion, climbing\n",
    "- **Diestrus**: Highest sleep, highest feeding, lowest activity\n",
    "- **Proestrus**: Second highest activity (between Estrus and Metestrus)\n",
    "- **Metestrus**: Intermediate, transition phase\n",
    "\n",
    "### Important caveat:\n",
    "Without vaginal cytology ground truth, these are **behavioral classifications** that we're labeling based on expected patterns. They may not perfectly correspond to true estrous phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterize each cluster by key behavioral features\n",
    "key_features = ['physically_demanding', 'activity_amplitude', 'locomotion_duration',\n",
    "                'climbing_duration', 'sleep_related', 'feeding_resourcing', 'total_distance']\n",
    "key_features = [f for f in key_features if f in df_analysis.columns]\n",
    "\n",
    "cluster_means = df_analysis.groupby('cluster')[key_features].mean()\n",
    "\n",
    "print(\"Cluster Behavioral Profiles:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMean values for key features in each cluster:\")\n",
    "print(cluster_means.round(1).to_string())\n",
    "\n",
    "# Normalize for easier comparison (z-score across clusters)\n",
    "cluster_means_z = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "print(\"\\nZ-scored (for comparison - positive = above average, negative = below):\")\n",
    "print(cluster_means_z.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign estrous phase labels based on behavioral signatures\n",
    "print(\"\\nAssigning Estrous Phase Labels:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estrus = highest physically demanding activity\n",
    "estrus_cluster = cluster_means['physically_demanding'].idxmax()\n",
    "print(f\"\\n1. ESTRUS = Cluster {estrus_cluster}\")\n",
    "print(f\"   Reason: Highest physically_demanding ({cluster_means.loc[estrus_cluster, 'physically_demanding']:.1f})\")\n",
    "\n",
    "# Diestrus = highest sleep\n",
    "diestrus_cluster = cluster_means['sleep_related'].idxmax()\n",
    "print(f\"\\n2. DIESTRUS = Cluster {diestrus_cluster}\")\n",
    "print(f\"   Reason: Highest sleep_related ({cluster_means.loc[diestrus_cluster, 'sleep_related']:.1f})\")\n",
    "\n",
    "# Remaining clusters are Proestrus and Metestrus\n",
    "remaining = [c for c in range(K) if c not in [estrus_cluster, diestrus_cluster]]\n",
    "print(f\"\\n3. Remaining clusters: {remaining}\")\n",
    "\n",
    "if len(remaining) >= 2:\n",
    "    # Proestrus = higher activity of the two remaining\n",
    "    if cluster_means.loc[remaining[0], 'activity_amplitude'] > cluster_means.loc[remaining[1], 'activity_amplitude']:\n",
    "        proestrus_cluster = remaining[0]\n",
    "        metestrus_cluster = remaining[1]\n",
    "    else:\n",
    "        proestrus_cluster = remaining[1]\n",
    "        metestrus_cluster = remaining[0]\n",
    "    \n",
    "    print(f\"\\n   PROESTRUS = Cluster {proestrus_cluster}\")\n",
    "    print(f\"   Reason: Higher activity_amplitude of remaining ({cluster_means.loc[proestrus_cluster, 'activity_amplitude']:.1f})\")\n",
    "    print(f\"\\n   METESTRUS = Cluster {metestrus_cluster}\")\n",
    "    print(f\"   Reason: Lower activity_amplitude of remaining ({cluster_means.loc[metestrus_cluster, 'activity_amplitude']:.1f})\")\n",
    "else:\n",
    "    proestrus_cluster = remaining[0] if remaining else None\n",
    "    metestrus_cluster = None\n",
    "\n",
    "# Create label mapping\n",
    "labels = {estrus_cluster: 'Estrus', diestrus_cluster: 'Diestrus'}\n",
    "if proestrus_cluster is not None:\n",
    "    labels[proestrus_cluster] = 'Proestrus'\n",
    "if metestrus_cluster is not None:\n",
    "    labels[metestrus_cluster] = 'Metestrus'\n",
    "\n",
    "# Apply labels\n",
    "df_analysis['estrous_phase'] = df_analysis['cluster'].map(labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Classification:\")\n",
    "for c, label in sorted(labels.items()):\n",
    "    n = counts[c]\n",
    "    pct = 100 * n / len(df_analysis)\n",
    "    print(f\"  Cluster {c} → {label:12} ({n:3d} nights, {pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.9: Visualize Results\n",
    "\n",
    "### What we're doing:\n",
    "Plotting the clusters in PCA space to see how well-separated they are.\n",
    "\n",
    "### What to look for:\n",
    "- Clear separation between colors = distinct behavioral phases\n",
    "- Overlap = phases are similar behaviorally (harder to distinguish)\n",
    "- Gradients in activity/sleep = continuous variation rather than discrete phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in PCA space\n",
    "colors = {'Proestrus': '#9b59b6', 'Estrus': '#e74c3c', 'Metestrus': '#3498db', 'Diestrus': '#2ecc71'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: PCA colored by cluster\n",
    "ax = axes[0]\n",
    "for phase in ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']:\n",
    "    mask = df_analysis['estrous_phase'] == phase\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(df_analysis.loc[mask, 'PC1'], df_analysis.loc[mask, 'PC2'],\n",
    "                   c=colors[phase], label=phase, alpha=0.6, s=50, edgecolors='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "ax.set_title('PCA - Estrous Phase Classification (k=4)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: PCA colored by physical activity\n",
    "ax = axes[1]\n",
    "sc = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                c=df_analysis['physically_demanding'], cmap='Reds', alpha=0.6, s=50)\n",
    "plt.colorbar(sc, ax=ax, label='Physically Demanding')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA - Colored by Physical Activity\\n(Should correlate with Estrus)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA colored by sleep\n",
    "ax = axes[2]\n",
    "sc = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                c=df_analysis['sleep_related'], cmap='Blues', alpha=0.6, s=50)\n",
    "plt.colorbar(sc, ax=ax, label='Sleep Duration')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA - Colored by Sleep\\n(Should correlate with Diestrus)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.10: Statistical Validation (ANOVA)\n",
    "\n",
    "### What we're doing:\n",
    "Testing whether the 4 clusters differ significantly on key behavioral features.\n",
    "\n",
    "### Method:\n",
    "One-way ANOVA (Analysis of Variance) tests if group means differ significantly.\n",
    "- H0: All group means are equal\n",
    "- H1: At least one group mean differs\n",
    "\n",
    "### What we expect (based on Khatiz):\n",
    "- `physically_demanding`: Highest in Estrus, should be significant\n",
    "- `sleep_related`: Highest in Diestrus, should be significant\n",
    "- `feeding_resourcing`: Highest in Diestrus, should be significant\n",
    "\n",
    "### Interpretation:\n",
    "- *** p < 0.001: Very strong evidence of difference\n",
    "- ** p < 0.01: Strong evidence\n",
    "- * p < 0.05: Moderate evidence\n",
    "- ns: Not significant (p ≥ 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA validation\n",
    "print(\"Statistical Validation: ANOVA\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing if clusters differ significantly on key behavioral features.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "validation = [\n",
    "    ('physically_demanding', 'Highest in Estrus'),\n",
    "    ('activity_amplitude', 'Highest in Estrus'),\n",
    "    ('locomotion_duration', 'Highest in Estrus'),\n",
    "    ('climbing_duration', 'Highest in Estrus'),\n",
    "    ('sleep_related', 'Highest in Diestrus'),\n",
    "    ('feeding_resourcing', 'Highest in Diestrus'),\n",
    "    ('total_distance', 'Highest in Estrus'),\n",
    "]\n",
    "validation = [(f, e) for f, e in validation if f in df_analysis.columns]\n",
    "\n",
    "print(f\"\\n{'Feature':<25} {'F-stat':<10} {'p-value':<12} {'Sig?':<8} {'Expected Pattern'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sig_count = 0\n",
    "for feat, expected in validation:\n",
    "    groups = [df_analysis[df_analysis['cluster'] == c][feat].dropna().values for c in range(K)]\n",
    "    groups = [g for g in groups if len(g) > 0]\n",
    "    \n",
    "    if len(groups) >= 2:\n",
    "        f_stat, p = f_oneway(*groups)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        if p < 0.05:\n",
    "            sig_count += 1\n",
    "        print(f\"{feat:<25} {f_stat:<10.2f} {p:<12.4f} {sig:<8} {expected}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nSignificant features (p < 0.05): {sig_count}/{len(validation)}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  If most features are significant, the clusters represent\")\n",
    "print(\"  behaviorally distinct groups (supporting estrous classification).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots comparing phases\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_features = [\n",
    "    ('physically_demanding', 'Physically Demanding\\n(locomotion + climbing)'),\n",
    "    ('locomotion_duration', 'Locomotion Duration (s)'),\n",
    "    ('climbing_duration', 'Climbing Duration (s)'),\n",
    "    ('sleep_related', 'Sleep Duration (s)'),\n",
    "    ('feeding_resourcing', 'Feeding + Drinking (s)'),\n",
    "    ('total_distance', 'Total Distance')\n",
    "]\n",
    "plot_features = [(f, l) for f, l in plot_features if f in df_analysis.columns]\n",
    "\n",
    "phase_order = ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']\n",
    "phase_order = [p for p in phase_order if p in df_analysis['estrous_phase'].values]\n",
    "\n",
    "for idx, (feat, label) in enumerate(plot_features):\n",
    "    ax = axes[idx]\n",
    "    data = [df_analysis[df_analysis['estrous_phase'] == p][feat].dropna() for p in phase_order]\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=phase_order, patch_artist=True)\n",
    "    for i, box in enumerate(bp['boxes']):\n",
    "        box.set_facecolor(colors.get(phase_order[i], 'gray'))\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(label)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Remove unused subplots\n",
    "for idx in range(len(plot_features), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.suptitle('Behavioral Features by Classified Estrous Phase\\n(Based on K-Means Clustering, k=4)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Summary and Export\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "phase_counts = df_analysis['estrous_phase'].value_counts()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B SUMMARY: Estrous Detection Using Khatiz Methodology\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET:\n",
    "--------\n",
    "• Study: JAX Envision Morph2REP (Study 1001, v2025v3.3)\n",
    "• Animals: {df_analysis['animal_id'].nunique()} female C57BL/6J mice (vehicle controls only)\n",
    "• Nights analyzed: {len(df_analysis)} (dark cycle, post-acclimation)\n",
    "• Features: {len(FEATURES)}\n",
    "• Time period: Dark cycle only (6 PM - 6 AM EST)\n",
    "\n",
    "METHODOLOGY (Following Khatiz et al. 2025):\n",
    "-------------------------------------------\n",
    "1. Hierarchical clustering of FEATURES (Spearman correlation)\n",
    "2. Factor Analysis with Varimax rotation ({n_factors} factors)\n",
    "3. PCA for dimensionality reduction\n",
    "4. K-Means clustering of NIGHTS with k=4 (4 estrous phases)\n",
    "5. Cluster labeling based on behavioral signatures:\n",
    "   - Highest physically_demanding → Estrus\n",
    "   - Highest sleep → Diestrus\n",
    "   - Higher activity of remaining → Proestrus\n",
    "   - Lower activity of remaining → Metestrus\n",
    "6. ANOVA validation\n",
    "\n",
    "CLASSIFICATION RESULTS:\n",
    "-----------------------\"\"\")\n",
    "\n",
    "for phase in ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']:\n",
    "    if phase in phase_counts.index:\n",
    "        n = phase_counts[phase]\n",
    "        pct = 100 * n / len(df_analysis)\n",
    "        print(f\"• {phase:12}: {n:3d} nights ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "EXPECTED DISTRIBUTION (based on cycle biology):\n",
    "-----------------------------------------------\n",
    "• Proestrus:  10-15%\n",
    "• Estrus:     10-15%\n",
    "• Metestrus:  20-25%\n",
    "• Diestrus:   45-55%\n",
    "\n",
    "VALIDATION:\n",
    "-----------\n",
    "• Optimal k by silhouette: {optimal_k}\n",
    "• Used k=4 for biological interpretability\n",
    "• Significant ANOVA features: {sig_count}/{len(validation)}\n",
    "\n",
    "KEY CAVEAT:\n",
    "-----------\n",
    "Without vaginal cytology ground truth, these classifications represent\n",
    "'estrous-like' behavioral patterns based on Khatiz et al. findings.\n",
    "They are behavioral classifications, not confirmed hormonal phases.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = 'phase3b_estrous_khatiz_k4.csv'\n",
    "df_analysis.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print(f\"\\nColumns in output:\")\n",
    "for col in df_analysis.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
