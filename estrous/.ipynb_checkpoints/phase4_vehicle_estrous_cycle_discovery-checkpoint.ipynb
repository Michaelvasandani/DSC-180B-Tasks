{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 4: Estrous Cycle Discovery (Vehicle Cages Only)\n",
        "## Cycle-aware inference from Envision video-derived behavioral bouts (Morph2REP Study 1001 v2025v3.3)\n",
        "\n",
        "### High-level goal\n",
        "We **do not have ground-truth estrous stage labels** (no vaginal cytology).  \n",
        "Instead, we want to discover whether the dataset contains a **within-mouse ~4–5 day cyclic signal** consistent with an estrous cycle, using **video-derived behavioral bouts** and **cycle-aware modeling**.\n",
        "\n",
        "### Core idea\n",
        "1. Build a daily behavioral feature table aligned to the facility light cycle (6am–6pm EST; data timestamps are UTC).\n",
        "2. Normalize **within mouse** (both simple z-score and a leakage-safe rolling baseline).\n",
        "3. Reduce dimensionality (PCA).\n",
        "4. Fit a **4-state time-aware model** (HMM) to infer latent states per day.\n",
        "5. Validate that inferred states show:\n",
        "   - **within-mouse periodicity** near 4–5 days,\n",
        "   - **reasonable dwell times**, and\n",
        "   - minimal confounding by **cage/date artifacts**.\n",
        "\n",
        "### Data used in this notebook\n",
        "We follow your previous notebook's loading approach (DuckDB `read_parquet` directly from S3) and start with the same two tables:\n",
        "- `animal_bouts.parquet`: start/end times and bout durations for behavior states\n",
        "- `animal_bout_metrics.parquet`: additional bout-level metrics (when available)\n",
        "\n",
        "This notebook is scoped to **vehicle cages only** (controls), because drug perturbations can dominate variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup: packages and imports\n",
        "We install and import the Python packages used for:\n",
        "- querying parquet on S3 (DuckDB + PyArrow)\n",
        "- feature engineering (Pandas/Numpy)\n",
        "- modeling (scikit-learn + optional `hmmlearn`)\n",
        "- statistics (SciPy)\n",
        "- plotting (Matplotlib)\n",
        "\n",
        "> If `hmmlearn` fails to install in your environment, you can still run everything up through PCA and periodicity checks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install required packages (quiet)\n",
        "!pip -q install duckdb pyarrow pandas numpy scikit-learn scipy matplotlib hmmlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import duckdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import date, datetime, timedelta\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from scipy.signal import lombscargle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Configuration: dataset paths, vehicle cages, and analysis windows\n",
        "We keep the same conventions as your last notebook:\n",
        "\n",
        "- S3 base path: `s3://jax-envision-public-data/study_1001/2025v3.3/tabular`\n",
        "- Vehicle cages:\n",
        "  - Rep 1: 4918, 4922, 4923\n",
        "  - Rep 2: 4928, 4929, 4934\n",
        "- Start after acclimation (3 days)\n",
        "- Exclude the **cage change day** (environmental perturbation)\n",
        "\n",
        "### Time alignment\n",
        "Facility schedule is **EST**:\n",
        "- Lights ON 6:00 AM EST\n",
        "- Lights OFF 6:00 PM EST\n",
        "\n",
        "Data timestamps are stored in **UTC**. EST = UTC − 5 hours, so:\n",
        "- 6:00 AM EST = 11:00 UTC\n",
        "- 6:00 PM EST = 23:00 UTC\n",
        "\n",
        "We define a **circadian day** as 11:00 UTC → next day 11:00 UTC (i.e., 6am→6am EST)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular\"\n",
        "\n",
        "VEHICLE_CAGES = {\n",
        "    \"Rep1\": {\n",
        "        \"cages\": [4918, 4922, 4923],\n",
        "        \"analysis_start\": \"2025-01-10\",  # after acclimation (Jan 7–9)\n",
        "        \"analysis_end\": \"2025-01-22\",\n",
        "        \"cage_change\": date(2025, 1, 15),\n",
        "    },\n",
        "    \"Rep2\": {\n",
        "        \"cages\": [4928, 4929, 4934],\n",
        "        \"analysis_start\": \"2025-01-25\",  # after acclimation (Jan 22–24)\n",
        "        \"analysis_end\": \"2025-02-04\",\n",
        "        \"cage_change\": date(2025, 1, 29),\n",
        "    },\n",
        "}\n",
        "\n",
        "# Light cycle boundaries in UTC\n",
        "LIGHTS_ON_UTC = 11   # 6:00 AM EST\n",
        "LIGHTS_OFF_UTC = 23  # 6:00 PM EST\n",
        "\n",
        "# Circadian day anchor: 6am EST (11:00 UTC)\n",
        "CIRCADIAN_DAY_START_HOUR_UTC = LIGHTS_ON_UTC\n",
        "\n",
        "print(\"Vehicle cages and analysis windows\")\n",
        "print(\"=\"*70)\n",
        "for rep, cfg in VEHICLE_CAGES.items():\n",
        "    print(f\"{rep}: cages={cfg['cages']}, window={cfg['analysis_start']}..{cfg['analysis_end']}, exclude={cfg['cage_change']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data loading utilities (DuckDB → Pandas)\n",
        "We read parquet partitions from S3 with DuckDB `read_parquet`, matching your previous notebook.\n",
        "We load:\n",
        "- `animal_bouts.parquet`\n",
        "- `animal_bout_metrics.parquet`\n",
        "\n",
        "We attach `cage_id` and `date` columns to support day-based filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def date_range(start: str, end: str) -> List[date]:\n",
        "    start_d = datetime.strptime(start, \"%Y-%m-%d\").date()\n",
        "    end_d = datetime.strptime(end, \"%Y-%m-%d\").date()\n",
        "    out = []\n",
        "    d = start_d\n",
        "    while d <= end_d:\n",
        "        out.append(d)\n",
        "        d += timedelta(days=1)\n",
        "    return out\n",
        "\n",
        "def load_table_for_cages_dates(table_name: str, cages: List[int], dates: List[date]) -> pd.DataFrame:\n",
        "    conn = duckdb.connect(database=\":memory:\")\n",
        "    all_data = []\n",
        "\n",
        "    for cage_id in cages:\n",
        "        for d in dates:\n",
        "            date_str = d.strftime(\"%Y-%m-%d\")\n",
        "            path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/{table_name}\"\n",
        "            try:\n",
        "                df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
        "                df[\"cage_id\"] = cage_id\n",
        "                df[\"date\"] = date_str\n",
        "                all_data.append(df)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    conn.close()\n",
        "    if not all_data:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(all_data, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load vehicle-only bouts data\n",
        "We load bouts and bout metrics across the analysis windows, excluding cage-change days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "TABLE_BOUTS = \"animal_bouts.parquet\"\n",
        "TABLE_BOUT_METRICS = \"animal_bout_metrics.parquet\"\n",
        "\n",
        "def load_vehicle_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    all_bouts = []\n",
        "    all_metrics = []\n",
        "\n",
        "    for rep, cfg in VEHICLE_CAGES.items():\n",
        "        cages = cfg[\"cages\"]\n",
        "        days = date_range(cfg[\"analysis_start\"], cfg[\"analysis_end\"])\n",
        "        days = [d for d in days if d != cfg[\"cage_change\"]]  # exclude cage change\n",
        "\n",
        "        df_b = load_table_for_cages_dates(TABLE_BOUTS, cages, days)\n",
        "        df_m = load_table_for_cages_dates(TABLE_BOUT_METRICS, cages, days)\n",
        "\n",
        "        if not df_b.empty:\n",
        "            df_b[\"replicate\"] = rep\n",
        "            all_bouts.append(df_b)\n",
        "        if not df_m.empty:\n",
        "            df_m[\"replicate\"] = rep\n",
        "            all_metrics.append(df_m)\n",
        "\n",
        "    bouts = pd.concat(all_bouts, ignore_index=True) if all_bouts else pd.DataFrame()\n",
        "    metrics = pd.concat(all_metrics, ignore_index=True) if all_metrics else pd.DataFrame()\n",
        "    return bouts, metrics\n",
        "\n",
        "df_bouts, df_metrics = load_vehicle_data()\n",
        "\n",
        "print(\"Loaded rows:\")\n",
        "print(f\"  animal_bouts:        {len(df_bouts):,}\")\n",
        "print(f\"  animal_bout_metrics: {len(df_metrics):,}\")\n",
        "\n",
        "df_bouts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Feature engineering: daily (6am→6am) features for light and dark\n",
        "We build daily features aligned to a **circadian day** starting at 6am EST (11:00 UTC).\n",
        "We compute features separately for light vs dark, then combine them into one row per mouse-day:\n",
        "- per-state total duration (seconds)\n",
        "- per-state bout count\n",
        "- per-state mean bout length\n",
        "- transition entropy (sequence fragmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def to_utc(series: pd.Series) -> pd.Series:\n",
        "    return pd.to_datetime(series, utc=True, errors=\"coerce\")\n",
        "\n",
        "def assign_circadian_day(ts_utc: pd.Series, day_start_hour_utc: int = 11) -> pd.Series:\n",
        "    ts = pd.to_datetime(ts_utc, utc=True)\n",
        "    shifted = ts - pd.to_timedelta(day_start_hour_utc, unit=\"h\")\n",
        "    return shifted.dt.date\n",
        "\n",
        "def is_light(ts_utc: pd.Series, lights_on_utc: int = 11, lights_off_utc: int = 23) -> pd.Series:\n",
        "    hrs = pd.to_datetime(ts_utc, utc=True).dt.hour\n",
        "    return (hrs >= lights_on_utc) & (hrs < lights_off_utc)\n",
        "\n",
        "def compute_daily_bout_features(df_bouts: pd.DataFrame,\n",
        "                               day_start_hour_utc: int = 11,\n",
        "                               lights_on_utc: int = 11,\n",
        "                               lights_off_utc: int = 23) -> pd.DataFrame:\n",
        "    df = df_bouts.copy()\n",
        "    df[\"start_time\"] = to_utc(df[\"start_time\"])\n",
        "    df[\"end_time\"] = to_utc(df[\"end_time\"])\n",
        "    df = df.dropna(subset=[\"start_time\", \"end_time\", \"animal_id\", \"state_name\", \"bout_length_seconds\"])\n",
        "\n",
        "    df[\"circadian_day\"] = assign_circadian_day(df[\"start_time\"], day_start_hour_utc=day_start_hour_utc)\n",
        "    df[\"period\"] = np.where(is_light(df[\"start_time\"], lights_on_utc, lights_off_utc), \"light\", \"dark\")\n",
        "    df[\"state_short\"] = df[\"state_name\"].astype(str).str.split(\".\").str[-1]\n",
        "\n",
        "    agg = df.groupby([\"animal_id\", \"circadian_day\", \"period\", \"state_short\"]).agg(\n",
        "        total_duration_s=(\"bout_length_seconds\", \"sum\"),\n",
        "        bout_count=(\"bout_length_seconds\", \"size\"),\n",
        "        mean_bout_s=(\"bout_length_seconds\", \"mean\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    def pivot(period: str, value_col: str) -> pd.DataFrame:\n",
        "        sub = agg[agg[\"period\"] == period]\n",
        "        wide = sub.pivot_table(index=[\"animal_id\", \"circadian_day\"], columns=\"state_short\",\n",
        "                               values=value_col, aggfunc=\"first\")\n",
        "        wide.columns = [f\"{c}_{value_col}_{period}\" for c in wide.columns]\n",
        "        return wide.reset_index()\n",
        "\n",
        "    dur_l = pivot(\"light\", \"total_duration_s\")\n",
        "    dur_d = pivot(\"dark\", \"total_duration_s\")\n",
        "    cnt_l = pivot(\"light\", \"bout_count\")\n",
        "    cnt_d = pivot(\"dark\", \"bout_count\")\n",
        "    mb_l  = pivot(\"light\", \"mean_bout_s\")\n",
        "    mb_d  = pivot(\"dark\", \"mean_bout_s\")\n",
        "\n",
        "    # transition entropy per day-period\n",
        "    from collections import Counter\n",
        "\n",
        "    def transition_entropy(g: pd.DataFrame) -> float:\n",
        "        g = g.sort_values(\"start_time\")\n",
        "        s = g[\"state_short\"].to_numpy()\n",
        "        if len(s) < 2:\n",
        "            return 0.0\n",
        "        pairs = list(zip(s[:-1], s[1:]))\n",
        "        c = Counter(pairs)\n",
        "        p = np.array(list(c.values()), dtype=float)\n",
        "        p = p / p.sum()\n",
        "        return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "    ent = df.groupby([\"animal_id\", \"circadian_day\", \"period\"]).apply(transition_entropy).reset_index()\n",
        "    ent = ent.rename(columns={0: \"transition_entropy\"})\n",
        "\n",
        "    ent_l = ent[ent[\"period\"]==\"light\"][[\"animal_id\",\"circadian_day\",\"transition_entropy\"]].rename(\n",
        "        columns={\"transition_entropy\":\"transition_entropy_light\"}\n",
        "    )\n",
        "    ent_d = ent[ent[\"period\"]==\"dark\"][[\"animal_id\",\"circadian_day\",\"transition_entropy\"]].rename(\n",
        "        columns={\"transition_entropy\":\"transition_entropy_dark\"}\n",
        "    )\n",
        "\n",
        "    out = dur_l.merge(dur_d, on=[\"animal_id\",\"circadian_day\"], how=\"outer\")\n",
        "    for piece in [cnt_l, cnt_d, mb_l, mb_d, ent_l, ent_d]:\n",
        "        out = out.merge(piece, on=[\"animal_id\",\"circadian_day\"], how=\"outer\")\n",
        "\n",
        "    return out\n",
        "\n",
        "daily_features = compute_daily_bout_features(\n",
        "    df_bouts,\n",
        "    day_start_hour_utc=CIRCADIAN_DAY_START_HOUR_UTC,\n",
        "    lights_on_utc=LIGHTS_ON_UTC,\n",
        "    lights_off_utc=LIGHTS_OFF_UTC,\n",
        ")\n",
        "\n",
        "print(\"Daily features shape:\", daily_features.shape)\n",
        "daily_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Within-mouse normalization (simple z-score + leakage-safe rolling z-score)\n",
        "Estrous is a within-mouse cycle, so we compute:\n",
        "1) **raw daily metrics**\n",
        "2) **within-mouse z-scores** (feature-wise across days)\n",
        "3) **rolling z-scores** using only past days (window=5; min 3 prior days)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "key_cols = [\"animal_id\", \"circadian_day\"]\n",
        "feature_cols = [c for c in daily_features.columns if c not in key_cols]\n",
        "\n",
        "df_daily = daily_features.copy()\n",
        "df_daily[\"circadian_day\"] = pd.to_datetime(df_daily[\"circadian_day\"])\n",
        "df_daily = df_daily.sort_values([\"animal_id\", \"circadian_day\"])\n",
        "\n",
        "def within_mouse_z(df: pd.DataFrame, feats: List[str]) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for f in feats:\n",
        "        out[f] = out.groupby(\"animal_id\")[f].transform(lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-8))\n",
        "    return out\n",
        "\n",
        "def rolling_z_past(df: pd.DataFrame, feats: List[str], window: int = 5, min_periods: int = 3) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for f in feats:\n",
        "        g = out.groupby(\"animal_id\")[f]\n",
        "        mu = g.shift(1).rolling(window=window, min_periods=min_periods).mean()\n",
        "        sd = g.shift(1).rolling(window=window, min_periods=min_periods).std(ddof=0)\n",
        "        out[f] = (out[f] - mu) / (sd + 1e-8)\n",
        "    return out\n",
        "\n",
        "df_within = within_mouse_z(df_daily, feature_cols)\n",
        "df_roll   = rolling_z_past(df_daily, feature_cols, window=5, min_periods=3)\n",
        "\n",
        "display(df_daily.head(3))\n",
        "display(df_within.head(3))\n",
        "display(df_roll.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Build the modeling matrix\n",
        "We start with **within-mouse z-scores** as the default input to modeling.\n",
        "We also:\n",
        "- impute missing values (median)\n",
        "- standardize across features\n",
        "- reduce via PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "MODEL_DF = df_within.copy()   # swap to df_roll for rolling baseline\n",
        "\n",
        "X = MODEL_DF[feature_cols].copy()\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_imp = imputer.fit_transform(X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imp)\n",
        "\n",
        "pca = PCA(n_components=min(10, X_scaled.shape[1]))\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "explained = np.cumsum(pca.explained_variance_ratio_)\n",
        "k = int(np.searchsorted(explained, 0.85) + 1)\n",
        "k = max(2, min(k, X_pca.shape[1]))\n",
        "X_pca_k = X_pca[:, :k]\n",
        "\n",
        "print(\"X_scaled:\", X_scaled.shape, \"X_pca_k:\", X_pca_k.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1, len(explained)+1), explained, marker=\"o\")\n",
        "plt.xlabel(\"Number of PCs\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.title(\"PCA explained variance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Fit a 4-state HMM (time-aware) and decode states\n",
        "This replaces K-means. We expect estrous-like states to be persistent and to show cyclic recurrence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "states = None\n",
        "post = None\n",
        "\n",
        "try:\n",
        "    from hmmlearn.hmm import GaussianHMM\n",
        "    hmm_ok = True\n",
        "except Exception as e:\n",
        "    hmm_ok = False\n",
        "    print(\"Could not import hmmlearn:\", e)\n",
        "\n",
        "if hmm_ok:\n",
        "    n_states = 4\n",
        "    model = GaussianHMM(\n",
        "        n_components=n_states,\n",
        "        covariance_type=\"full\",\n",
        "        n_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # initialize with sticky transitions\n",
        "    transmat = np.full((n_states, n_states), 0.05 / (n_states - 1))\n",
        "    np.fill_diagonal(transmat, 0.95)\n",
        "    model.transmat_ = transmat\n",
        "    model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
        "\n",
        "    model.fit(X_pca_k)\n",
        "    states = model.predict(X_pca_k)\n",
        "    post = model.predict_proba(X_pca_k)\n",
        "\n",
        "    print(\"Learned transition matrix:\")\n",
        "    display(pd.DataFrame(model.transmat_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Results table (mouse-day)\n",
        "We attach inferred states back to the daily table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = MODEL_DF[[\"animal_id\", \"circadian_day\"]].copy()\n",
        "results[\"circadian_day\"] = pd.to_datetime(results[\"circadian_day\"]).dt.date\n",
        "\n",
        "if states is not None:\n",
        "    results[\"state\"] = states\n",
        "    results[\"state_conf\"] = post.max(axis=1)\n",
        "else:\n",
        "    results[\"state\"] = np.nan\n",
        "    results[\"state_conf\"] = np.nan\n",
        "\n",
        "# Attach PC1 for rhythm testing\n",
        "results[\"PC1\"] = X_pca_k[:, 0]\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Visualize per-mouse state trajectories\n",
        "Heatmap: rows = mice, columns = days, values = inferred state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pivot = results.pivot_table(index=\"animal_id\", columns=\"circadian_day\", values=\"state\", aggfunc=\"first\").sort_index()\n",
        "\n",
        "plt.figure(figsize=(12, max(4, 0.15 * len(pivot))))\n",
        "plt.imshow(pivot.values, aspect=\"auto\", interpolation=\"nearest\")\n",
        "plt.yticks(np.arange(len(pivot.index)), pivot.index)\n",
        "plt.xticks(np.arange(len(pivot.columns)), [d.strftime(\"%m-%d\") for d in pivot.columns], rotation=90)\n",
        "plt.colorbar(label=\"Inferred state\")\n",
        "plt.title(\"Inferred 4-state sequence per mouse (vehicle cages)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 11. Periodicity test (per mouse) using Lomb–Scargle on PC1\n",
        "We look for strongest period in the 2.5–8 day range and summarize across mice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def lomb_scargle_best_period(t_days, y, min_period=2.5, max_period=8.0, n_freq=500):\n",
        "    t = np.asarray(t_days, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    m = np.isfinite(t) & np.isfinite(y)\n",
        "    t, y = t[m], y[m]\n",
        "    if len(t) < 6:\n",
        "        return np.nan\n",
        "\n",
        "    periods = np.linspace(min_period, max_period, n_freq)\n",
        "    freqs = 2 * np.pi / periods\n",
        "    y0 = y - y.mean()\n",
        "    pgram = lombscargle(t, y0, freqs, normalize=True)\n",
        "    return float(periods[int(np.argmax(pgram))])\n",
        "\n",
        "peaks = []\n",
        "for aid, g in results.groupby(\"animal_id\"):\n",
        "    g = g.sort_values(\"circadian_day\")\n",
        "    t0 = pd.to_datetime(g[\"circadian_day\"]).min()\n",
        "    t_days = (pd.to_datetime(g[\"circadian_day\"]) - t0).dt.days.values\n",
        "    best = lomb_scargle_best_period(t_days, g[\"PC1\"].values)\n",
        "    peaks.append((aid, best))\n",
        "\n",
        "peak_df = pd.DataFrame(peaks, columns=[\"animal_id\", \"best_period_days\"]).dropna()\n",
        "peak_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 12. Dwell times and transitions (sanity check)\n",
        "Estrous-like states should not jump randomly day-to-day.\n",
        "We compute:\n",
        "- empirical transition matrix from decoded states\n",
        "- dwell time distributions (run lengths)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "res_sorted = results.sort_values([\"animal_id\", \"circadian_day\"])\n",
        "trans_counts = np.zeros((4,4), dtype=int)\n",
        "dwell = {s: [] for s in range(4)}\n",
        "\n",
        "for aid, g in res_sorted.groupby(\"animal_id\"):\n",
        "    seq = g[\"state\"].dropna().astype(int).values\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "    for a,b in zip(seq[:-1], seq[1:]):\n",
        "        trans_counts[a,b] += 1\n",
        "    cur = seq[0]; run = 1\n",
        "    for x in seq[1:]:\n",
        "        if x == cur:\n",
        "            run += 1\n",
        "        else:\n",
        "            dwell[cur].append(run)\n",
        "            cur = x; run = 1\n",
        "    dwell[cur].append(run)\n",
        "\n",
        "trans_prob = trans_counts / np.maximum(trans_counts.sum(axis=1, keepdims=True), 1)\n",
        "display(pd.DataFrame(trans_prob))\n",
        "\n",
        "print(\"Mean dwell length (days) per state:\")\n",
        "for s in range(4):\n",
        "    vals = dwell[s]\n",
        "    if vals:\n",
        "        print(f\"state {s}: mean={np.mean(vals):.2f}, n={len(vals)}\")\n",
        "    else:\n",
        "        print(f\"state {s}: no data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 13. Export results\n",
        "We save a CSV with inferred states and PC1 values for downstream analyses and plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "out_path = \"/mnt/data/phase4_vehicle_cycle_states.csv\"\n",
        "results.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path)\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 14. Next extensions (optional but recommended)\n",
        "To strengthen estrous inference without labels:\n",
        "1. Add modalities:\n",
        "   - `animal_drinking` (consumption rhythms)\n",
        "   - `animal_respiration` / `animal_tsdb_mvp`\n",
        "   - `animal_sociability_pairwise`\n",
        "2. Use a null test:\n",
        "   - shuffle days within each mouse; confirm 4–5 day periodic peaks disappear\n",
        "3. Move from sticky HMM → **cyclic-constrained** HMM (P→E→M→D→P) once periodicity is established."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}