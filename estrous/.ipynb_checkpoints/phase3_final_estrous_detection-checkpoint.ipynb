{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Estrous Cycle Detection from Behavioral Data\n",
    "## A Complete Analysis Following Khatiz et al. (2025) Methodology\n",
    "\n",
    "---\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "1. [Background & Goals](#1.-Background-&-Goals)\n",
    "2. [Dataset Overview](#2.-Dataset-Overview)\n",
    "3. [Setup & Data Loading](#3.-Setup-&-Data-Loading)\n",
    "4. [Feature Engineering](#4.-Feature-Engineering)\n",
    "5. [Data Cleaning](#5.-Data-Cleaning)\n",
    "6. [Analysis Pipeline](#6.-Analysis-Pipeline)\n",
    "   - 6.1 Hierarchical Clustering of Features\n",
    "   - 6.2 Factor Analysis\n",
    "   - 6.3 Principal Component Analysis (PCA)\n",
    "   - 6.4 K-Means Clustering\n",
    "7. [K=2 vs K=4 Comparison](#7.-K=2-vs-K=4-Comparison)\n",
    "8. [Final Results & Recommendations](#8.-Final-Results-&-Recommendations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Background & Goals\n",
    "\n",
    "## 1.1 Why Detect Estrous Cycle?\n",
    "\n",
    "Female mice undergo a **4-5 day estrous cycle** that causes significant hormonal fluctuations. These hormonal changes affect:\n",
    "- Pain sensitivity\n",
    "- Drug metabolism\n",
    "- Behavioral patterns\n",
    "- Neural activity\n",
    "\n",
    "**The Problem:** The parent study (Morph2REP) investigates morphine effects in female mice, but didn't collect vaginal cytology (the gold standard for estrous staging). Without knowing the estrous phase, we can't determine if morphine response varies across the cycle.\n",
    "\n",
    "**Our Solution:** Use behavioral data to *infer* estrous phase, following the methodology of Khatiz et al. (2025), who demonstrated that estrous phases have distinct behavioral signatures.\n",
    "\n",
    "## 1.2 The Four Estrous Phases\n",
    "\n",
    "| Phase | Duration | Hormones | Behavioral Signature |\n",
    "|-------|----------|----------|---------------------|\n",
    "| **Proestrus** | ~12h | Estrogen rising, LH surge | Moderate-high activity, exploration |\n",
    "| **Estrus** | ~12h | Estrogen peak, ovulation | **Highest** physical activity, locomotion, climbing |\n",
    "| **Metestrus** | ~24h | Progesterone rising | Transition to rest, declining activity |\n",
    "| **Diestrus** | ~48-72h | Low hormones | **Lowest** activity, highest sleep & feeding |\n",
    "\n",
    "**Expected Distribution (based on phase durations):**\n",
    "- Proestrus: 10-15%\n",
    "- Estrus: 10-15%\n",
    "- Metestrus: 20-25%\n",
    "- Diestrus: 45-55%\n",
    "\n",
    "**Simplified Binary View:**\n",
    "- High Estrogen (Proestrus + Estrus): 20-30%\n",
    "- Low Estrogen (Metestrus + Diestrus): 70-80%\n",
    "\n",
    "## 1.3 Khatiz et al. (2025) Key Findings\n",
    "\n",
    "Khatiz validated behavioral estrous detection using vaginal cytology ground truth:\n",
    "\n",
    "1. **Estrus females showed 30% more physically demanding activity** than males\n",
    "2. **Diestrus females had highest sleep-related behaviors**\n",
    "3. **Five behavioral clusters** emerged from factor analysis:\n",
    "   - Cluster 1: Feeding & Resource Interaction\n",
    "   - Cluster 2: Exploratory behaviors\n",
    "   - Cluster 3: Sleep-Related\n",
    "   - Cluster 4: Physically Demanding (locomotion, climbing, jumping)\n",
    "   - Cluster 5: Habituation-Like\n",
    "\n",
    "**Reference:** Khatiz et al. (2025) Frontiers in Neuroscience 19:1509822\n",
    "\n",
    "## 1.4 Our Approach\n",
    "\n",
    "We will:\n",
    "1. Extract behavioral features from automated monitoring data\n",
    "2. Apply Khatiz's statistical pipeline (hierarchical clustering, factor analysis, PCA, K-means)\n",
    "3. Compare K=2 (binary: high vs low activity) vs K=4 (4 estrous phases)\n",
    "4. Validate clusters against expected biological distributions\n",
    "\n",
    "**Key Caveat:** Without vaginal cytology, we're creating *behavioral classifications* that we *hypothesize* correspond to estrous phases. We cannot confirm true hormonal status.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Overview\n",
    "\n",
    "## 2.1 Study Information\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Study ID | 1001 (Morph2REP) |\n",
    "| Data Version | 2025v3.3 |\n",
    "| Platform | JAX Envision 2025 v3.0 (automated behavioral monitoring) |\n",
    "| Location | JAX Envision East facility |\n",
    "| Animals | 54 female C57BL/6J mice (strain 664) |\n",
    "| Housing | 3 mice per cage, 18 cages total |\n",
    "| Study Period | January 7 - February 4, 2025 |\n",
    "| Light Cycle | 12h:12h, lights ON 6:00 AM - 6:00 PM EST |\n",
    "\n",
    "## 2.2 Treatment Groups\n",
    "\n",
    "| Group | Rep 1 Cages | Rep 2 Cages | N Mice | Use for Estrous? |\n",
    "|-------|-------------|-------------|--------|------------------|\n",
    "| **Vehicle Control** | 4918, 4922, 4923 | 4928, 4929, 4934 | 18 | ✓ YES |\n",
    "| 5 mg/kg Morphine | 4917, 4921, 4925 | 4927, 4931, 4932 | 18 | ✗ No (drug confound) |\n",
    "| 25 mg/kg Morphine | 4919, 4920, 4924 | 4926, 4930, 4933 | 18 | ✗ No (drug confound) |\n",
    "\n",
    "**We only use VEHICLE CONTROLS** for estrous detection to avoid drug effects confounding behavioral patterns.\n",
    "\n",
    "## 2.3 Experimental Timeline\n",
    "\n",
    "```\n",
    "REPLICATE 1 (January 7-22, 2025)\n",
    "══════════════════════════════════════════════════════════════════════════\n",
    "Jan 7-9      │ ACCLIMATION (3 days)          │ ⚠️ EXCLUDE - abnormal behavior\n",
    "Jan 10-13    │ BASELINE (4 days)             │ ✓ Clean behavioral data\n",
    "Jan 14       │ Morphine Dose #1 (6:00 AM)    │ ✓ Vehicle = no drug\n",
    "Jan 15       │ CAGE CHANGE (12:00 PM)        │ ⚠️ EXCLUDE - environmental stress\n",
    "Jan 17       │ Morphine Dose #2 (5:00 PM)    │ ✓ Vehicle = no drug\n",
    "Jan 18-22    │ Post-treatment monitoring     │ ✓ Clean behavioral data\n",
    "\n",
    "REPLICATE 2 (January 22 - February 4, 2025)\n",
    "══════════════════════════════════════════════════════════════════════════\n",
    "Jan 22-24    │ ACCLIMATION (3 days)          │ ⚠️ EXCLUDE - abnormal behavior\n",
    "Jan 25-27    │ BASELINE (3 days)             │ ✓ Clean behavioral data\n",
    "Jan 28       │ Morphine Dose #1 (5:00 PM)    │ ✓ Vehicle = no drug\n",
    "Jan 29       │ CAGE CHANGE (12:00 PM)        │ ⚠️ EXCLUDE - environmental stress\n",
    "Jan 31       │ Morphine Dose #2 (6:00 AM)    │ ✓ Vehicle = no drug\n",
    "Feb 1-2      │ Post-treatment monitoring     │ ✓ Clean behavioral data\n",
    "Feb 3-4      │ Study termination             │ ⚠️ EXCLUDE - truncated data\n",
    "```\n",
    "\n",
    "## 2.4 Behavioral States (Envision System)\n",
    "\n",
    "The Envision system classifies mouse behavior into discrete states:\n",
    "\n",
    "| State | Description | Estrous Relevance |\n",
    "|-------|-------------|-------------------|\n",
    "| `active` | General activity (moving, not sleeping) | Higher in Estrus |\n",
    "| `inactive` | Stationary but awake | - |\n",
    "| `locomotion` | Walking/running | Higher in Estrus |\n",
    "| `climbing` | Vertical exploration (rearing, hanging) | Higher in Estrus |\n",
    "| `feeding` | Eating at food hopper | Higher in Diestrus |\n",
    "| `drinking` | At water bottle | Higher in Diestrus |\n",
    "| `inferred_sleep` | Sleep (posture + extended inactivity) | Higher in Diestrus |\n",
    "\n",
    "## 2.5 Data Tables (S3)\n",
    "\n",
    "```\n",
    "s3://jax-envision-public-data/study_1001/2025v3.3/tabular/\n",
    "└── cage_id={cage}/date={date}/{table}.parquet\n",
    "```\n",
    "\n",
    "| Table | Description | Key Columns |\n",
    "|-------|-------------|-------------|\n",
    "| `animal_bouts.parquet` | Behavioral state bouts | start_time, end_time, state_name, bout_length_seconds |\n",
    "| `animal_bout_metrics.parquet` | Per-bout metrics | state_name, metric_name (distance), metric_value |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup & Data Loading\n",
    "\n",
    "## 3.1 What We're Doing\n",
    "\n",
    "Installing and importing the tools needed for analysis:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|--------|\n",
    "| `duckdb` | SQL engine that queries Parquet files directly from S3 |\n",
    "| `pandas` | Data manipulation and analysis |\n",
    "| `numpy` | Numerical operations |\n",
    "| `scipy` | Statistical tests (ANOVA, Spearman correlation) |\n",
    "| `sklearn` | Machine learning (PCA, Factor Analysis, K-Means) |\n",
    "| `matplotlib/seaborn` | Visualization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install duckdb pyarrow scikit-learn scipy seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tools\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, f_oneway\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Configuration\n",
    "\n",
    "### What We're Doing\n",
    "Defining which cages to analyze and which dates to include.\n",
    "\n",
    "### Why These Choices\n",
    "- **Vehicle controls only**: Drug-treated mice have altered behavior\n",
    "- **Post-acclimation only**: First 3 days show abnormal stress/novelty responses\n",
    "- **Dark cycle focus**: Mice are nocturnal; estrous differences are most visible at night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular\"\n",
    "\n",
    "# Vehicle control cages with their analysis windows\n",
    "# We start AFTER acclimation (3 days) to get clean behavioral data\n",
    "VEHICLE_CAGES = {\n",
    "    'Rep1': {\n",
    "        'cages': [4918, 4922, 4923],\n",
    "        'analysis_start': '2025-01-10',  # After 3-day acclimation (Jan 7-9)\n",
    "        'analysis_end': '2025-01-22',\n",
    "        'cage_change': date(2025, 1, 15),\n",
    "    },\n",
    "    'Rep2': {\n",
    "        'cages': [4928, 4929, 4934],\n",
    "        'analysis_start': '2025-01-25',  # After 3-day acclimation (Jan 22-24)\n",
    "        'analysis_end': '2025-02-04',\n",
    "        'cage_change': date(2025, 1, 29),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Light cycle timing (in UTC - data is stored in UTC)\n",
    "# JAX facility: Lights ON 6:00 AM - 6:00 PM EST\n",
    "# EST = UTC - 5 hours, so:\n",
    "#   6 AM EST = 11:00 UTC\n",
    "#   6 PM EST = 23:00 UTC\n",
    "LIGHT_START_UTC = 11  # Lights ON\n",
    "LIGHT_END_UTC = 23    # Lights OFF\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"\\n{rep}:\")\n",
    "    print(f\"  Cages: {cfg['cages']}\")\n",
    "    print(f\"  Analysis period: {cfg['analysis_start']} to {cfg['analysis_end']}\")\n",
    "    print(f\"  Cage change day: {cfg['cage_change']} (will be excluded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Loading Functions\n",
    "\n",
    "### What We're Doing\n",
    "Creating a function to load Parquet files from S3 for specific cages and date ranges.\n",
    "\n",
    "### How It Works\n",
    "1. Connect to DuckDB with S3 access enabled\n",
    "2. Loop through each date in the range\n",
    "3. Construct the S3 path using Hive-style partitioning: `cage_id={cage}/date={date}/`\n",
    "4. Read Parquet file and concatenate all days\n",
    "\n",
    "### Why DuckDB + Parquet\n",
    "- **Parquet**: Columnar format, efficient for analytics\n",
    "- **DuckDB**: Can query S3 directly without downloading entire files\n",
    "- **Hive partitioning**: Makes it easy to select specific cages/dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_s3(cage_id, start_date, end_date, table_name):\n",
    "    \"\"\"\n",
    "    Load parquet table from S3 for a single cage across a date range.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cage_id : int - The cage number (e.g., 4918)\n",
    "    start_date : str - Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str - End date in 'YYYY-MM-DD' format\n",
    "    table_name : str - Name of parquet file (e.g., 'animal_bouts.parquet')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with all data for that cage/date range\n",
    "    \"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for d in dates:\n",
    "        date_str = d.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/{table_name}\"\n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Load Behavioral Data\n",
    "\n",
    "### What We're Loading\n",
    "\n",
    "**`animal_bouts.parquet`**: Each row is a **bout** - a continuous period of one behavioral state.\n",
    "\n",
    "Example:\n",
    "```\n",
    "| animal_id | state_name              | start_time          | bout_length_seconds |\n",
    "|-----------|-------------------------|---------------------|---------------------|\n",
    "| 9266      | animal_bouts.locomotion | 2025-01-10 23:15:00 | 45.2                |\n",
    "| 9266      | animal_bouts.feeding    | 2025-01-10 23:15:45 | 120.5               |\n",
    "```\n",
    "\n",
    "**`animal_bout_metrics.parquet`**: Distance traveled during each bout.\n",
    "\n",
    "### Why We Need Both\n",
    "- **Bouts**: Time spent in each behavior (Khatiz's primary measure)\n",
    "- **Metrics**: Distance traveled (exploration, a key estrous marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load animal_bouts.parquet\n",
    "print(\"Loading animal_bouts.parquet...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_bouts = []\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"\\n{rep}:\")\n",
    "    for cage_id in cfg['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_parquet_s3(cage_id, cfg['analysis_start'], cfg['analysis_end'], 'animal_bouts.parquet')\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep\n",
    "            all_bouts.append(df)\n",
    "            print(f\"{len(df):,} bouts\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bouts = pd.concat(all_bouts, ignore_index=True)\n",
    "print(f\"\\nTotal bouts: {len(df_bouts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load animal_bout_metrics.parquet (distance traveled)\n",
    "print(\"Loading animal_bout_metrics.parquet...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_metrics = []\n",
    "for rep, cfg in VEHICLE_CAGES.items():\n",
    "    print(f\"\\n{rep}:\")\n",
    "    for cage_id in cfg['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_parquet_s3(cage_id, cfg['analysis_start'], cfg['analysis_end'], 'animal_bout_metrics.parquet')\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep\n",
    "            all_metrics.append(df)\n",
    "            print(f\"{len(df):,} rows\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bout_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "print(f\"\\nTotal metrics: {len(df_bout_metrics):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect available behavioral states\n",
    "print(\"Behavioral states in data:\")\n",
    "print(\"=\"*50)\n",
    "for state, count in df_bouts['state_name'].value_counts().items():\n",
    "    print(f\"  {state}: {count:,} bouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Feature Engineering\n",
    "\n",
    "## 4.1 Concept: Why Compute Features?\n",
    "\n",
    "Raw bout data has thousands of rows per animal per day. We need to **summarize** this into meaningful features:\n",
    "\n",
    "| Feature Type | What It Measures | Example |\n",
    "|--------------|------------------|--------|\n",
    "| **Duration** | Total time in a state | \"Mouse spent 3,600s locomoting\" |\n",
    "| **Bout Count** | Number of bouts | \"Mouse had 45 locomotion bouts\" |\n",
    "| **Mean Bout Length** | Average bout duration | \"Average locomotion bout was 80s\" |\n",
    "| **Distance** | Total distance traveled | \"Mouse traveled 50,000 units\" |\n",
    "| **Derived** | Combinations | \"Physically demanding = locomotion + climbing\" |\n",
    "\n",
    "## 4.2 Time Periods\n",
    "\n",
    "Khatiz analyzed behavior separately for:\n",
    "- **Dark cycle** (6 PM - 6 AM): When mice are active; estrous differences most visible\n",
    "- **Light cycle** (6 AM - 6 PM): Rest period\n",
    "- **24-hour**: Full day summary\n",
    "\n",
    "**We focus on DARK CYCLE** because mice are nocturnal and Khatiz found the strongest estrous signals during active periods.\n",
    "\n",
    "## 4.3 Night Date Assignment\n",
    "\n",
    "**Important nuance:** A \"night\" spans two calendar dates:\n",
    "- Night of Jan 10 = 6 PM Jan 10 → 6 AM Jan 11\n",
    "\n",
    "For bouts occurring after midnight (e.g., 2 AM Jan 11), we assign them to the \"night of Jan 10\" since they're part of the same dark cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dark_cycle_features(df_bouts, df_metrics):\n",
    "    \"\"\"\n",
    "    Compute behavioral features for dark cycle (6 PM - 6 AM EST).\n",
    "    \n",
    "    This function:\n",
    "    1. Filters to dark cycle bouts only\n",
    "    2. Assigns each bout to a \"night\" (spanning 2 calendar dates)\n",
    "    3. Aggregates by animal and night\n",
    "    4. Computes duration, bout count, mean bout length for each state\n",
    "    5. Merges distance metrics\n",
    "    6. Computes derived features (physically_demanding, sleep_related, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with one row per animal-night, columns for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # State name mapping (short name -> full name in data)\n",
    "    STATE_MAP = {\n",
    "        'active': 'animal_bouts.active',\n",
    "        'inactive': 'animal_bouts.inactive',\n",
    "        'locomotion': 'animal_bouts.locomotion',\n",
    "        'feeding': 'animal_bouts.feeding',\n",
    "        'drinking': 'animal_bouts.drinking',\n",
    "        'climbing': 'animal_bouts.climbing',\n",
    "        'inferred_sleep': 'animal_bouts.inferred_sleep',\n",
    "    }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Parse timestamps and identify dark cycle\n",
    "    # =========================================================================\n",
    "    df_b = df_bouts.copy()\n",
    "    df_b['start_time'] = pd.to_datetime(df_b['start_time'])\n",
    "    df_b['hour_utc'] = df_b['start_time'].dt.hour\n",
    "    \n",
    "    # Dark cycle: NOT between 11:00-23:00 UTC (i.e., before 11 or after 23)\n",
    "    # This is 6PM-6AM EST\n",
    "    df_b['is_dark'] = ~((df_b['hour_utc'] >= LIGHT_START_UTC) & (df_b['hour_utc'] < LIGHT_END_UTC))\n",
    "    \n",
    "    # Filter to dark cycle only\n",
    "    df_b = df_b[df_b['is_dark']].copy()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Assign night date\n",
    "    # Bouts after midnight belong to the previous calendar day's \"night\"\n",
    "    # =========================================================================\n",
    "    df_b['night_date'] = df_b['start_time'].dt.date\n",
    "    # If hour < 11 UTC (before 6 AM EST), it's part of previous night\n",
    "    mask_early = df_b['hour_utc'] < LIGHT_START_UTC\n",
    "    df_b.loc[mask_early, 'night_date'] = (df_b.loc[mask_early, 'start_time'] - timedelta(days=1)).dt.date\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Aggregate bouts by animal and night\n",
    "    # =========================================================================\n",
    "    results = []\n",
    "    for (cage_id, animal_id, night), grp in df_b.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'night_date': night,\n",
    "        }\n",
    "        \n",
    "        # For each behavioral state\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_df = grp[grp['state_name'] == full_name]\n",
    "            row[f'{short_name}_duration'] = state_df['bout_length_seconds'].sum()\n",
    "            row[f'{short_name}_bout_count'] = len(state_df)\n",
    "            row[f'{short_name}_mean_bout'] = state_df['bout_length_seconds'].mean() if len(state_df) > 0 else 0\n",
    "        \n",
    "        row['total_duration'] = grp['bout_length_seconds'].sum()\n",
    "        results.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(results)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Process distance metrics\n",
    "    # =========================================================================\n",
    "    df_m = df_metrics.copy()\n",
    "    df_m['start_time'] = pd.to_datetime(df_m['start_time'])\n",
    "    df_m['hour_utc'] = df_m['start_time'].dt.hour\n",
    "    df_m['is_dark'] = ~((df_m['hour_utc'] >= LIGHT_START_UTC) & (df_m['hour_utc'] < LIGHT_END_UTC))\n",
    "    df_m = df_m[df_m['is_dark']].copy()\n",
    "    \n",
    "    df_m['night_date'] = df_m['start_time'].dt.date\n",
    "    mask_early = df_m['hour_utc'] < LIGHT_START_UTC\n",
    "    df_m.loc[mask_early, 'night_date'] = (df_m.loc[mask_early, 'start_time'] - timedelta(days=1)).dt.date\n",
    "    \n",
    "    dist_results = []\n",
    "    for (cage_id, animal_id, night), grp in df_m.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {'cage_id': cage_id, 'animal_id': animal_id, 'night_date': night}\n",
    "        row['total_distance'] = grp['metric_value'].sum()\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_df = grp[grp['state_name'] == full_name]\n",
    "            row[f'{short_name}_distance'] = state_df['metric_value'].sum()\n",
    "        dist_results.append(row)\n",
    "    \n",
    "    df_dist = pd.DataFrame(dist_results)\n",
    "    if len(df_dist) > 0:\n",
    "        df_summary = df_summary.merge(df_dist, on=['cage_id', 'animal_id', 'night_date'], how='left')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Compute derived features (matching Khatiz's behavioral clusters)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Physically demanding = locomotion + climbing\n",
    "    # Khatiz Cluster 4: High in Estrus\n",
    "    df_summary['physically_demanding'] = df_summary['locomotion_duration'] + df_summary['climbing_duration']\n",
    "    \n",
    "    # Sleep-related\n",
    "    # Khatiz Cluster 3: High in Diestrus\n",
    "    df_summary['sleep_related'] = df_summary['inferred_sleep_duration']\n",
    "    \n",
    "    # Feeding/resourcing = feeding + drinking\n",
    "    # Khatiz Cluster 1: High in Diestrus\n",
    "    df_summary['feeding_resourcing'] = df_summary['feeding_duration'] + df_summary['drinking_duration']\n",
    "    \n",
    "    # Activity amplitude = overall activity level\n",
    "    df_summary['activity_amplitude'] = df_summary['active_duration'] + df_summary['locomotion_duration']\n",
    "    \n",
    "    # Fragmentation = more bouts per unit time = more transitions\n",
    "    # Higher fragmentation may indicate different behavioral organization\n",
    "    df_summary['sleep_fragmentation'] = df_summary['inferred_sleep_bout_count'] / (df_summary['inferred_sleep_duration'] + 1)\n",
    "    df_summary['active_fragmentation'] = df_summary['active_bout_count'] / (df_summary['active_duration'] + 1)\n",
    "    \n",
    "    # Exploration intensity = distance per unit activity time\n",
    "    if 'total_distance' in df_summary.columns:\n",
    "        df_summary['exploration_intensity'] = df_summary['total_distance'] / (df_summary['activity_amplitude'] + 1)\n",
    "    \n",
    "    return df_summary\n",
    "\n",
    "print(\"Feature engineering function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for dark cycle\n",
    "print(\"Computing dark cycle features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_features = compute_dark_cycle_features(df_bouts, df_bout_metrics)\n",
    "\n",
    "# Remove tracking errors (animal_id = 0 means unidentified)\n",
    "df_features = df_features[df_features['animal_id'] != 0].copy()\n",
    "\n",
    "print(f\"\\nResult: {len(df_features)} animal-nights\")\n",
    "print(f\"Animals: {df_features['animal_id'].nunique()}\")\n",
    "print(f\"Cages: {df_features['cage_id'].nunique()}\")\n",
    "print(f\"Date range: {df_features['night_date'].min()} to {df_features['night_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Data Cleaning\n",
    "\n",
    "## 5.1 Why Clean the Data?\n",
    "\n",
    "Raw experimental data contains artifacts that can confuse our analysis:\n",
    "\n",
    "| Issue | Days Affected | Problem | Solution |\n",
    "|-------|---------------|---------|----------|\n",
    "| **Acclimation spillover** | Jan 9, Jan 24 | Early morning hours of first analysis day contain acclimation behavior | Exclude |\n",
    "| **Cage changes** | Jan 15, Jan 29 | Environmental stress causes abnormal behavior | Exclude |\n",
    "| **Study termination** | Feb 3-4 | Truncated data collection | Exclude |\n",
    "\n",
    "## 5.2 Identify Problematic Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness by date\n",
    "print(\"Data completeness check:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Expected dark cycle duration: ~43,200 seconds (12 hours)\")\n",
    "print(\"\")\n",
    "\n",
    "date_totals = df_features.groupby('night_date')['total_duration'].agg(['mean', 'count'])\n",
    "date_totals.columns = ['mean_duration', 'n_animals']\n",
    "date_totals['pct_expected'] = 100 * date_totals['mean_duration'] / 80000  # Approximate expected\n",
    "\n",
    "print(date_totals.to_string())\n",
    "\n",
    "# Flag problematic dates\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROBLEMATIC DATES:\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in date_totals.iterrows():\n",
    "    if row['mean_duration'] < 60000:  # Less than ~75% of expected\n",
    "        print(f\"  {idx}: {row['mean_duration']:.0f}s ({row['pct_expected']:.0f}% of expected) - TRUNCATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Define Exclusions\n",
    "\n",
    "Based on our analysis, we exclude:\n",
    "\n",
    "1. **Cage change days**: Jan 15 (Rep1), Jan 29 (Rep2)\n",
    "   - Mice are stressed from new environment\n",
    "   - Behavior is abnormal for 24-48 hours\n",
    "\n",
    "2. **Truncated end-of-study days**: Feb 3-4\n",
    "   - Data collection ended before full dark cycle\n",
    "   - Severely incomplete data (some only 8% of expected)\n",
    "\n",
    "3. **Acclimation spillover** (if present): Jan 9, Jan 24\n",
    "   - Early morning hours may contain acclimation behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dates to exclude\n",
    "EXCLUDE_DATES = [\n",
    "    # Cage change days\n",
    "    date(2025, 1, 15),  # Rep 1 cage change\n",
    "    date(2025, 1, 29),  # Rep 2 cage change\n",
    "    \n",
    "    # Truncated end-of-study days\n",
    "    date(2025, 2, 3),   # 72% data\n",
    "    date(2025, 2, 4),   # 8% data - severely truncated\n",
    "    \n",
    "    # Acclimation spillover (if present)\n",
    "    date(2025, 1, 9),   # Rep 1 acclimation night\n",
    "    date(2025, 1, 24),  # Rep 2 acclimation night\n",
    "]\n",
    "\n",
    "print(\"Dates to exclude:\")\n",
    "for d in EXCLUDE_DATES:\n",
    "    reason = {\n",
    "        date(2025, 1, 15): \"Rep 1 cage change\",\n",
    "        date(2025, 1, 29): \"Rep 2 cage change\",\n",
    "        date(2025, 2, 3): \"Truncated (72%)\",\n",
    "        date(2025, 2, 4): \"Severely truncated (8%)\",\n",
    "        date(2025, 1, 9): \"Rep 1 acclimation spillover\",\n",
    "        date(2025, 1, 24): \"Rep 2 acclimation spillover\",\n",
    "    }.get(d, \"Unknown\")\n",
    "    print(f\"  {d}: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply exclusions\n",
    "df_clean = df_features[~df_features['night_date'].isin(EXCLUDE_DATES)].copy()\n",
    "\n",
    "print(\"Data cleaning results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Before: {len(df_features)} animal-nights\")\n",
    "print(f\"After:  {len(df_clean)} animal-nights\")\n",
    "print(f\"Removed: {len(df_features) - len(df_clean)} animal-nights\")\n",
    "\n",
    "print(f\"\\nRemaining date range: {df_clean['night_date'].min()} to {df_clean['night_date'].max()}\")\n",
    "print(f\"Remaining animals: {df_clean['animal_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning worked\n",
    "print(\"Verification: Data completeness after cleaning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clean_totals = df_clean.groupby('night_date')['total_duration'].mean()\n",
    "print(f\"Min mean duration: {clean_totals.min():.0f}s\")\n",
    "print(f\"Max mean duration: {clean_totals.max():.0f}s\")\n",
    "print(f\"All days > 60,000s: {(clean_totals > 60000).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Analysis Pipeline\n",
    "\n",
    "Now we apply Khatiz's methodology to classify nights into estrous phases.\n",
    "\n",
    "## Overview of the Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│  1. SELECT FEATURES                                                 │\n",
    "│     Choose behavioral metrics relevant to estrous detection         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  2. STANDARDIZE                                                     │\n",
    "│     Put all features on same scale (z-score)                        │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  3. HIERARCHICAL CLUSTERING OF FEATURES                             │\n",
    "│     See which behaviors co-vary (feature relationships)             │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  4. FACTOR ANALYSIS                                                 │\n",
    "│     Find underlying behavioral dimensions                           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  5. PCA                                                             │\n",
    "│     Reduce dimensionality for visualization                         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  6. K-MEANS CLUSTERING OF NIGHTS                                    │\n",
    "│     Group nights by behavioral similarity                           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  7. LABEL CLUSTERS                                                  │\n",
    "│     Assign estrous phase names based on behavioral signatures       │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  8. VALIDATE                                                        │\n",
    "│     Statistical tests (ANOVA) to confirm clusters differ            │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Feature Selection\n",
    "\n",
    "### What We're Doing\n",
    "Selecting which features to include in clustering.\n",
    "\n",
    "### Selection Criteria\n",
    "- **Biologically relevant**: Features Khatiz found to differ by estrous phase\n",
    "- **Non-redundant**: Avoid including highly correlated features\n",
    "- **Available**: Must exist in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for analysis\n",
    "FEATURES = [\n",
    "    # Duration features (total time in each state)\n",
    "    'active_duration', 'inactive_duration', 'locomotion_duration',\n",
    "    'feeding_duration', 'drinking_duration', 'climbing_duration', 'inferred_sleep_duration',\n",
    "    \n",
    "    # Bout features (how behavior is organized)\n",
    "    'active_bout_count', 'locomotion_bout_count', 'climbing_bout_count', 'inferred_sleep_bout_count',\n",
    "    \n",
    "    # Derived features (matching Khatiz clusters)\n",
    "    'physically_demanding',   # locomotion + climbing - HIGH in Estrus\n",
    "    'sleep_related',          # sleep duration - HIGH in Diestrus\n",
    "    'feeding_resourcing',     # feeding + drinking - HIGH in Diestrus\n",
    "    'activity_amplitude',     # overall activity level\n",
    "    \n",
    "    # Fragmentation features\n",
    "    'sleep_fragmentation',\n",
    "    'active_fragmentation',\n",
    "    \n",
    "    # Distance/exploration features\n",
    "    'total_distance',\n",
    "    'locomotion_distance',\n",
    "    'climbing_distance',\n",
    "    'exploration_intensity',\n",
    "]\n",
    "\n",
    "# Keep only features that exist\n",
    "FEATURES = [f for f in FEATURES if f in df_clean.columns]\n",
    "\n",
    "print(f\"Selected {len(FEATURES)} features:\")\n",
    "print(\"=\"*50)\n",
    "for i, f in enumerate(FEATURES, 1):\n",
    "    print(f\"  {i:2d}. {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Standardization\n",
    "\n",
    "### What We're Doing\n",
    "Converting all features to z-scores: `z = (x - mean) / std`\n",
    "\n",
    "### Why Standardize?\n",
    "\n",
    "Different features have different scales:\n",
    "- `locomotion_duration` might range from 0 to 5,000 seconds\n",
    "- `sleep_fragmentation` might range from 0 to 0.1\n",
    "\n",
    "Without standardization, features with larger values would dominate:\n",
    "- Distance calculations would be driven by `locomotion_duration`\n",
    "- K-means would ignore `sleep_fragmentation`\n",
    "\n",
    "After standardization, each feature has:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "### Intuition\n",
    "Think of it like converting different currencies to a common unit. You can't compare 1,000 yen to 10 dollars without conversion. Standardization converts all features to \"standard units.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: remove rows with missing values and standardize\n",
    "df_analysis = df_clean.dropna(subset=FEATURES).copy()\n",
    "X = df_analysis[FEATURES].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Standardization results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Observations (animal-nights): {X_scaled.shape[0]}\")\n",
    "print(f\"Features: {X_scaled.shape[1]}\")\n",
    "print(f\"\")\n",
    "print(f\"After standardization:\")\n",
    "print(f\"  Mean of each feature: {X_scaled.mean(axis=0).mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std of each feature:  {X_scaled.std(axis=0).mean():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Hierarchical Clustering of FEATURES\n",
    "\n",
    "### What We're Doing\n",
    "Clustering the **features** (not nights!) to see which behaviors co-vary.\n",
    "\n",
    "### Method\n",
    "1. Compute **Spearman correlation** between all pairs of features\n",
    "2. Convert to distance: `distance = 1 - correlation`\n",
    "3. Apply **complete linkage** hierarchical clustering\n",
    "4. Visualize as dendrogram\n",
    "\n",
    "### Why Spearman (not Pearson)?\n",
    "- Spearman uses **ranks**, not raw values\n",
    "- Robust to outliers and non-normal distributions\n",
    "- Behavioral data is often skewed (many zeros, some high values)\n",
    "\n",
    "### How to Read the Dendrogram\n",
    "- **Y-axis**: Distance (1 - correlation)\n",
    "- Features that join at **low height** are highly correlated\n",
    "- Features that join at **high height** are less related\n",
    "- Clusters form where branches split\n",
    "\n",
    "### What to Look For\n",
    "We expect to see clusters matching Khatiz's findings:\n",
    "- Activity features together (locomotion, climbing, active)\n",
    "- Rest features together (sleep, inactive)\n",
    "- Feeding features together (feeding, drinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering of FEATURES\n",
    "# This groups features that co-vary together across nights\n",
    "\n",
    "# Step 1: Compute Spearman correlation matrix\n",
    "corr_matrix = df_analysis[FEATURES].corr(method='spearman')\n",
    "\n",
    "# Step 2: Convert to distance\n",
    "distance_matrix = 1 - corr_matrix\n",
    "\n",
    "# Step 3: Hierarchical clustering\n",
    "linkage_features = linkage(squareform(distance_matrix), method='complete')\n",
    "\n",
    "# Step 4: Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "dendrogram(\n",
    "    linkage_features,\n",
    "    labels=FEATURES,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=9,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Hierarchical Clustering of Behavioral Features\\n'\n",
    "             '(Spearman Correlation, Complete Linkage)\\n'\n",
    "             'Features that cluster together co-vary across nights', fontsize=12)\n",
    "ax.set_ylabel('Distance (1 - Spearman r)')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Potential cut height')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"• Features joining at LOW height are highly correlated (co-vary together)\")\n",
    "print(\"• Features joining at HIGH height are less related\")\n",
    "print(\"• Look for clusters of activity features vs rest features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Factor Analysis (Varimax Rotation)\n",
    "\n",
    "### What We're Doing\n",
    "Finding underlying **latent dimensions** (factors) that explain the variation in all features.\n",
    "\n",
    "### Intuition\n",
    "Instead of 21 separate features, we ask: \"What are the underlying behavioral dimensions?\"\n",
    "\n",
    "For example, Khatiz found 7 factors:\n",
    "1. Exploratory (Hang, Walk, Climb)\n",
    "2. Foraging (Dig, Sniff)\n",
    "3. Postural-Locomotor (Rear Up, Come Down)\n",
    "4. Sleep-Related (Sleep, Groom, Pause)\n",
    "5. Physically Demanding (Jump, Land)\n",
    "6. Pre/Post Sleep (Awaken, Twitch)\n",
    "7. Nourishment (Eat, Drink)\n",
    "\n",
    "### Varimax Rotation\n",
    "Makes factors easier to interpret by:\n",
    "- Maximizing variance of loadings within each factor\n",
    "- Each factor has a few HIGH loadings and many LOW loadings\n",
    "- Cleaner interpretation: \"Factor 1 = locomotion and climbing\"\n",
    "\n",
    "### How to Read Loadings\n",
    "- **Loading**: How strongly a feature is associated with a factor\n",
    "- |loading| > 0.3 is typically considered meaningful\n",
    "- Positive loading: Feature increases with factor\n",
    "- Negative loading: Feature decreases with factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Analysis with Varimax rotation\n",
    "n_factors = min(7, len(FEATURES) - 1)\n",
    "\n",
    "fa = FactorAnalysis(n_components=n_factors, rotation='varimax', random_state=42)\n",
    "fa.fit(X_scaled)\n",
    "\n",
    "# Extract loadings\n",
    "loadings = pd.DataFrame(\n",
    "    fa.components_.T,\n",
    "    index=FEATURES,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "# Visualize as heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    loadings,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    annot_kws={'size': 8}\n",
    ")\n",
    "ax.set_title('Factor Analysis Loadings\\n'\n",
    "             'Red = positive loading (feature increases with factor)\\n'\n",
    "             'Blue = negative loading (feature decreases with factor)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret factors\n",
    "print(\"\\nFactor Interpretation (|loading| > 0.25):\")\n",
    "print(\"=\"*60)\n",
    "for i in range(n_factors):\n",
    "    col = f'Factor_{i+1}'\n",
    "    top_pos = loadings[col].nlargest(3)\n",
    "    top_neg = loadings[col].nsmallest(3)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    high = [f\"{idx} ({val:+.2f})\" for idx, val in top_pos.items() if abs(val) > 0.25]\n",
    "    low = [f\"{idx} ({val:+.2f})\" for idx, val in top_neg.items() if abs(val) > 0.25]\n",
    "    \n",
    "    if high:\n",
    "        print(f\"  HIGH: {', '.join(high)}\")\n",
    "    if low:\n",
    "        print(f\"  LOW:  {', '.join(low)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Principal Component Analysis (PCA)\n",
    "\n",
    "### What We're Doing\n",
    "Reducing the dimensionality of the data for visualization.\n",
    "\n",
    "### How PCA Works\n",
    "1. Find the direction of **maximum variance** in the data → PC1\n",
    "2. Find the direction **perpendicular to PC1** with maximum variance → PC2\n",
    "3. Continue for more components\n",
    "\n",
    "### Intuition\n",
    "Imagine a cloud of points in 21-dimensional space (21 features). PCA finds the \"best angle\" to view this cloud in 2D, preserving as much of the original structure as possible.\n",
    "\n",
    "### What to Look For\n",
    "- **Variance explained**: How much information is captured by each PC\n",
    "- **Separation**: Do we see distinct clusters in PC1 vs PC2 space?\n",
    "- **Gradients**: Is there a continuous gradient (suggesting continuous variation) or discrete groups?\n",
    "\n",
    "### Expectations\n",
    "If estrous phases are behaviorally distinct, we should see some separation. However, Levy et al. (2023) found estrous explains only ~3% of behavioral variance, so don't expect dramatic separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=min(10, len(FEATURES)))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Add PC coordinates to dataframe\n",
    "for i in range(min(5, X_pca.shape[1])):\n",
    "    df_analysis[f'PC{i+1}'] = X_pca[:, i]\n",
    "\n",
    "# Print variance explained\n",
    "print(\"PCA Results:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVariance explained by each principal component:\")\n",
    "cumulative = 0\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:7]):\n",
    "    cumulative += var\n",
    "    bar = '█' * int(var * 50)\n",
    "    print(f\"  PC{i+1}: {var*100:5.1f}% {bar}  (cumulative: {cumulative*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nFirst 2 PCs explain {pca.explained_variance_ratio_[:2].sum()*100:.1f}% of variance\")\n",
    "print(f\"First 5 PCs explain {pca.explained_variance_ratio_[:5].sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA with behavioral gradients\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Colored by physical activity\n",
    "ax = axes[0]\n",
    "sc = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                c=df_analysis['physically_demanding'], cmap='Reds', alpha=0.6, s=50)\n",
    "plt.colorbar(sc, ax=ax, label='Physically Demanding')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA colored by Physical Activity\\n(Should be HIGH in Estrus)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Colored by sleep\n",
    "ax = axes[1]\n",
    "sc = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                c=df_analysis['sleep_related'], cmap='Blues', alpha=0.6, s=50)\n",
    "plt.colorbar(sc, ax=ax, label='Sleep Duration')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA colored by Sleep\\n(Should be HIGH in Diestrus)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Colored by feeding\n",
    "ax = axes[2]\n",
    "sc = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                c=df_analysis['feeding_resourcing'], cmap='Greens', alpha=0.6, s=50)\n",
    "plt.colorbar(sc, ax=ax, label='Feeding + Drinking')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA colored by Feeding\\n(Should be HIGH in Diestrus)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"• Look for gradients: Do colors change smoothly across the plot?\")\n",
    "print(\"• Look for clusters: Are there distinct groups?\")\n",
    "print(\"• Activity and sleep should show opposite patterns (negative correlation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 K-Means Clustering\n",
    "\n",
    "### What We're Doing\n",
    "Grouping the **nights** (not features!) into clusters based on behavioral similarity.\n",
    "\n",
    "### How K-Means Works\n",
    "1. Randomly initialize K cluster centers\n",
    "2. Assign each night to the nearest center (Euclidean distance in standardized feature space)\n",
    "3. Update centers to be the mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "### Choosing K\n",
    "- **K=2**: Binary classification (high vs low activity)\n",
    "- **K=4**: Four estrous phases (Proestrus, Estrus, Metestrus, Diestrus)\n",
    "- **Optimal K**: Determined by silhouette score\n",
    "\n",
    "### Silhouette Score\n",
    "Measures how well-separated clusters are:\n",
    "- **1.0**: Perfect separation (each point far from other clusters)\n",
    "- **0.0**: Overlapping clusters (points on cluster boundaries)\n",
    "- **< 0**: Points likely assigned to wrong cluster\n",
    "\n",
    "**Interpretation guidelines:**\n",
    "- 0.71-1.0: Strong structure\n",
    "- 0.51-0.70: Reasonable structure\n",
    "- 0.26-0.50: Weak structure\n",
    "- < 0.25: No substantial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different K values\n",
    "print(\"Finding Optimal K:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "k_range = range(2, 8)\n",
    "silhouettes = []\n",
    "inertias = []\n",
    "\n",
    "print(f\"{'k':<5} {'Silhouette':<12} {'Inertia':<15} {'Interpretation'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    silhouettes.append(sil)\n",
    "    inertias.append(km.inertia_)\n",
    "    \n",
    "    interp = {\n",
    "        2: \"Binary: High vs Low activity\",\n",
    "        3: \"Three behavioral states\",\n",
    "        4: \"Four estrous phases (P, E, M, D)\",\n",
    "        5: \"Khatiz used this (4 + male)\",\n",
    "        6: \"Too many for 4-phase cycle\",\n",
    "        7: \"Too many\"\n",
    "    }.get(k, \"\")\n",
    "    \n",
    "    marker = \" ← OPTIMAL\" if sil == max(silhouettes) else \"\"\n",
    "    print(f\"{k:<5} {sil:<12.3f} {km.inertia_:<15.1f} {interp}{marker}\")\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouettes)]\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nStatistically optimal k: {optimal_k} (highest silhouette)\")\n",
    "print(f\"Biologically meaningful k: 4 (matches 4 estrous phases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k selection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Silhouette plot\n",
    "ax = axes[0]\n",
    "ax.plot(list(k_range), silhouettes, 'bo-', markersize=10, linewidth=2)\n",
    "ax.axvline(x=optimal_k, color='green', linestyle='-', linewidth=2, label=f'Optimal k={optimal_k}')\n",
    "ax.axvline(x=4, color='red', linestyle='--', linewidth=2, label='k=4 (biological)')\n",
    "ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax.set_title('Silhouette Score vs k\\n(Higher = better separated clusters)', fontsize=12)\n",
    "ax.set_xticks(list(k_range))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Elbow plot\n",
    "ax = axes[1]\n",
    "ax.plot(list(k_range), inertias, 'ro-', markersize=10, linewidth=2)\n",
    "ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax.set_ylabel('Inertia (Within-cluster SS)', fontsize=12)\n",
    "ax.set_title('Elbow Plot\\n(Look for \"elbow\" where curve bends)', fontsize=12)\n",
    "ax.set_xticks(list(k_range))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. K=2 vs K=4 Comparison\n",
    "\n",
    "Now we compare two approaches:\n",
    "- **K=2**: Binary classification (statistically optimal)\n",
    "- **K=4**: Four phases (biologically motivated)\n",
    "\n",
    "## 7.1 Run Both Clustering Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K=2 clustering\n",
    "print(\"Running K=2 (Binary) Clustering...\")\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_analysis['cluster_k2'] = kmeans_2.fit_predict(X_scaled)\n",
    "\n",
    "# Run K=4 clustering\n",
    "print(\"Running K=4 (4 Phases) Clustering...\")\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df_analysis['cluster_k4'] = kmeans_4.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Characterize and Label K=2 Clusters\n",
    "\n",
    "### Labeling Logic\n",
    "- Cluster with **higher `physically_demanding`** → \"High Activity (Estrus-like)\"\n",
    "- Cluster with **lower `physically_demanding`** → \"Low Activity (Diestrus-like)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features for characterization\n",
    "key_features = ['physically_demanding', 'activity_amplitude', 'locomotion_duration',\n",
    "                'climbing_duration', 'sleep_related', 'feeding_resourcing', 'total_distance']\n",
    "key_features = [f for f in key_features if f in df_analysis.columns]\n",
    "\n",
    "# K=2 cluster means\n",
    "cluster_means_k2 = df_analysis.groupby('cluster_k2')[key_features].mean()\n",
    "\n",
    "print(\"K=2 Cluster Behavioral Profiles:\")\n",
    "print(\"=\"*70)\n",
    "print(cluster_means_k2.round(1).to_string())\n",
    "\n",
    "# Identify high vs low activity clusters\n",
    "high_activity_cluster = cluster_means_k2['physically_demanding'].idxmax()\n",
    "low_activity_cluster = 1 - high_activity_cluster\n",
    "\n",
    "print(f\"\\nCluster {high_activity_cluster} = HIGH ACTIVITY (Estrus-like)\")\n",
    "print(f\"Cluster {low_activity_cluster} = LOW ACTIVITY (Diestrus-like)\")\n",
    "\n",
    "# Apply labels\n",
    "df_analysis['phase_k2'] = df_analysis['cluster_k2'].apply(\n",
    "    lambda x: 'High Activity (Estrus-like)' if x == high_activity_cluster else 'Low Activity (Diestrus-like)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Characterize and Label K=4 Clusters\n",
    "\n",
    "### Labeling Logic (from Khatiz)\n",
    "1. **Estrus**: Highest `physically_demanding`\n",
    "2. **Diestrus**: Highest `sleep_related`\n",
    "3. **Proestrus**: Higher activity of remaining two\n",
    "4. **Metestrus**: Lower activity of remaining two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=4 cluster means\n",
    "cluster_means_k4 = df_analysis.groupby('cluster_k4')[key_features].mean()\n",
    "\n",
    "print(\"K=4 Cluster Behavioral Profiles:\")\n",
    "print(\"=\"*70)\n",
    "print(cluster_means_k4.round(1).to_string())\n",
    "\n",
    "# Label clusters\n",
    "estrus_cluster = cluster_means_k4['physically_demanding'].idxmax()\n",
    "diestrus_cluster = cluster_means_k4['sleep_related'].idxmax()\n",
    "remaining = [c for c in range(4) if c not in [estrus_cluster, diestrus_cluster]]\n",
    "\n",
    "if len(remaining) >= 2:\n",
    "    proestrus_cluster = remaining[0] if cluster_means_k4.loc[remaining[0], 'activity_amplitude'] > cluster_means_k4.loc[remaining[1], 'activity_amplitude'] else remaining[1]\n",
    "    metestrus_cluster = remaining[1] if proestrus_cluster == remaining[0] else remaining[0]\n",
    "else:\n",
    "    proestrus_cluster = remaining[0] if remaining else None\n",
    "    metestrus_cluster = None\n",
    "\n",
    "print(f\"\\nCluster assignments:\")\n",
    "print(f\"  Cluster {estrus_cluster} → ESTRUS (highest physically_demanding)\")\n",
    "print(f\"  Cluster {diestrus_cluster} → DIESTRUS (highest sleep)\")\n",
    "if proestrus_cluster is not None:\n",
    "    print(f\"  Cluster {proestrus_cluster} → PROESTRUS (higher activity of remaining)\")\n",
    "if metestrus_cluster is not None:\n",
    "    print(f\"  Cluster {metestrus_cluster} → METESTRUS (lower activity of remaining)\")\n",
    "\n",
    "# Apply labels\n",
    "labels_k4 = {estrus_cluster: 'Estrus', diestrus_cluster: 'Diestrus'}\n",
    "if proestrus_cluster is not None:\n",
    "    labels_k4[proestrus_cluster] = 'Proestrus'\n",
    "if metestrus_cluster is not None:\n",
    "    labels_k4[metestrus_cluster] = 'Metestrus'\n",
    "\n",
    "df_analysis['phase_k4'] = df_analysis['cluster_k4'].map(labels_k4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Compare Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DISTRIBUTION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- K=2 Classification ---\")\n",
    "k2_counts = df_analysis['phase_k2'].value_counts()\n",
    "for phase, count in k2_counts.items():\n",
    "    pct = 100 * count / len(df_analysis)\n",
    "    bar = '█' * int(pct / 2)\n",
    "    print(f\"  {phase:<35}: {count:3d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\n--- K=4 Classification ---\")\n",
    "k4_counts = df_analysis['phase_k4'].value_counts()\n",
    "for phase in ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']:\n",
    "    if phase in k4_counts.index:\n",
    "        count = k4_counts[phase]\n",
    "        pct = 100 * count / len(df_analysis)\n",
    "        bar = '█' * int(pct / 2)\n",
    "        print(f\"  {phase:<35}: {count:3d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\n--- Expected (Biology) ---\")\n",
    "print(\"  Proestrus:  10-15%\")\n",
    "print(\"  Estrus:     10-15%\")\n",
    "print(\"  Metestrus:  20-25%\")\n",
    "print(\"  Diestrus:   45-55%\")\n",
    "print(\"  OR Binary:\")\n",
    "print(\"  High Activity (P+E): 20-30%\")\n",
    "print(\"  Low Activity (M+D):  70-80%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Cross-Tabulation: How K=4 Maps to K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-TABULATION: How K=4 phases map to K=2 groups\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "crosstab = pd.crosstab(df_analysis['phase_k4'], df_analysis['phase_k2'], margins=True)\n",
    "print(\"\\nCounts:\")\n",
    "print(crosstab.to_string())\n",
    "\n",
    "print(\"\\nPercentages (within each K=4 phase):\")\n",
    "crosstab_pct = pd.crosstab(df_analysis['phase_k4'], df_analysis['phase_k2'], normalize='index') * 100\n",
    "print(crosstab_pct.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Statistical Validation (ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANOVA VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# K=2 ANOVA\n",
    "print(\"\\n--- K=2 ANOVA ---\")\n",
    "print(f\"{'Feature':<25} {'F-stat':<10} {'p-value':<12} {'Sig?'}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "sig_k2 = 0\n",
    "for feat in key_features:\n",
    "    groups = [df_analysis[df_analysis['cluster_k2'] == c][feat].dropna().values for c in range(2)]\n",
    "    groups = [g for g in groups if len(g) > 0]\n",
    "    if len(groups) >= 2:\n",
    "        f_stat, p = f_oneway(*groups)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        if p < 0.05:\n",
    "            sig_k2 += 1\n",
    "        print(f\"{feat:<25} {f_stat:<10.2f} {p:<12.6f} {sig}\")\n",
    "\n",
    "print(f\"\\nSignificant: {sig_k2}/{len(key_features)}\")\n",
    "\n",
    "# K=4 ANOVA\n",
    "print(\"\\n--- K=4 ANOVA ---\")\n",
    "print(f\"{'Feature':<25} {'F-stat':<10} {'p-value':<12} {'Sig?'}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "sig_k4 = 0\n",
    "for feat in key_features:\n",
    "    groups = [df_analysis[df_analysis['cluster_k4'] == c][feat].dropna().values for c in range(4)]\n",
    "    groups = [g for g in groups if len(g) > 0]\n",
    "    if len(groups) >= 2:\n",
    "        f_stat, p = f_oneway(*groups)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        if p < 0.05:\n",
    "            sig_k4 += 1\n",
    "        print(f\"{feat:<25} {f_stat:<10.2f} {p:<12.6f} {sig}\")\n",
    "\n",
    "print(f\"\\nSignificant: {sig_k4}/{len(key_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Visualization: K=2 vs K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "colors_k2 = {'High Activity (Estrus-like)': '#e74c3c', 'Low Activity (Diestrus-like)': '#3498db'}\n",
    "colors_k4 = {'Proestrus': '#9b59b6', 'Estrus': '#e74c3c', 'Metestrus': '#3498db', 'Diestrus': '#2ecc71'}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Row 1: K=2\n",
    "ax = axes[0, 0]\n",
    "for phase in df_analysis['phase_k2'].unique():\n",
    "    mask = df_analysis['phase_k2'] == phase\n",
    "    ax.scatter(df_analysis.loc[mask, 'PC1'], df_analysis.loc[mask, 'PC2'],\n",
    "               c=colors_k2.get(phase, 'gray'), label=phase, alpha=0.6, s=50)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('K=2: PCA Visualization')\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "k2_counts.plot(kind='bar', ax=ax, color=[colors_k2.get(x, 'gray') for x in k2_counts.index])\n",
    "ax.set_title('K=2: Distribution')\n",
    "ax.set_ylabel('Number of Nights')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for i, (idx, val) in enumerate(k2_counts.items()):\n",
    "    ax.text(i, val + 2, f'{100*val/len(df_analysis):.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "ax = axes[0, 2]\n",
    "data_k2 = [df_analysis[df_analysis['phase_k2'] == p]['physically_demanding'].dropna() \n",
    "           for p in ['High Activity (Estrus-like)', 'Low Activity (Diestrus-like)'] \n",
    "           if p in df_analysis['phase_k2'].values]\n",
    "labels_k2_box = [p for p in ['High Activity (Estrus-like)', 'Low Activity (Diestrus-like)'] \n",
    "                 if p in df_analysis['phase_k2'].values]\n",
    "bp = ax.boxplot(data_k2, labels=['High Activity', 'Low Activity'], patch_artist=True)\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    box.set_facecolor(colors_k2.get(labels_k2_box[i], 'gray'))\n",
    "ax.set_title('K=2: Physically Demanding by Phase')\n",
    "ax.set_ylabel('Duration (s)')\n",
    "\n",
    "# Row 2: K=4\n",
    "ax = axes[1, 0]\n",
    "for phase in ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']:\n",
    "    mask = df_analysis['phase_k4'] == phase\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(df_analysis.loc[mask, 'PC1'], df_analysis.loc[mask, 'PC2'],\n",
    "                   c=colors_k4.get(phase, 'gray'), label=phase, alpha=0.6, s=50)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('K=4: PCA Visualization')\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "k4_order = ['Proestrus', 'Estrus', 'Metestrus', 'Diestrus']\n",
    "k4_vals = [k4_counts.get(p, 0) for p in k4_order]\n",
    "ax.bar(k4_order, k4_vals, color=[colors_k4[p] for p in k4_order])\n",
    "ax.set_title('K=4: Distribution')\n",
    "ax.set_ylabel('Number of Nights')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for i, val in enumerate(k4_vals):\n",
    "    if val > 0:\n",
    "        ax.text(i, val + 2, f'{100*val/len(df_analysis):.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "ax = axes[1, 2]\n",
    "data_k4 = [df_analysis[df_analysis['phase_k4'] == p]['physically_demanding'].dropna() \n",
    "           for p in k4_order if p in df_analysis['phase_k4'].values]\n",
    "labels_k4_box = [p for p in k4_order if p in df_analysis['phase_k4'].values]\n",
    "bp = ax.boxplot(data_k4, labels=labels_k4_box, patch_artist=True)\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    box.set_facecolor(colors_k4.get(labels_k4_box[i], 'gray'))\n",
    "ax.set_title('K=4: Physically Demanding by Phase')\n",
    "ax.set_ylabel('Duration (s)')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Comparison: K=2 (Binary) vs K=4 (4 Phases)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "sil_k2 = silhouette_score(X_scaled, df_analysis['cluster_k2'])\n",
    "sil_k4 = silhouette_score(X_scaled, df_analysis['cluster_k4'])\n",
    "\n",
    "high_pct = 100 * k2_counts.get('High Activity (Estrus-like)', 0) / len(df_analysis)\n",
    "low_pct = 100 * k2_counts.get('Low Activity (Diestrus-like)', 0) / len(df_analysis)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "┌────────────────────────────┬─────────────────────────┬─────────────────────────┐\n",
    "│ Metric                     │ K=2 (Binary)            │ K=4 (4 Phases)          │\n",
    "├────────────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Silhouette Score           │ {sil_k2:.3f}                   │ {sil_k4:.3f}                   │\n",
    "│ Better separated?          │ {'YES ✓' if sil_k2 > sil_k4 else 'NO':<23} │ {'YES ✓' if sil_k4 > sil_k2 else 'NO':<23} │\n",
    "├────────────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Significant ANOVA features │ {sig_k2}/{len(key_features)}                     │ {sig_k4}/{len(key_features)}                     │\n",
    "├────────────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Distribution               │ High: {high_pct:.1f}%             │ Varies                  │\n",
    "│                            │ Low: {low_pct:.1f}%              │                         │\n",
    "├────────────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Matches biology?           │ {'YES ✓' if 20 <= high_pct <= 40 else 'NO':<23} │ Check above             │\n",
    "│ (Expected: 20-30% high)    │                         │                         │\n",
    "├────────────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Interpretation             │ Clear                   │ Ambiguous               │\n",
    "└────────────────────────────┴─────────────────────────┴─────────────────────────┘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Final Results & Recommendations\n",
    "\n",
    "## 8.1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 3 FINAL SUMMARY: Estrous Detection from Behavioral Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET:\n",
    "--------\n",
    "• Study: JAX Envision Morph2REP (Study 1001, v2025v3.3)\n",
    "• Animals: {df_analysis['animal_id'].nunique()} female C57BL/6J mice (vehicle controls)\n",
    "• Nights analyzed: {len(df_analysis)} (after cleaning)\n",
    "• Features: {len(FEATURES)}\n",
    "• Time period: Dark cycle (6 PM - 6 AM EST)\n",
    "\n",
    "DATA CLEANING:\n",
    "--------------\n",
    "• Excluded cage change days (abnormal stress behavior)\n",
    "• Excluded truncated end-of-study days (incomplete data)\n",
    "• Excluded acclimation spillover\n",
    "\n",
    "METHODOLOGY:\n",
    "------------\n",
    "Following Khatiz et al. (2025):\n",
    "1. Hierarchical clustering of features (Spearman correlation)\n",
    "2. Factor Analysis with Varimax rotation\n",
    "3. PCA for visualization\n",
    "4. K-Means clustering of nights\n",
    "5. ANOVA validation\n",
    "\n",
    "KEY FINDINGS:\n",
    "-------------\n",
    "1. Optimal k by silhouette score: {optimal_k}\n",
    "2. K=2 has better cluster separation (silhouette: {sil_k2:.3f} vs {sil_k4:.3f})\n",
    "3. K=2 distribution ({high_pct:.1f}% high / {low_pct:.1f}% low) matches expected biology\n",
    "4. Both K=2 and K=4 show significant ANOVA results\n",
    "\n",
    "RECOMMENDATION:\n",
    "---------------\n",
    "Use K=2 BINARY CLASSIFICATION for downstream morphine analysis:\n",
    "\n",
    "  ┌─────────────────────────────────────────────────────────┐\n",
    "  │  HIGH ACTIVITY STATE (Estrus-like)                      │\n",
    "  │  • {high_pct:.1f}% of nights                                    │\n",
    "  │  • Likely corresponds to Proestrus + Estrus             │\n",
    "  │  • Higher estrogen period                               │\n",
    "  │  • More locomotion, climbing, exploration               │\n",
    "  ├─────────────────────────────────────────────────────────┤\n",
    "  │  LOW ACTIVITY STATE (Diestrus-like)                     │\n",
    "  │  • {low_pct:.1f}% of nights                                    │\n",
    "  │  • Likely corresponds to Metestrus + Diestrus           │\n",
    "  │  • Lower estrogen period                                │\n",
    "  │  • More sleep, feeding, less physical activity          │\n",
    "  └─────────────────────────────────────────────────────────┘\n",
    "\n",
    "IMPORTANT CAVEAT:\n",
    "-----------------\n",
    "Without vaginal cytology ground truth, these are BEHAVIORAL classifications\n",
    "that we HYPOTHESIZE correspond to estrous phases. We cannot confirm true\n",
    "hormonal status from behavior alone.\n",
    "\n",
    "For morphine analysis, we can ask:\n",
    "\"Does morphine response differ between High vs Low activity behavioral states?\"\n",
    "rather than claiming to know exact estrous phases.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = 'phase3_estrous_classification_final.csv'\n",
    "df_analysis.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print(f\"\\nKey columns:\")\n",
    "print(f\"  • cage_id, animal_id, night_date: Identifiers\")\n",
    "print(f\"  • cluster_k2, phase_k2: Binary classification\")\n",
    "print(f\"  • cluster_k4, phase_k4: 4-phase classification\")\n",
    "print(f\"  • PC1-PC5: Principal component coordinates\")\n",
    "print(f\"  • All behavioral features: For downstream analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: Technical Reference\n",
    "\n",
    "## A.1 Glossary of Statistical Methods\n",
    "\n",
    "| Method | What It Does | When to Use |\n",
    "|--------|--------------|-------------|\n",
    "| **Spearman Correlation** | Measures monotonic relationship between variables using ranks | Non-normal data, outliers present |\n",
    "| **Hierarchical Clustering** | Groups items by similarity, creates tree structure | Exploring relationships without predefined k |\n",
    "| **Factor Analysis** | Finds latent variables underlying observed features | Reducing many variables to interpretable dimensions |\n",
    "| **PCA** | Linear projection maximizing variance | Dimensionality reduction, visualization |\n",
    "| **K-Means** | Partitions data into k groups minimizing within-cluster variance | When you know (or hypothesize) number of groups |\n",
    "| **Silhouette Score** | Measures cluster separation quality | Choosing optimal k |\n",
    "| **ANOVA** | Tests if group means differ significantly | Validating that clusters are distinct |\n",
    "\n",
    "## A.2 Key References\n",
    "\n",
    "1. **Khatiz et al. (2025)** - Frontiers in Neuroscience 19:1509822\n",
    "   - Validated behavioral estrous detection with vaginal cytology\n",
    "   - Our methodology is based on their approach\n",
    "\n",
    "2. **Levy et al. (2023)** - Found estrous explains ~3% of behavioral variance\n",
    "   - Explains why we don't see dramatic cluster separation\n",
    "\n",
    "## A.3 Reproducibility\n",
    "\n",
    "- Random seed: 42 (for K-Means, Factor Analysis)\n",
    "- Data version: 2025v3.3\n",
    "- Analysis date: February 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
