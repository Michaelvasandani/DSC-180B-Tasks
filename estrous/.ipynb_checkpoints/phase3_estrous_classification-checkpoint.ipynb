{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Behavioral State Classification for Estrous Detection\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Objective:** Classify individual nights as \"estrus-like\" or \"diestrus-like\" based on behavioral features.\n",
    "\n",
    "**Dataset:** JAX Envision Morph2REP (Study 1001, 2025v3.3)\n",
    "- 18 female C57BL/6J mice (vehicle controls)\n",
    "- 14-16 nights per animal\n",
    "- Dark cycle behavioral bouts (6 PM - 6 AM EST)\n",
    "\n",
    "---\n",
    "\n",
    "## Literature Foundation: Khatiz et al. (2025)\n",
    "\n",
    "**\"Real-time behavioral monitoring of C57BL/6J mice during reproductive cycle\"**  \n",
    "*Frontiers in Neuroscience, 19:1509822*\n",
    "\n",
    "### Key Behavioral Markers\n",
    "\n",
    "| Estrus (High Estrogen) | Metestrus/Diestrus (Low Estrogen) |\n",
    "|------------------------|-----------------------------------|\n",
    "| 30% more physically demanding activity | Lower overall activity |\n",
    "| Sustained activity bouts (low fragmentation) | Fragmented activity (more bouts) |\n",
    "| Higher exploratory behavior | More sleep-related behavior |\n",
    "| Lower feeding during dark cycle | Higher feeding/habituation |\n",
    "| More locomotion bout counts | Fewer locomotion bouts |\n",
    "| Less sleep fragmentation | More sleep fragmentation (more rousings) |\n",
    "\n",
    "### Statistical Methods (Data-Driven, No Manual Weights)\n",
    "\n",
    "1. **Standardization** - All features on equal footing\n",
    "2. **Factor Analysis** - Identify underlying behavioral dimensions (loadings = data-derived weights)\n",
    "3. **PCA** - Reduce dimensionality, find primary axes of differentiation\n",
    "4. **K-Means Clustering** - Let data find natural groupings\n",
    "5. **Hierarchical Clustering** - Alternative clustering for comparison\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Khatiz A, et al. (2025). Real-time behavioral monitoring of C57BL/6J mice during reproductive cycle. *Front. Neurosci.* 19:1509822.\n",
    "2. Levy DR, et al. (2023). Mouse spontaneous behavior reflects individual variation rather than estrous state. *Curr Biol.* 33:1358-1364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup and Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install duckdb pyarrow umap-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tools\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, ttest_ind\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# ML tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import umap\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "S3_BASE = \"s3://jax-envision-public-data/study_1001/2025v3.3/tabular\"\n",
    "\n",
    "# Vehicle control cages (14-16 days of unconfounded baseline)\n",
    "VEHICLE_CAGES = {\n",
    "    'Rep1': {\n",
    "        'cages': [4918, 4922, 4923],\n",
    "        'start_date': '2025-01-07',\n",
    "        'end_date': '2025-01-22',\n",
    "    },\n",
    "    'Rep2': {\n",
    "        'cages': [4928, 4929, 4934],\n",
    "        'start_date': '2025-01-22',\n",
    "        'end_date': '2025-02-04',\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Vehicle cages: {[c for rep in VEHICLE_CAGES.values() for c in rep['cages']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load animal_bouts.parquet\n",
    "def load_bout_data(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal_bouts.parquet for a single cage across date range.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_bouts.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Function to load animal_bout_metrics.parquet (distance traveled)\n",
    "def load_bout_metrics(cage_id, start_date, end_date):\n",
    "    \"\"\"Load animal_bout_metrics.parquet with distance traveled per bout.\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        path = f\"{S3_BASE}/cage_id={cage_id}/date={date_str}/animal_bout_metrics.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = conn.execute(f\"SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "            df['cage_id'] = cage_id\n",
    "            df['date'] = date_str\n",
    "            all_data.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bout data for all vehicle control cages\n",
    "print(\"Loading bout data (animal_bouts.parquet)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_bouts = []\n",
    "for rep_name, rep_config in VEHICLE_CAGES.items():\n",
    "    print(f\"{rep_name}:\")\n",
    "    for cage_id in rep_config['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_bout_data(cage_id, rep_config['start_date'], rep_config['end_date'])\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep_name\n",
    "            all_bouts.append(df)\n",
    "            print(f\"{len(df):,} bouts\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bouts = pd.concat(all_bouts, ignore_index=True)\n",
    "print(\"=\"*60)\n",
    "print(f\"Total bouts loaded: {len(df_bouts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bout metrics (distance traveled) for all vehicle control cages\n",
    "print(\"Loading bout metrics (animal_bout_metrics.parquet)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_bout_metrics = []\n",
    "for rep_name, rep_config in VEHICLE_CAGES.items():\n",
    "    print(f\"{rep_name}:\")\n",
    "    for cage_id in rep_config['cages']:\n",
    "        print(f\"  Cage {cage_id}...\", end=\" \")\n",
    "        df = load_bout_metrics(cage_id, rep_config['start_date'], rep_config['end_date'])\n",
    "        if len(df) > 0:\n",
    "            df['replicate'] = rep_name\n",
    "            all_bout_metrics.append(df)\n",
    "            print(f\"{len(df):,} rows\")\n",
    "        else:\n",
    "            print(\"No data\")\n",
    "\n",
    "df_bout_metrics = pd.concat(all_bout_metrics, ignore_index=True)\n",
    "print(\"=\"*60)\n",
    "print(f\"Total bout metrics loaded: {len(df_bout_metrics):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "print(\"Bout data columns:\", df_bouts.columns.tolist())\n",
    "print(\"\\nUnique state names (bouts):\")\n",
    "print(df_bouts['state_name'].unique())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Bout metrics columns:\", df_bout_metrics.columns.tolist())\n",
    "print(\"\\nMetric names:\", df_bout_metrics['metric_name'].unique())\n",
    "\n",
    "print(f\"\\nUnique animals: {df_bouts['animal_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nightly_summaries_with_exploration(df_bouts, df_bout_metrics):\n",
    "    \"\"\"\n",
    "    Compute comprehensive nightly summaries including:\n",
    "    - Duration metrics (from animal_bouts)\n",
    "    - Bout count metrics (from animal_bouts)  \n",
    "    - Exploration metrics (from animal_bout_metrics - distance traveled)\n",
    "    \"\"\"\n",
    "    \n",
    "    STATE_MAP = {\n",
    "        'active': 'animal_bouts.active',\n",
    "        'inactive': 'animal_bouts.inactive',\n",
    "        'locomotion': 'animal_bouts.locomotion',\n",
    "        'feeding': 'animal_bouts.feeding',\n",
    "        'drinking': 'animal_bouts.drinking',\n",
    "        'climbing': 'animal_bouts.climbing',\n",
    "        'inferred_sleep': 'animal_bouts.inferred_sleep',\n",
    "    }\n",
    "    \n",
    "    # ============================================\n",
    "    # Process bouts data\n",
    "    # ============================================\n",
    "    df_bouts = df_bouts.copy()\n",
    "    df_bouts['start_time'] = pd.to_datetime(df_bouts['start_time'])\n",
    "    df_bouts['hour_utc'] = df_bouts['start_time'].dt.hour\n",
    "    \n",
    "    # Dark cycle: 6 PM - 6 AM EST = 23:00 - 11:00 UTC\n",
    "    df_bouts['is_dark'] = (df_bouts['hour_utc'] >= 23) | (df_bouts['hour_utc'] < 11)\n",
    "    \n",
    "    # Night date assignment\n",
    "    df_bouts['night_date'] = df_bouts['start_time'].dt.date\n",
    "    mask_early = df_bouts['hour_utc'] < 11\n",
    "    df_bouts.loc[mask_early, 'night_date'] = (\n",
    "        pd.to_datetime(df_bouts.loc[mask_early, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    \n",
    "    df_dark = df_bouts[df_bouts['is_dark']].copy()\n",
    "    \n",
    "    # ============================================\n",
    "    # Process bout metrics (distance traveled)\n",
    "    # ============================================\n",
    "    df_metrics = df_bout_metrics.copy()\n",
    "    df_metrics['start_time'] = pd.to_datetime(df_metrics['start_time'])\n",
    "    df_metrics['hour_utc'] = df_metrics['start_time'].dt.hour\n",
    "    df_metrics['is_dark'] = (df_metrics['hour_utc'] >= 23) | (df_metrics['hour_utc'] < 11)\n",
    "    \n",
    "    df_metrics['night_date'] = df_metrics['start_time'].dt.date\n",
    "    mask_early = df_metrics['hour_utc'] < 11\n",
    "    df_metrics.loc[mask_early, 'night_date'] = (\n",
    "        pd.to_datetime(df_metrics.loc[mask_early, 'start_time']) - timedelta(days=1)\n",
    "    ).dt.date\n",
    "    \n",
    "    df_metrics_dark = df_metrics[df_metrics['is_dark']].copy()\n",
    "    \n",
    "    # ============================================\n",
    "    # Aggregate bout-based metrics\n",
    "    # ============================================\n",
    "    print(\"Aggregating bout data...\")\n",
    "    results = []\n",
    "    \n",
    "    for (cage_id, animal_id, night_date), group in df_dark.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'night_date': night_date,\n",
    "        }\n",
    "        \n",
    "        # Duration and bout count per state\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_data = group[group['state_name'] == full_name]\n",
    "            row[f'{short_name}_duration'] = state_data['bout_length_seconds'].sum()\n",
    "            row[f'{short_name}_bout_count'] = len(state_data)\n",
    "            row[f'{short_name}_mean_bout'] = state_data['bout_length_seconds'].mean() if len(state_data) > 0 else 0\n",
    "        \n",
    "        row['total_dark_seconds'] = group['bout_length_seconds'].sum()\n",
    "        results.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(results)\n",
    "    \n",
    "    # ============================================\n",
    "    # Aggregate exploration metrics (distance traveled)\n",
    "    # ============================================\n",
    "    print(\"Aggregating exploration data...\")\n",
    "    exploration_results = []\n",
    "    \n",
    "    for (cage_id, animal_id, night_date), group in df_metrics_dark.groupby(['cage_id', 'animal_id', 'night_date']):\n",
    "        row = {\n",
    "            'cage_id': cage_id,\n",
    "            'animal_id': animal_id,\n",
    "            'night_date': night_date,\n",
    "        }\n",
    "        \n",
    "        # Total distance traveled (all states)\n",
    "        row['total_distance'] = group['metric_value'].sum()\n",
    "        \n",
    "        # Distance by state\n",
    "        for short_name, full_name in STATE_MAP.items():\n",
    "            state_data = group[group['state_name'] == full_name]\n",
    "            row[f'{short_name}_distance'] = state_data['metric_value'].sum()\n",
    "            row[f'{short_name}_distance_per_bout'] = state_data['metric_value'].mean() if len(state_data) > 0 else 0\n",
    "        \n",
    "        exploration_results.append(row)\n",
    "    \n",
    "    df_exploration = pd.DataFrame(exploration_results)\n",
    "    \n",
    "    # ============================================\n",
    "    # Merge bout and exploration summaries\n",
    "    # ============================================\n",
    "    print(\"Merging data...\")\n",
    "    df_summary = df_summary.merge(\n",
    "        df_exploration, \n",
    "        on=['cage_id', 'animal_id', 'night_date'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute derived features\n",
    "    # ============================================\n",
    "    \n",
    "    # Activity metrics\n",
    "    df_summary['activity_amplitude'] = df_summary['active_duration'] + df_summary['locomotion_duration']\n",
    "    \n",
    "    # Fragmentation metrics (bouts per unit time)\n",
    "    df_summary['sleep_fragmentation'] = df_summary['inferred_sleep_bout_count'] / (df_summary['inferred_sleep_duration'] + 1)\n",
    "    df_summary['active_fragmentation'] = df_summary['active_bout_count'] / (df_summary['active_duration'] + 1)\n",
    "    \n",
    "    # Ratios\n",
    "    df_summary['feeding_ratio'] = df_summary['feeding_duration'] / (df_summary['active_duration'] + 1)\n",
    "    df_summary['sleep_ratio'] = df_summary['inferred_sleep_duration'] / (df_summary['total_dark_seconds'] + 1)\n",
    "    \n",
    "    # Exploration metrics\n",
    "    df_summary['exploration_intensity'] = df_summary['total_distance'] / (df_summary['activity_amplitude'] + 1)\n",
    "    df_summary['locomotion_efficiency'] = df_summary['locomotion_distance'] / (df_summary['locomotion_duration'] + 1)\n",
    "    \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nightly summaries\n",
    "print(\"Computing nightly summaries with exploration metrics...\")\n",
    "print(\"=\"*60)\n",
    "df_nightly = compute_nightly_summaries_with_exploration(df_bouts, df_bout_metrics)\n",
    "\n",
    "# Filter out animal_id = 0 (tracking errors)\n",
    "df_nightly = df_nightly[df_nightly['animal_id'] != 0].copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Nightly summaries: {len(df_nightly)} animal-nights\")\n",
    "print(f\"Animals: {df_nightly['animal_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect features\n",
    "print(\"Feature Summary:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "key_features = [\n",
    "    'activity_amplitude', 'locomotion_duration', 'inactive_duration',\n",
    "    'inferred_sleep_duration', 'feeding_duration', 'climbing_duration',\n",
    "    'locomotion_bout_count', 'climbing_bout_count', 'active_mean_bout',\n",
    "    'active_fragmentation', 'sleep_fragmentation',\n",
    "    'total_distance', 'locomotion_distance', 'exploration_intensity'\n",
    "]\n",
    "\n",
    "for feat in key_features:\n",
    "    if feat in df_nightly.columns:\n",
    "        print(f\"{feat:30}: mean={df_nightly[feat].mean():.2f}, std={df_nightly[feat].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Data-Driven Classification (Khatiz Method)\n",
    "---\n",
    "\n",
    "**Key principle:** NO manual weights. All features are standardized equally, and we let statistical methods (Factor Analysis, PCA, K-Means) find the natural structure in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for analysis (based on Khatiz findings)\n",
    "FEATURES_FOR_ANALYSIS = [\n",
    "    # Duration metrics\n",
    "    'activity_amplitude',\n",
    "    'locomotion_duration', \n",
    "    'inactive_duration',\n",
    "    'inferred_sleep_duration',\n",
    "    'feeding_duration',\n",
    "    'climbing_duration',\n",
    "    \n",
    "    # Bout metrics\n",
    "    'locomotion_bout_count',\n",
    "    'climbing_bout_count',\n",
    "    'active_mean_bout',\n",
    "    'active_fragmentation',\n",
    "    'sleep_fragmentation',\n",
    "    \n",
    "    # Exploration metrics\n",
    "    'total_distance',\n",
    "    'locomotion_distance',\n",
    "    'exploration_intensity',\n",
    "]\n",
    "\n",
    "# Prepare data - drop rows with missing values\n",
    "df_analysis = df_nightly.dropna(subset=FEATURES_FOR_ANALYSIS).copy()\n",
    "X = df_analysis[FEATURES_FOR_ANALYSIS].values\n",
    "\n",
    "# STANDARDIZE (like Khatiz - puts all metrics on equal footing)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Data prepared: {X_scaled.shape[0]} nights × {X_scaled.shape[1]} features\")\n",
    "print(f\"\\nFeatures (all equally weighted after standardization):\")\n",
    "for f in FEATURES_FOR_ANALYSIS:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Factor Analysis (Data-Derived Loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Analysis - let the DATA determine the weights (loadings)\n",
    "n_factors = 3\n",
    "fa = FactorAnalysis(n_components=n_factors, random_state=42)\n",
    "fa.fit(X_scaled)\n",
    "\n",
    "# Get loadings (these are DATA-DERIVED weights!)\n",
    "loadings = pd.DataFrame(\n",
    "    fa.components_.T,\n",
    "    index=FEATURES_FOR_ANALYSIS,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FACTOR ANALYSIS LOADINGS (Data-Derived Weights)\")\n",
    "print(\"=\"*70)\n",
    "print(loadings.round(3).to_string())\n",
    "\n",
    "# Interpret factors\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FACTOR INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "for i in range(n_factors):\n",
    "    col = f'Factor_{i+1}'\n",
    "    print(f\"\\n{col}:\")\n",
    "    \n",
    "    top_pos = loadings[col].nlargest(3)\n",
    "    print(f\"  HIGH: {', '.join([f'{idx} ({val:+.2f})' for idx, val in top_pos.items()])}\")\n",
    "    \n",
    "    top_neg = loadings[col].nsmallest(3)\n",
    "    print(f\"  LOW:  {', '.join([f'{idx} ({val:+.2f})' for idx, val in top_neg.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PCA (Principal Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - standardized, no manual weights\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Add PCA coordinates to dataframe\n",
    "for i in range(5):\n",
    "    df_analysis[f'PC{i+1}'] = X_pca[:, i]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PCA RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nVariance explained:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var*100:.1f}%\")\n",
    "print(f\"  Total (5 PCs): {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n",
    "\n",
    "# PCA loadings\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=FEATURES_FOR_ANALYSIS,\n",
    "    columns=[f'PC{i+1}' for i in range(5)]\n",
    ")\n",
    "print(\"\\nPCA Loadings (first 3 components):\")\n",
    "print(pca_loadings[['PC1', 'PC2', 'PC3']].round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 K-Means Clustering (Find Natural Groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means - let algorithm find natural clusters\n",
    "print(\"=\"*70)\n",
    "print(\"K-MEANS CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try different k values\n",
    "print(f\"\\n{'k':<5} {'Silhouette':<12} {'Interpretation'}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "silhouettes = []\n",
    "for k in range(2, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    silhouettes.append(sil)\n",
    "    \n",
    "    if k == 2:\n",
    "        interp = \"High vs Low activity?\"\n",
    "    elif k == 4:\n",
    "        interp = \"4 estrous phases?\"\n",
    "    else:\n",
    "        interp = \"\"\n",
    "    print(f\"{k:<5} {sil:<12.3f} {interp}\")\n",
    "\n",
    "optimal_k = range(2, 7)[np.argmax(silhouettes)]\n",
    "print(f\"\\nOptimal k by silhouette: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with k=2 for binary classification\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_analysis['kmeans_cluster'] = kmeans_2.fit_predict(X_scaled)\n",
    "\n",
    "# Determine which cluster is \"estrus-like\" based on activity\n",
    "cluster_means = df_analysis.groupby('kmeans_cluster')[FEATURES_FOR_ANALYSIS].mean()\n",
    "print(\"\\nCluster means (key features):\")\n",
    "print(cluster_means[['activity_amplitude', 'total_distance', 'inactive_duration', 'inferred_sleep_duration']].round(1))\n",
    "\n",
    "# Higher activity cluster = estrus-like\n",
    "estrus_cluster = cluster_means['activity_amplitude'].idxmax()\n",
    "df_analysis['kmeans_state'] = df_analysis['kmeans_cluster'].apply(\n",
    "    lambda x: 'Estrus-like' if x == estrus_cluster else 'Diestrus-like'\n",
    ")\n",
    "\n",
    "km_counts = df_analysis['kmeans_state'].value_counts()\n",
    "print(f\"\\nK-Means Classification (k=2):\")\n",
    "print(f\"  Estrus-like:   {km_counts.get('Estrus-like', 0)} ({100*km_counts.get('Estrus-like', 0)/len(df_analysis):.1f}%)\")\n",
    "print(f\"  Diestrus-like: {km_counts.get('Diestrus-like', 0)} ({100*km_counts.get('Diestrus-like', 0)/len(df_analysis):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering with Ward linkage\n",
    "print(\"=\"*70)\n",
    "print(\"HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Cut at k=2\n",
    "hc_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n",
    "df_analysis['hc_cluster'] = hc_labels\n",
    "\n",
    "# Determine which cluster is estrus-like\n",
    "hc_means = df_analysis.groupby('hc_cluster')[FEATURES_FOR_ANALYSIS].mean()\n",
    "hc_estrus = hc_means['activity_amplitude'].idxmax()\n",
    "\n",
    "df_analysis['hc_state'] = df_analysis['hc_cluster'].apply(\n",
    "    lambda x: 'Estrus-like' if x == hc_estrus else 'Diestrus-like'\n",
    ")\n",
    "\n",
    "hc_counts = df_analysis['hc_state'].value_counts()\n",
    "print(f\"\\nHierarchical Classification (k=2):\")\n",
    "print(f\"  Estrus-like:   {hc_counts.get('Estrus-like', 0)} ({100*hc_counts.get('Estrus-like', 0)/len(df_analysis):.1f}%)\")\n",
    "print(f\"  Diestrus-like: {hc_counts.get('Diestrus-like', 0)} ({100*hc_counts.get('Diestrus-like', 0)/len(df_analysis):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Compare Classification Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classifications from different methods\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON OF DATA-DRIVEN CLASSIFICATION METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nClassification Distribution:\")\n",
    "print(\"-\"*50)\n",
    "km_pct = 100 * (df_analysis['kmeans_state'] == 'Estrus-like').sum() / len(df_analysis)\n",
    "hc_pct = 100 * (df_analysis['hc_state'] == 'Estrus-like').sum() / len(df_analysis)\n",
    "print(f\"  K-Means:       {km_pct:.1f}% Estrus-like\")\n",
    "print(f\"  Hierarchical:  {hc_pct:.1f}% Estrus-like\")\n",
    "print(f\"  Expected:      ~25% (biological expectation)\")\n",
    "\n",
    "# Agreement between methods\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Method Agreement:\")\n",
    "km_binary = (df_analysis['kmeans_state'] == 'Estrus-like').astype(int)\n",
    "hc_binary = (df_analysis['hc_state'] == 'Estrus-like').astype(int)\n",
    "ari = adjusted_rand_score(km_binary, hc_binary)\n",
    "print(f\"  K-Means vs Hierarchical: ARI = {ari:.3f}\")\n",
    "print(f\"  (ARI: 1.0 = perfect agreement, 0 = random)\")\n",
    "\n",
    "# Cross-tabulation\n",
    "print(\"\\nCross-tabulation:\")\n",
    "print(pd.crosstab(df_analysis['kmeans_state'], df_analysis['hc_state']))\n",
    "\n",
    "# Use K-Means as primary classification\n",
    "df_analysis['estrous_state'] = df_analysis['kmeans_state']\n",
    "print(f\"\\n→ Using K-Means as primary classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that classified groups differ on key features\n",
    "print(\"=\"*70)\n",
    "print(\"CLASSIFICATION VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "estrus_df = df_analysis[df_analysis['estrous_state'] == 'Estrus-like']\n",
    "diestrus_df = df_analysis[df_analysis['estrous_state'] == 'Diestrus-like']\n",
    "\n",
    "# Features to validate (based on Khatiz findings)\n",
    "validation_features = [\n",
    "    ('activity_amplitude', 'Higher in Estrus'),\n",
    "    ('locomotion_duration', 'Higher in Estrus'),\n",
    "    ('total_distance', 'Higher in Estrus'),\n",
    "    ('locomotion_distance', 'Higher in Estrus'),\n",
    "    ('climbing_duration', 'Higher in Estrus'),\n",
    "    ('exploration_intensity', 'Higher in Estrus'),\n",
    "    ('locomotion_bout_count', 'Higher in Estrus'),\n",
    "    ('active_mean_bout', 'Higher in Estrus'),\n",
    "    ('inactive_duration', 'Lower in Estrus'),\n",
    "    ('inferred_sleep_duration', 'Lower in Estrus'),\n",
    "    ('feeding_duration', 'Lower in Estrus'),\n",
    "    ('active_fragmentation', 'Lower in Estrus'),\n",
    "    ('sleep_fragmentation', 'Lower in Estrus'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Feature':<25} {'Estrus':<12} {'Diestrus':<12} {'p-value':<12} {'Expected?'}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "sig_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "for feat, expected in validation_features:\n",
    "    if feat not in df_analysis.columns:\n",
    "        continue\n",
    "    \n",
    "    e_vals = estrus_df[feat].dropna()\n",
    "    d_vals = diestrus_df[feat].dropna()\n",
    "    \n",
    "    e_mean = e_vals.mean()\n",
    "    d_mean = d_vals.mean()\n",
    "    _, p_val = ttest_ind(e_vals, d_vals)\n",
    "    \n",
    "    if 'Higher' in expected:\n",
    "        correct = e_mean > d_mean\n",
    "    else:\n",
    "        correct = e_mean < d_mean\n",
    "    \n",
    "    dir_mark = \"✓\" if correct else \"✗\"\n",
    "    sig_mark = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        sig_count += 1\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print(f\"{feat:<25} {e_mean:<12.2f} {d_mean:<12.2f} {p_val:<12.4f} {dir_mark} {sig_mark}\")\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Significant (p<0.05): {sig_count}/{len(validation_features)}\")\n",
    "print(f\"Correct direction:    {correct_count}/{len(validation_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP embedding\n",
    "print(\"Computing UMAP embedding...\")\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "X_umap = umap_reducer.fit_transform(X_scaled)\n",
    "df_analysis['UMAP1'] = X_umap[:, 0]\n",
    "df_analysis['UMAP2'] = X_umap[:, 1]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification in PCA and UMAP space\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Row 1: PCA\n",
    "ax = axes[0, 0]\n",
    "for state, color in [('Estrus-like', '#e74c3c'), ('Diestrus-like', '#3498db')]:\n",
    "    mask = df_analysis['estrous_state'] == state\n",
    "    ax.scatter(df_analysis.loc[mask, 'PC1'], df_analysis.loc[mask, 'PC2'],\n",
    "               c=color, label=state, alpha=0.6, s=40)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA - Classification')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                     c=df_analysis['total_distance'], cmap='viridis', alpha=0.6, s=40)\n",
    "plt.colorbar(scatter, ax=ax, label='Total Distance')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA - Total Distance')\n",
    "\n",
    "ax = axes[0, 2]\n",
    "scatter = ax.scatter(df_analysis['PC1'], df_analysis['PC2'],\n",
    "                     c=df_analysis['activity_amplitude'], cmap='Reds', alpha=0.6, s=40)\n",
    "plt.colorbar(scatter, ax=ax, label='Activity')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('PCA - Activity Amplitude')\n",
    "\n",
    "# Row 2: UMAP\n",
    "ax = axes[1, 0]\n",
    "for state, color in [('Estrus-like', '#e74c3c'), ('Diestrus-like', '#3498db')]:\n",
    "    mask = df_analysis['estrous_state'] == state\n",
    "    ax.scatter(df_analysis.loc[mask, 'UMAP1'], df_analysis.loc[mask, 'UMAP2'],\n",
    "               c=color, label=state, alpha=0.6, s=40)\n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "ax.set_title('UMAP - Classification')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(df_analysis['UMAP1'], df_analysis['UMAP2'],\n",
    "                     c=df_analysis['total_distance'], cmap='viridis', alpha=0.6, s=40)\n",
    "plt.colorbar(scatter, ax=ax, label='Total Distance')\n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "ax.set_title('UMAP - Total Distance')\n",
    "\n",
    "ax = axes[1, 2]\n",
    "scatter = ax.scatter(df_analysis['UMAP1'], df_analysis['UMAP2'],\n",
    "                     c=df_analysis['activity_amplitude'], cmap='Reds', alpha=0.6, s=40)\n",
    "plt.colorbar(scatter, ax=ax, label='Activity')\n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "ax.set_title('UMAP - Activity Amplitude')\n",
    "\n",
    "plt.suptitle('Dimensionality Reduction: Classification vs Key Features', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature comparison boxplots\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "features_to_plot = [\n",
    "    ('activity_amplitude', 'Activity (s)'),\n",
    "    ('locomotion_duration', 'Locomotion (s)'),\n",
    "    ('total_distance', 'Total Distance'),\n",
    "    ('exploration_intensity', 'Exploration Intensity'),\n",
    "    ('climbing_duration', 'Climbing (s)'),\n",
    "    ('inactive_duration', 'Inactive (s)'),\n",
    "    ('inferred_sleep_duration', 'Sleep (s)'),\n",
    "    ('feeding_duration', 'Feeding (s)'),\n",
    "]\n",
    "\n",
    "for idx, (feat, label) in enumerate(features_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    data = [df_analysis[df_analysis['estrous_state'] == 'Diestrus-like'][feat],\n",
    "            df_analysis[df_analysis['estrous_state'] == 'Estrus-like'][feat]]\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=['Diestrus', 'Estrus'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('#3498db')\n",
    "    bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "    \n",
    "    _, p_val = ttest_ind(data[0].dropna(), data[1].dropna())\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    \n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_title(f'{label}\\np={p_val:.3f} {sig}')\n",
    "\n",
    "plt.suptitle('Feature Comparison: Estrus-like vs Diestrus-like (K-Means Classification)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering dendrogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dendrogram\n",
    "ax = axes[0]\n",
    "dendrogram(linkage_matrix, ax=ax, truncate_mode='level', p=5,\n",
    "           leaf_rotation=90, leaf_font_size=8, color_threshold=0)\n",
    "ax.set_xlabel('Nights (clustered)')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.set_title('Hierarchical Clustering Dendrogram\\n(Ward linkage)')\n",
    "\n",
    "# Silhouette scores for different k\n",
    "ax = axes[1]\n",
    "ax.plot(range(2, 7), silhouettes, 'bo-', markersize=10)\n",
    "ax.set_xlabel('Number of Clusters (k)')\n",
    "ax.set_ylabel('Silhouette Score')\n",
    "ax.set_title('Optimal Number of Clusters')\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xticks(range(2, 7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-animal classification summary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Proportion estrus-like by animal\n",
    "ax = axes[0]\n",
    "estrus_prop = df_analysis.groupby('animal_id').apply(\n",
    "    lambda x: (x['estrous_state'] == 'Estrus-like').mean()\n",
    ")\n",
    "ax.bar(range(len(estrus_prop)), estrus_prop.values, color='#e74c3c', alpha=0.7)\n",
    "ax.axhline(0.25, color='green', linestyle='--', linewidth=2, label='Expected 25%')\n",
    "ax.axhline(estrus_prop.mean(), color='blue', linestyle='-', linewidth=2, label=f'Mean {estrus_prop.mean()*100:.1f}%')\n",
    "ax.set_xticks(range(len(estrus_prop)))\n",
    "ax.set_xticklabels([str(aid)[-4:] for aid in estrus_prop.index], rotation=45, fontsize=8)\n",
    "ax.set_xlabel('Animal ID')\n",
    "ax.set_ylabel('Proportion Estrus-like')\n",
    "ax.set_title('Estrus-like Proportion by Animal')\n",
    "ax.legend()\n",
    "\n",
    "# Nights per animal\n",
    "ax = axes[1]\n",
    "nights_per_animal = df_analysis.groupby('animal_id').size()\n",
    "ax.bar(range(len(nights_per_animal)), nights_per_animal.values, color='#95a5a6', alpha=0.7)\n",
    "ax.set_xticks(range(len(nights_per_animal)))\n",
    "ax.set_xticklabels([str(aid)[-4:] for aid in nights_per_animal.index], rotation=45, fontsize=8)\n",
    "ax.set_xlabel('Animal ID')\n",
    "ax.set_ylabel('Number of Nights')\n",
    "ax.set_title('Data Availability by Animal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Summary and Save Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "n_total = len(df_analysis)\n",
    "n_estrus = (df_analysis['estrous_state'] == 'Estrus-like').sum()\n",
    "pct_estrus = 100 * n_estrus / n_total\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3: BEHAVIORAL STATE CLASSIFICATION - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET:\n",
    "--------\n",
    "• Animals: {df_analysis['animal_id'].nunique()} female C57BL/6J mice (vehicle controls)\n",
    "• Nights: {n_total} (dark cycle only)\n",
    "• Features: {len(FEATURES_FOR_ANALYSIS)} behavioral metrics\n",
    "\n",
    "METHOD:\n",
    "-------\n",
    "• Data-driven classification (no manual weights)\n",
    "• All features standardized equally\n",
    "• K-Means clustering (k=2) to find natural groups\n",
    "• Groups labeled based on activity level\n",
    "\n",
    "CLASSIFICATION RESULTS:\n",
    "-----------------------\n",
    "• Estrus-like nights:   {n_estrus:3d} ({pct_estrus:.1f}%)\n",
    "• Diestrus-like nights: {n_total-n_estrus:3d} ({100-pct_estrus:.1f}%)\n",
    "• Expected: ~25% estrus-like\n",
    "\n",
    "VALIDATION:\n",
    "-----------\n",
    "• Significant features (p<0.05): {sig_count}/{len(validation_features)}\n",
    "• Correct direction: {correct_count}/{len(validation_features)}\n",
    "\"\"\")\n",
    "\n",
    "print(\"KEY CAVEAT:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Without vaginal cytology ground truth, classifications represent\")\n",
    "print(\"'estrus-like' and 'diestrus-like' behavioral patterns, not confirmed\")\n",
    "print(\"estrous phases. Validation is based on expected feature differences\")\n",
    "print(\"reported by Khatiz et al. (2025).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = 'phase3_estrous_classifications.csv'\n",
    "df_analysis.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Summary by animal\n",
    "summary = df_analysis.groupby('animal_id').agg({\n",
    "    'estrous_state': lambda x: (x == 'Estrus-like').sum(),\n",
    "    'night_date': 'count',\n",
    "}).reset_index()\n",
    "summary.columns = ['animal_id', 'n_estrus', 'n_nights']\n",
    "summary['pct_estrus'] = 100 * summary['n_estrus'] / summary['n_nights']\n",
    "\n",
    "print(\"\\nPer-Animal Summary:\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "summary_file = 'phase3_animal_summary.csv'\n",
    "summary.to_csv(summary_file, index=False)\n",
    "print(f\"\\nAnimal summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataframe info\n",
    "print(\"\\nFinal DataFrame Info:\")\n",
    "print(f\"Shape: {df_analysis.shape}\")\n",
    "print(f\"\\nKey columns:\")\n",
    "key_cols = ['cage_id', 'animal_id', 'night_date', 'estrous_state', \n",
    "            'activity_amplitude', 'total_distance', 'locomotion_duration']\n",
    "print(df_analysis[key_cols].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
